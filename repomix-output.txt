This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: **/site/*, dracon/asizeof.py, **/resources/parts/**
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
docs/
  advanced.md
  cli.md
  includes.md
  index.md
  instructions.md
  interpolation.md
  merging.md
dracon/
  loaders/
    env.py
    file.py
    load_utils.py
    pkg.py
  tests/
    configs/
      sub/
        deferred_root.yaml
        instructions.yaml
        subbase.yaml
        subincl.yaml
        variables.yaml
      base.yaml
      complex_interp.yaml
      deferred.yaml
      edge_cases.yaml
      fstem.yaml
      generator_include.yaml
      generators.yaml
      incl_contexts.yaml
      include_define.yaml
      include_interpolations.yaml
      interp_include.yaml
      interpolation.yaml
      list2.yaml
      main.yaml
      override.yaml
      params 2.yaml
      params.yaml
      resolvable.yaml
      scalar.yaml
      simple.yaml
    test_config_composition.py
    test_deepcopy.py
    test_deferred.py
    test_edge_cases.py
    test_interpolate.py
    test_keypath.py
    test_merge.py
    test_mergekey.py
    test_picklable.py
    test_serde.py
  __init__.py
  .repomixignore
  commandline.py
  composer.py
  deferred.py
  draconstructor.py
  dracontainer.py
  include.py
  instructions.py
  interpolation_utils.py
  interpolation.py
  keypath.py
  lazy.py
  loader.py
  merge.py
  nodes.py
  representer.py
  resolvable.py
  trace.py
  utils.py
  yaml.py
.gitignore
LICENSE
mkdocs.yml
pyproject.toml
README.md
repomix.config.json

================================================================
Files
================================================================

================
File: docs/advanced.md
================
# Advanced Usage

## Deferred Nodes

Deferred nodes are nodes of the configuration object that are not constructed immediately when the config is loaded. Instead, they are kept in their raw node form, and will require an explicit call to their `construct` method to be constructed.

It can be pretty useful in a few situations.

An example is when you need to add some context in order to properly construct this node (for example some variable has to be defined), but you need to first load the rest of the configuration to know what this variable should be.

```yaml
# config.yaml
password_path: /path/to/credentials.txt

!deferred(DB) database:
  host: "localhost"
  port: 5432
  password: ${password}
```

```python
from dracon import DraconLoader, DeferredNode
from pydantic import BaseModel

class DB(BaseModel):
    host: str
    port: int
    password: str

loader = DraconLoader(context={'DB': DB})
config = loader.load('config.yaml')

assert isinstance(config.database, DeferredNode[DB])

password = open(config.password_path).read().strip()

config.database = config.database.construct({'password': password})

assert isinstance(config.database, DB)
```

================
File: docs/cli.md
================
# Command Line Programs

Dracon provides utilities to generate command line programs from Pydantic models, leveraging the configuration system for flexible program configuration.

## Basic Usage

Define your program model:

```python
from typing import Annotated
from pydantic import BaseModel
from dracon import Arg, make_program

class DatabaseConfig(BaseModel):
    host: str
    port: int
    username: str
    password: str

class MyProgram(BaseModel):
    database: Annotated[DatabaseConfig, Arg(
        help='Database configuration',
        short='d',
        expand_help=True,
    )]
    verbose: Annotated[bool, Arg(help='Enable verbose output')]

    def run(self):
        # Your program logic here
        if self.verbose:
            print(f"Connecting to {self.database.host}...")
```

Create and run the program:

```python
program = make_program(
    MyProgram,
    name='my-program',
    description='My awesome program'
)

if __name__ == '__main__':
    program_model, args = program.parse_args(sys.argv[1:])
    program_model.run()
```

## Configuration Loading

Your program can load configuration from files:

```yaml
# config.yaml
database:
    host: localhost
    port: 5432
    username: *env:DB_USER
    password: *env:DB_PASSWORD
verbose: false
```

Run with configuration:

```bash
# Load config file
python my_program.py +config.yaml

# Override values
python my_program.py +config.yaml --database.host remotehost --verbose
```

## Argument Annotations

Control how arguments are handled:

```python
class ProcessingConfig(BaseModel):
    input_file: Annotated[str, Arg(
        help='Input file to process',
        short='i',
        is_file=True,
        positional=True
    )]
    output_dir: Annotated[str, Arg(
        help='Output directory',
        short='o',
        is_file=True
    )]
    threads: Annotated[int, Arg(
        help='Number of processing threads',
        short='t',
        default=1
    )]
```

## Resolvable Arguments

Some arguments can be resolved after initialization:

```python
class AdvancedConfig(BaseModel):
    template: Annotated[str, Arg(
        help='Template file',
        is_file=True
    )]
    output: Annotated[str, Arg(
        help='Output file',
        resolvable=True  # Will be resolved after other args
    )]
```

## Custom Actions

Add custom argument actions:

```python
def setup_logging(program: MyProgram, value: Any) -> None:
    level = logging.DEBUG if value else logging.INFO
    logging.basicConfig(level=level)

class LoggingConfig(BaseModel):
    debug: Annotated[bool, Arg(
        help='Enable debug logging',
        action=setup_logging
    )]
```

## Help Messages

Dracon generates formatted help messages:

```bash
$ python my_program.py --help

MyProgram (v1.0.0)
─────────────────

Usage: my-program [OPTIONS]

Options:
  -d, --database DATABASE
    Database configuration
    type: DatabaseConfig
    default: None

  --verbose
    Enable verbose output
    default: False
```

## Advanced Usage

### Environment-Specific Configs

```yaml
# base.yaml
database:
    host: localhost
    port: 5432

# prod.yaml
<<{+<}: *file:base.yaml
database:
    host: prod-db.example.com
    ssl: true
```

Run with environment config:

```bash
python my_program.py +base.yaml +prod.yaml
```

### Dynamic Configuration

```python
class DynamicConfig(BaseModel):
    template: Annotated[str, Arg(
        help='Template file',
        is_file=True
    )]
    variables: Annotated[dict, Arg(
        help='Template variables',
        resolvable=True
    )]

    def resolve_variables(self):
        # Load variables based on template
        with open(self.template) as f:
            template = f.read()
            # Process template to find required variables
            return extract_variables(template)
```

## Best Practices

1. **Structured Configuration**:
   ```python
   class Config(BaseModel):
       input: InputConfig
       processing: ProcessingConfig
       output: OutputConfig
   ```

2. **Clear Help Messages**:
   ```python
   class Config(BaseModel):
       threads: Annotated[int, Arg(
           help='Number of processing threads\n'
                'Use 0 for auto-detection',
           short='t'
       )]
   ```

3. **Default Values**:
   ```python
   class Config(BaseModel):
       log_level: Annotated[str, Arg(
           help='Logging level',
           default='INFO',
       )]
   ```

## Error Handling

```python
try:
    program_model, args = program.parse_args(sys.argv[1:])
except ValidationError as e:
    print("Configuration error:")
    for error in e.errors():
        print(f"  - {error['loc'][0]}: {error['msg']}")
    sys.exit(1)
```

================
File: docs/includes.md
================
# File Inclusion

Dracon's file inclusion system allows you to break down your configurations into reusable, modular files. This is particularly useful for handling environment-specific settings, separating credentials, or organizing large configurations.

## Inclusion Syntax

Dracon offers two ways to include files:

### 1. Tag Syntax

The `!include` tag is the most explicit way to include a file:

```yaml
# Include a file using the default (file) loader
settings: !include "config/settings.yaml"

# Explicitly specify the file loader
database: !include file:config/database.yaml

# Include from a Python package
templates: !include pkg:my_package:configs/templates.yaml
```

### 2. Anchor Syntax

The `*loader:` syntax is shorter and can be used in more contexts:

```yaml
# Include a file using the file loader
database: *file:config/database.yaml

# Include an environment variable
api_key: *env:API_KEY

# Include from a package
defaults: *pkg:my_package:configs/defaults.yaml
```

Both syntaxes work in most cases, but the anchor syntax (`*`) is more concise and has better compatibility with other YAML processors.

## Available Loaders

Dracon comes with these built-in loaders:

### File Loader

Loads files from the filesystem:

```yaml
# Absolute path
config: *file:/etc/myapp/config.yaml

# Relative path (to current file)
settings: *file:./settings.yaml

# Path with filename only (searches in relative paths)
logging: *file:logging.yaml
```

### Environment Variables

Loads values from environment variables:

```yaml
api_key: *env:API_KEY
debug_mode: *env:DEBUG
port: ${int(env.get('PORT', '8080'))}
```

### Package Resources

Loads files from installed Python packages:

```yaml
defaults: *pkg:my_package:configs/defaults.yaml
```

## Context Variables

When Dracon loads a file, it adds special variables to the context that you can use in expressions:

```yaml
# These values are set automatically for each included file
file_info:
  directory: ${$DIR} # Directory containing the current file
  full_path: ${$FILE} # Full path to the current file
  filename: ${$FILE_STEM} # Filename without extension
  extension: ${$FILE_EXT} # File extension
  load_time: ${$FILE_LOAD_TIME} # Timestamp when the file was loaded
```

These variables are particularly useful when including files that need to reference their location:

```yaml
# In config/app.yaml
log_directory: ${$DIR}/logs
templates: ${$DIR}/templates
log_file: ${$FILE_STEM}.log # Will resolve to "app.log"
```

## Including Specific Keys

You can include just a part of another file using the `@` syntax:

```yaml
# Include only the database section from settings.yaml
database: *file:settings.yaml@database

# Include a deeply nested key
timeout: *file:config.yaml@services.api.timeout

# Note: Keys with dots must be escaped
dotted_key: *file:config.yaml@section\.with\.dots
```

## Variables in Paths

You can use interpolation in include paths:

```yaml
# Use environment-specific settings
!define env: "production"
settings: !include "configs/${env}/settings.yaml"

# Use version-specific configurations
version_config: *file:configs/v${version}/config.yaml
```

## Best Practices

### 1. Organize by Feature or Component

Group related settings into separate files:

```yaml
# main.yaml
database: !include "components/database.yaml"
api: !include "components/api.yaml"
logging: !include "components/logging.yaml"
```

### 2. Layer by Environment

Create a base config and environment-specific overrides:

```yaml
# prod.yaml
<<{+<}: *file:base.yaml
database:
  host: "prod-db.example.com"
  ssl: true
```

### 3. Manage Secrets Separately

Keep sensitive data in separate files:

```yaml
# app.yaml
database:
  host: "db.example.com"
  port: 5432
  credentials: !include "secrets/db_creds.yaml"
```

### 4. Use Relative Paths

For portable configurations, use relative paths:

```yaml
# utils/config.yaml
templates: *file:${$DIR}/templates
resources: *file:${$DIR}/../resources
```

## Error Handling

Dracon will raise an error if an included file cannot be found. For optional includes, use interpolation with fallbacks:

```yaml
# Try to include a file, fallback to an empty dict if not found
overrides: ${try_include('file:overrides.yaml', {})}

# Helper function in your context
def try_include(path, default=None):
  try: return loader.load(path)
  except FileNotFoundError: return default
```

## Custom Loaders

You can create custom loaders for additional sources:

```python
def read_from_redis(path: str, loader=None):
    """Load configuration from Redis"""
    import redis
    r = redis.Redis()

    # Path format: redis:key
    key = path
    yaml_data = r.get(key)

    if yaml_data is None:
        raise FileNotFoundError(f"Redis key not found: {key}")

    return yaml_data.decode('utf-8'), {
        '$REDIS_KEY': key,
        '$REDIS_TIMESTAMP': time.time()
    }

# Register the loader
loader = DraconLoader(
    custom_loaders={'redis': read_from_redis}
)

# Now you can use it
# config: *redis:app:settings
```

================
File: docs/index.md
================
# Dracon

Dracon is a modular configuration system for Python that extends YAML with powerful features. It bridges the gap between simple configuration files and more complex application needs with features like file inclusion, expression interpolation, and type validation.

## Why Dracon?

Regular configuration systems often fall short when your application gets more complex:

- Need to share settings across multiple files? Dracon lets you include and merge files.
- Have environment-specific settings? Use expressions and interpolation.
- Want type safety? Dracon integrates with Pydantic for validation.
- Building a CLI? Generate full command-line interfaces from your config models.

## Core Features

- **File Inclusion**: Include and merge other configuration files
- **Expression Interpolation**: Use Python expressions within your YAML files
- **Advanced Merging**: Control exactly how configurations combine with flexible merge strategies
- **Deferred Construction**: Postpone object creation until you have all the context you need
- **Pydantic Integration**: Build type-safe configuration models
- **CLI Support**: Generate full-featured command-line programs from your models

## Quick Start

### Installation

```bash
pip install dracon
```

### Basic Usage

```python
from dracon import DraconLoader

# Load a configuration file with some context variables
loader = DraconLoader(context={"instance_id": 1})
config = loader.load("config.yaml")

# Access your configuration values
print(config.database.host)
print(config.service.port)
```

### Example Configuration

Here's a simple configuration file that shows some of Dracon's features:

```yaml
# Define variables for use throughout the config
!define env: ${os.getenv('ENV', 'development')}
!define debug: ${env != 'production'}

# Include common settings
common: !include file:./common.yaml

# Database configuration
database:
  host: ${env.get('DB_HOST', 'localhost')}
  port: 5432
  # Include credentials from a separate file
  credentials: !include file:./credentials/${env}.yaml

# Service configuration
service:
  name: "MyService"
  # Dynamic port calculation
  port: ${8080 + instance_id}
  # Include environment-specific settings
  settings: !include file:./settings/${env}.yaml
  # Conditional configuration
  !if ${debug}:
    log_level: "DEBUG"
    profiling: true
```

## Working with Models

Dracon integrates well with Pydantic for type validation:

```python
from pydantic import BaseModel
from typing import List, Optional
from dracon import DraconLoader

class DatabaseConfig(BaseModel):
    host: str
    port: int
    user: str
    password: str

class ServiceConfig(BaseModel):
    name: str
    port: int
    log_level: Optional[str] = "INFO"
    profiling: bool = False

class AppConfig(BaseModel):
    database: DatabaseConfig
    service: ServiceConfig
    version: str

# Load and validate configuration
loader = DraconLoader()
config_data = loader.load("config.yaml")
app_config = AppConfig(**config_data)

# Now you have a fully validated configuration
print(f"Connecting to database at {app_config.database.host}:{app_config.database.port}")
```

## Architecture Overview

Dracon works in several phases:

1. **Composition**: Parse YAML and handle includes and merges
2. **Interpolation**: Resolve expressions and references
3. **Construction**: Build Python objects from the composed config
4. **Validation**: Optionally validate using Pydantic models

```
┌──────────┐   ┌─────────────┐   ┌─────────────┐   ┌───────────┐
│ YAML File│──>│ Composition │──>│Construction │──>│  Python   │
└──────────┘   │  (include,  │   │ (objects,   │   │  Objects  │
               │   interp,   │   │ validation) │   └───────────┘
               │    merge)   │   └─────────────┘
               └─────────────┘
```

## Next Steps

Check out the detailed guides for each feature:

- [File Inclusion](includes.md): Learn how to modularize your configurations
- [Expression Interpolation](interpolation.md): Add dynamic expressions to your configs
- [Advanced Merging](merging.md): Control how configurations are combined
- [Node Instructions](instructions.md): Use special directives like `!if` and `!each`
- [Command Line Programs](cli.md): Generate CLIs from your config models
- [Advanced Usage](advanced.md): Explore deferred nodes and more advanced features

================
File: docs/instructions.md
================
# Summary of instruction Nodes and special tags

Dracon can parse special instruction nodes that help manipulate the configuration object.
Instructions are tags that start with `!`.

## !define varname: value

The `!define` (and `!set_default`) instructions allow you to define variables in your configuration file.

```yaml
!define var1: 42
!set_default var2: 3.14 # only set if not already defined

# Use the defined variables
value1: ${var1}
value2: ${var2}
```

## !if

The `!if` instruction allows you to conditionally include or exclude parts of your configuration.

```yaml
!define condition: true

!if: ${condition}
  key: value
```

## !each(varname): iterable

The `!each` instruction allows you to iterate over a list or dictionary and include or exclude parts of your configuration.

```yaml
!each(item): [1, 2, 3]
  item_${item}: ${item}
```

## !noconstruct

The `!noconstruct` instruction prevents the construction of the current node.

example:

```yaml
!noconstruct key: value # won't appear in the final configuration
other_key: ${&/key} # OK (it's a copy of the /key node), will be correctly replaced by "value" on evaluation
```

The final configuration will be `{other_key: value}`.

> [!Note]
> You can also use any top-level mapping node with a key that starts with `__dracon__`:
> just as if it had a `!noconstruct` tag, it won't appear in the final configuration.

## !include path

See [File Inclusion](includes.md)

## !MyType, !package.MyType

That's the general syntax for specifying that a node should be constructed as an instance of a specific class. You can register any class in the `DraconLoader` instance within the context dictionary. If a package (or module) is specified, Dracon will try to import it and use the class from there.

By default, Dracon will try to use a Pydantic model if available to validata and construct the node.

```yaml
!Person
name: Alice
age: 42
```

```python
from dracon import DraconLoader
from pydantic import BaseModel

class Person(BaseModel):
    name: str
    age: int

loader = DraconLoader(context={'Person': Person})
person = loader.load('person.yaml')

assert isinstance(person, Person)
assert person.name == 'Alice'
assert person.age == 42
```

================
File: docs/interpolation.md
================
# Expression Interpolation

Dracon's interpolation system enables you to embed Python expressions in your YAML configurations. This brings dynamic value generation, calculations, and complex logic to what would otherwise be static configuration files.

## Interpolation Types

Dracon supports two distinct types of interpolation:

### 1. Lazy Interpolation - `${...}`

The primary syntax uses curly braces and evaluates expressions when the value is accessed:

```yaml
port: ${8080 + instance_id}
debug: ${env != 'production'}
greeting: ${"Hello, " + username + "!"}
```

Lazy interpolation means the expression is calculated "just in time" when the value is actually used. This allows referencing values that might not be available during parsing.

### 2. Immediate Interpolation - `$(...)`

This alternative syntax with parentheses evaluates expressions during parsing:

```yaml
!define current_time: $(time.strftime('%Y-%m-%d'))
tag_immediate: !$(str('float')) 5.0 # Results in: !float 5.0
```

Immediate interpolation is useful for type tags and other values needed during the parsing phase.

## Using Python Expressions

You can use most Python expressions inside interpolation:

### Basic Operations

```yaml
# Arithmetic
memory_mb: ${1024 * 8}
timeout_sec: ${timeout_min * 60}

# String operations
greeting: ${"Hello, " + username.title() + "!"}
uppercase: ${service_name.upper()}

# Boolean logic
is_admin: ${role == 'admin' or username in admin_list}
debug_logging: ${env != 'production' and enable_debug}
```

### Conditional Expressions

```yaml
# Ternary conditionals
mode: ${'production' if env == 'prod' else 'development'}
log_level: ${'DEBUG' if debug else 'INFO'}
```

### Working with Collections

```yaml
# List operations
first_item: ${items[0]}
item_count: ${len(items)}
filtered: ${[x for x in items if x > threshold]}

# Dictionary operations
api_url: ${urls.get('api', 'https://api.default.com')}
```

## Path References

Dracon has a special syntax for referencing other values in your configuration using `@`:

### Absolute Paths

Use a leading slash (`/`) to reference from the root of the configuration:

```yaml
database:
  host: "db.example.com"
  port: 5432
  url: ${"postgresql://" + @/database/host + ":" + str(@/database/port)}
# This references database.host from the root config
```

### Relative Paths

Reference values relative to the current location:

```yaml
service:
  name: "api-service"
  # Reference the sibling "name" field
  log_prefix: ${@name + ": "}

  logging:
    # Reference parent's "name" field
    file: ${@../name + ".log"}
```

### Parent References

Navigate up the tree with `..`:

```yaml
deep:
  nested:
    structure:
      value: 42
      # Go up two levels and access a sibling
      reference: ${@../../sibling}
  sibling: "hello"
```

## Node References

Use `&` to reference entire nodes (not just their values):

```yaml
__dracon__template: # This node won't appear in final config
  base_service: &base_service
    created_at: ${datetime.now()}
    version: ${VERSION}

# Create multiple objects from the template
services:
  web: &base_service
  api:
    <<: &base_service
    port: 8080
```

The difference between `&` and `@`:

- `&` creates a reference to a node, which can be duplicated and modified
- `@` references a value in the final, constructed configuration

## Context and Variables

### Predefined Variables

Dracon provides several built-in variables:

```yaml
# Environment variables
host: ${env.get('HOST', 'localhost')}

# Special file context variables (available in included files)
file_dir: ${$DIR}
file_name: ${$FILE_STEM}
```

### Custom Context Variables

You can provide additional variables when loading configurations:

```python
loader = DraconLoader(
    context={
        'VERSION': '1.0.0',
        'ENVIRONMENT': 'production',
        'get_uuid': lambda: str(uuid.uuid4()),
        'now': datetime.now
    }
)
```

Then use them in your YAML:

```yaml
app:
  version: ${VERSION}
  env: ${ENVIRONMENT}
  id: ${get_uuid()}
  start_time: ${now()}
```

## Type Interpolation

You can interpolate both values and types:

```yaml
# Interpolate the type tag
value: !${type_name} ${value}

# Examples
as_int: !${'int'} ${2.5}          # Results in integer 2
as_str: !${'str'} ${123}          # Results in string "123"
dynamic_type: !${chosen_type} 42  # Type from variable
```

## Advanced Techniques

### Multiple Interpolations

You can nest interpolations:

```yaml
# Nested interpolations
double: ${int(${value} * 2)}

# Multiple interpolations in a string
connection: ${"Host=" + host + ";Port=" + str(${port})}
```

### Function Calls

Call functions from your context:

```yaml
# Assuming these are in your context
uuid: ${generate_uuid()}
timestamp: ${time.time()}
random_value: ${random.choice(['a', 'b', 'c'])}
```

### Working with Defaults

Handle potentially undefined values:

```yaml
# Default if the variable doesn't exist
region: ${globals().get('AWS_REGION', 'us-east-1')}

# Using or with interpolation
db_url: ${database_url or "sqlite:///app.db"}
```

## Best Practices

1. **Keep expressions simple** - Complex logic belongs in your code, not config
2. **Use meaningful defaults** - Handle missing values gracefully
3. **Watch for side effects** - Avoid expressions with unintended consequences
4. **Define common values** - Use `!define` for values referenced multiple times

By using interpolation effectively, you can create flexible configurations that adapt to their environment without sacrificing readability or maintainability.

================
File: docs/merging.md
================
# Advanced Merging

Dracon provides a flexible merging system that allows you to combine configurations in sophisticated ways using merge operators.

## Basic Merge Syntax

The merge operator follows this pattern:

```yaml
<<{dict_options}[list_options]@keypath: value
```

- `{dict_options}`: How to merge dictionaries
  - `~`: Overwrite same-key values (default)
  - `+`: Recursively append values (i.e., deep merge)
    - `number`: Limit recursion depth
  - `<`: Priority to existing values (default)
  - `>`: Priority to new values

- `[list_options]`: How to merge lists
  - `~`: Overwrite lists entirely (default)
  - `+`: Concatenate lists
  - `<`: Priority to new items when overwriting, prepend new when concatenating (default)
  - `>`: Priority to existing items when overwriting, prepend existing when concatenating

- `@keypath`: Where to apply the merge (optional) (default = no path = in-place merge)

## Merge Options

### Dictionary Merging

```yaml
<<: *file:base.yaml # Default merge, priority to existing values, non-recursive
# Equivalent to
<<{>~}: !include file:override.yaml
# or (order doesn't matter)
<<{~>}: !include file:override.yaml

# Recursive merge with priority to new values
<<{+<}: *file:new_values.yaml

# Recursive merge with priority to new values, limit recursion depth
<<{+<2}: *file:new_values.yaml

# Recursive merge with priority to existing values
<<{+>}: !include file:defaults.yaml
```

### List Merging

```yaml
# Replace lists
<<[~<]: [new_item1, new_item2] # new list overwrites existing list
<<[~>]: [new_item1, new_item2] # existing list overwrites new list

# Merge with priority
<<[+<]: [priority_items] # Existing items at the end
<<[+>]: [fallback_items] # New items at the end
```

## Targeting Specific Paths

```yaml
# Merge at a specific path
<<{+}@settings.database: *file:db_override.yaml

# Merge multiple paths
<<{+}@settings.logging: *file:logging.yaml
<<{+}@settings.cache: *file:cache.yaml
```

## Misc Examples

```yaml
# Merge only 2 levels deep
<<{+2}: *file:shallow_merge.yaml

# Deep merge for dictionaries, shallow for lists
<<{+<}[~]: *file:mixed_merge.yaml
```

```yaml
# base.yaml
app:
  name: "MyApp"
  database:
    host: localhost
    port: 5432
```

```yaml
# prod.yaml
<<{+>}: *file:base.yaml # priority to the new values, recursively
app:
  database:
    host: "prod-db.example.com"
    ssl: true
```

```yaml
# features.yaml
features:
  base: &base_features
    logging: true
    metrics: true

  development:
    <<: *base_features # default yaml merge, priority to the new value, non-recursive merge
    debug: true

  production:
    <<: *base_features
    audit: true
    ssl: true
```

```yaml
# service_base.yaml
service:
  timeout: 30
  retries: 3
  endpoints:
    - "/api/v1"
    - "/health"
```

```yaml
# service_override.yaml
<<{+<}[+<]@service: # Recursive merge with service config (override timeout), append new endpoints at the end
  timeout: 60
  endpoints:
    - "/metrics"
```

================
File: dracon/loaders/env.py
================
import os
from typing import ForwardRef, TypeAlias, Optional

DraconLoader = ForwardRef('DraconLoader')


def read_from_env(path: str, loader: Optional[DraconLoader] = None):
    return str(os.getenv(path)), {}

================
File: dracon/loaders/file.py
================
from pathlib import Path
from typing import ForwardRef, TypeAlias
from .load_utils import with_possible_ext
from typing import Optional

from cachetools import cached, LRUCache
from cachetools.keys import hashkey
import time

DraconLoader = ForwardRef('DraconLoader')


@cached(LRUCache(maxsize=50))
def read_from_file(path: str, extra_paths=None):
    """
    Reads the content of a file, searching in the specified path and additional paths if provided.

    Args:
        path (str): The primary path to the file.
        extra_paths (list, optional): Additional paths to search for the file. Defaults to None.
        loader (Optional[DraconLoader], optional): An optional loader to update context. Defaults to None.

    Returns:
        str: The content of the file.

    Raises:
        FileNotFoundError: If the file is not found in any of the specified paths.
    """
    all_paths = with_possible_ext(path)
    if not extra_paths:
        extra_paths = []

    extra_path = [Path('./')] + [Path(p) for p in extra_paths]

    for ep in extra_path:
        for p in all_paths:
            p = (ep / p).expanduser().resolve()
            if Path(p).exists():
                path = p.as_posix()
                break

    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f'File not found: {path}')

    with open(p, 'r') as f:
        raw = f.read()

    now = time.time()

    new_context = {
        '$DIR': p.parent.as_posix(),
        '$FILE': p.as_posix(),
        '$FILE_PATH': p.as_posix(),
        '$FILE_STEM': p.stem,
        '$FILE_EXT': p.suffix,
        '$FILE_LOAD_TIME': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(now)),
        '$FILE_LOAD_TIME_UNIX': int(now),
        '$FILE_LOAD_TIME_UNIX_MS': int(now * 1000),
        '$FILE_SIZE': p.stat().st_size,
    }

    return raw, new_context

================
File: dracon/loaders/load_utils.py
================
from pathlib import Path


def with_possible_ext(path: str):
    # return: the original, with .yaml, with .yml, without extension. in that order
    p = Path(path)
    return [p, p.with_suffix('.yaml'), p.with_suffix('.yml'), p.with_suffix('')]

================
File: dracon/loaders/pkg.py
================
from .load_utils import with_possible_ext
import time
from importlib.resources import files, as_file
from typing import ForwardRef, Optional
from pathlib import Path
from cachetools import cached, LRUCache
from cachetools.keys import hashkey

DraconLoader = ForwardRef('DraconLoader')


@cached(LRUCache(maxsize=50))
def read_from_pkg(path: str):
    pkg = None

    if ':' in path:
        pkg, path = path.split(':', maxsplit=1)

    if not pkg:
        raise ValueError('No package specified in path')

    all_paths = with_possible_ext(path)

    for fpath in all_paths:
        try:
            with as_file(files(pkg) / fpath.as_posix()) as p:
                now = time.time()
                with open(p, 'r') as f:
                    pp = Path(p).resolve().absolute()
                    new_context = {
                        '$DIR': pp.parent.as_posix(),
                        '$FILE': pp.as_posix(),
                        '$FILE_PATH': pp.as_posix(),
                        '$FILE_STEM': pp.stem,
                        '$FILE_EXT': pp.suffix,
                        '$FILE_LOAD_TIME': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(now)),
                        '$FILE_LOAD_TIME_UNIX': int(now),
                        '$FILE_LOAD_TIME_UNIX_MS': int(now * 1000),
                        '$FILE_SIZE': pp.stat().st_size,
                        '$PACKAGE_NAME': pkg,
                    }

                    return f.read(), new_context
        except FileNotFoundError:
            pass

    # it failed
    tried_files = [str(files(pkg) / p.as_posix()) for p in all_paths]
    tried_str = '\n'.join(tried_files)
    resources = [resource.name for resource in files(pkg).iterdir() if not resource.is_file()]
    resources_str = '\n  - '.join(resources)
    raise FileNotFoundError(
        f'''File not found in package {pkg}: {path}. Tried: {tried_str}.
        Package root: {files(pkg)}
        Available subdirs:
        - {resources_str}'''
    )

================
File: dracon/tests/configs/sub/deferred_root.yaml
================
!deferred
!set_default var_a: 4
!define debug0: "${print('in deferred_root, var_a is', var_a)}"
ayy: lmao
<<: !include file:$DIR/../interpolation

instructs:
  <<: !include file:$DIR/instructions

a: ${var_a}

================
File: dracon/tests/configs/sub/instructions.yaml
================
!set_default elements: [1, 2, 3]

things:
  !each(elt) "${elements}":
    - !deferred
      !define avar: ${elt}
      !set_default elt_value: ${elt}
      !set_default plus_one: ${int(elt_value) + 1}
      <<: !include file:$DIR/variables
      fstem: !include file:$DIR/../fstem
      elt: ${plus_one}

================
File: dracon/tests/configs/sub/subbase.yaml
================
default_settings: 
  setting1: default_value1
  setting2: default_value2
  setting_list:
    - item1
    - item2
    - item3
  <<: !include file:$DIR/../params
  again: *pkg:dracon:tests/configs/params
  just_simple: *pkg:dracon:tests/configs/params@simple_params.additional_settings

fstem: !include file:$DIR/../fstem

ppath: *env:TESTVAR2

================
File: dracon/tests/configs/sub/subincl.yaml
================
fstem_here: ${"$FILE_STEM"}
fstem_above: !include file:$DIR/../fstem

================
File: dracon/tests/configs/sub/variables.yaml
================
!set_default avar: 1
!define bvar: 2

a: ${avar}
b: ${bvar}

================
File: dracon/tests/configs/base.yaml
================
default_settings: 
  setting1: default_value1
  setting2: default_value2
  setting_list:
    - item1
    - item2
    - item3
  <<: *pkg:dracon:tests/configs/params
  again: *pkg:dracon:tests/configs/params
  just_simple: *file:$DIR/params@simple_params.additional_settings


ppath: *env:TESTVAR2

empty: *pkg:dracon:tests/configs/empty

scalar: !include file:$DIR/scalar

================
File: dracon/tests/configs/complex_interp.yaml
================
a_obj: !ClassA
    index: &aid ${i}
    name: oldname
    <<{<+}: 
        name: "oldname ${&aid}"

================
File: dracon/tests/configs/deferred.yaml
================
!set_default var_a: 0

!define debug0: "${print('in deferred, var_a is', var_a)}"

some_basic_key: "ok"

main_content: !deferred
  <<: !include file:$DIR/main

simple_merge: !deferred
  !set_default var_a: 3
  <<: !include file:$DIR/simple
  <<{<+}:
    root:
      a: ${var_a}

deferred_root: !include file:$DIR/sub/deferred_root

a: ${var_a}

================
File: dracon/tests/configs/edge_cases.yaml
================
# filename: dracon/tests/configs/edge_cases.yaml
# Keys with names that could clash with internals
value: not_an_internal_value
context: not_an_internal_context
tag: not_an_internal_tag
anchor: not_an_internal_anchor

# Keys with dots
dotted.keys:
  nested.value: simple_value
  another.dotted.key: another_value
  value: not_an_internal_value
  context: not_an_internal_context
  tag: not_an_internal_tag
  anchor: not_an_internal_anchor

# Using each with dotted keys and keyword keys
!define items: [1, 2, 3]
each_with_dots:
  !each(item) "${items}":
    # these keys MUST be interpolable for !each on mappings
    item.${item}: value_${item}
    nested.item.${item}: nested_value_${item}
    value_${item}: not_an_internal_value # made dynamic
    context_${item}: not_an_internal_context # made dynamic
    tag_${item}: not_an_internal_tag # made dynamic
    anchor_${item}: not_an_internal_anchor # made dynamic

# Nested structures with dot-containing keys
nested:
  level1:
    dotted.key: deep_value
  array:
    - key.with.dots: array_value1
    - key.with.dots: array_value2

# References to dotted keys using @ syntax
reference_test:
  simple_ref: ${@dotted.keys/nested.value} # changed \. to . and . to /
  nested_ref: ${@nested/level1/dotted.key} # changed . to /

!define suffix: dynamic
interpolated.keys.${suffix}:
  value: interpolated_value

# Merge operations with dotted keys
base.with.dots:
  key1: base_value1
  nested.key: base_nested

<<{+<}@base.with.dots: # changed \. to .
  key2: override_value2
  nested.key: override_nested

# NOT PASSING FOR NOW:

# Deferred nodes with dotted keys
!deferred deferred.node:
  dotted.key: deferred_value
  reference: ${@dotted.keys/nested.value} # changed \. to . and . to /

# Complex nested structures with dotted keys and interpolation
complex:
  !define level: first
  ${level}.level:
    !define inner_key: inner.value
    ${inner_key}: complex_inner_value
    reference: ${@complex/${level}/level/${inner_key}} # changed . to /, removed \\

================
File: dracon/tests/configs/fstem.yaml
================
here: ${"$FILE_STEM"}

================
File: dracon/tests/configs/generator_include.yaml
================
!define n: !generate [1, 2]

<<: !include file:$DIR/generators

!if ${n == 2}:
  n_is_2: true

================
File: dracon/tests/configs/generators.yaml
================
!set_default n: 3

!noconstruct value: &val
  b: ${a}

wrapper: !generate ${[&val:a=i for i in range(n)]}

================
File: dracon/tests/configs/incl_contexts.yaml
================
fstem_basedir: ${"$FILE_STEM"}
fstem_subdir: !include file:$DIR/sub/subincl

!define avar: 3
!define bvar: 4
avar_from_sub: !include file:$DIR/sub/variables@a
bvar_from_sub: !include file:$DIR/sub/variables@b


vars: 
  !define avar: 5
  <<: !include file:$DIR/sub/variables

================
File: dracon/tests/configs/include_define.yaml
================
!set_default var_a: 2

!if ${var_a == 2}:
  a: 2
!if ${var_a == 3}:
  a: 3

!define var_b: ${undefined_var + 5}

var_b_value: !include $var_b

================
File: dracon/tests/configs/include_interpolations.yaml
================
!define filename: "base"
base:
  <<: !include file:$DIR/${filename}.yaml

!define addsettings: "additional_settings"

just_simple: !include file:$DIR/params@simple_params.${addsettings}

================
File: dracon/tests/configs/interp_include.yaml
================
!noconstruct _: !include &aobj pkg:dracon:tests/configs/complex_interp@a_obj

!define var_a: 3
!define undefined_var: ${5 + 5}
other: !include file:$DIR/include_define

nested:
  !define a_id: ${get_index(construct(&aobj:i=2))}
  a_index: ${a_id}
  oldname: ${(&aobj:i=2).name}
  !define new_a: ${&aobj:i=3}
  !define constructed_a: ${construct(new_a)}
  nameindex: ${get_nameindex(constructed_a)}
  nameindex_2: ${constructed_a.name_index}
  a_nested: &a_nest
    <<: !include $new_a # include a copy of aobj with i=3
    <<{<+}:
      name: "newer_name ${&a_nest.index}"

  alist:
    !each(i) ${list(range(1, 3))}:
      - <<: !include aobj
        <<{<+}:
          name: "name ${i}"

================
File: dracon/tests/configs/interpolation.yaml
================
base:
  <<{>}: *file:$DIR/base.yaml
  <<: *file:$DIR/sub/subbase.yaml
  interpolated_addition: ${2+2}
  file_stem: ${"$FILE_STEM"}

loaded_base: !include file:$DIR/base


int4: ${2+2}

nested_int4: ${${2+2}}

floatstr: "$(str('float'))"

tag_interp: !$(str('float')) ${2+2}

interp_later: 4
<<{<}@interp_later: ${3+2}

interp_later_tag: 4
<<{<}@interp_later_tag: !$(str('float')) 5

================
File: dracon/tests/configs/list2.yaml
================
list2: [7, 8, 9]

================
File: dracon/tests/configs/main.yaml
================
base: &base
  setting1: baseval
  setting2: baseval2
  setting.with.dot: baseval3

other_base: *pkg:dracon:tests/configs/base.yaml

config:
  setting1: newval
  <<{<}[+]:
    setting1: newval1
    setting3: *base # testing anchor reference
    setting2: *base@setting2
    setting2_incl: !include "base@setting2"
    extra: *pkg:dracon:tests/configs/simple.yaml
    home: *env:TESTVAR1
    new_with.dot: */base.setting\.with\.dot
    a_list:
      - item4

  a_list:
    - item1
    - item2
    - item3


new_simple:
  <<{~>}: *pkg:dracon:tests/configs/simple
  root:
    a: 'new_a'

================
File: dracon/tests/configs/override.yaml
================
default_settings: 
  setting1: default_value1
  setting2: default_value2
  setting_list:
    - item1
    - item2
    - item3
  <<{<}[<]: *pkg:dracon:tests/configs/simple@additional_settings

<<{<}@default_settings.setting1: override_value1

<<[>+]@default_settings:
  setting3: override_value3
  setting_list:
    - item4

<<@default_settings.setting_list.0: override_item1

================
File: dracon/tests/configs/params 2.yaml
================
param1: value1_overriden

================
File: dracon/tests/configs/params.yaml
================
param1: value1
param2: value2
setting2: value_params_2
simple_params:
  <<: *pkg:dracon:tests/configs/simple

<<{~<}[~<]: *pkg:dracon:tests/configs/list2
<<{<}: !include "pkg:dracon:tests/configs/params 2"

================
File: dracon/tests/configs/resolvable.yaml
================
ned: !Resolvable[Person]
  name: Eddard
  age: 40

jon: !Resolvable[$(str('Person'))]
  name: Jon
  age: 20

================
File: dracon/tests/configs/scalar.yaml
================
'hello'

================
File: dracon/tests/configs/simple.yaml
================
root:
  a: 3
  b: 4
  inner:
    c: 5
    d: 6

param2: value_simple_2

additional_settings:
  setting3: additional_value3
  setting_list:
    - item_lol
    - */root.a
    - *.0

================
File: dracon/tests/test_config_composition.py
================
import os
from pathlib import Path
import pytest
from ruamel.yaml import YAML
from dracon.loader import DraconLoader
from dracon.resolvable import Resolvable
from pydantic import BaseModel
from dracon.include import compose_from_include_str
from dracon.utils import deepcopy
import tempfile

# Set a dummy environment variable for testing purposes
os.environ["TESTVAR1"] = "test_var_1"
os.environ["TESTVAR2"] = "test_var_2"

# Test file paths
simple_config_path = 'dracon:tests/configs/simple.yaml'

main_config_path = 'dracon:tests/configs/main.yaml'
params_config_path = 'dracon:tests/configs/params.yaml'
base_config_path = 'dracon:tests/configs/base.yaml'
interp_config_path = 'dracon:tests/configs/interpolation.yaml'
resolvable_config_path = 'dracon:tests/configs/resolvable.yaml'
override_config_path = 'dracon:tests/configs/override.yaml'


def main_config_ok(config):
    # Check if the composition result matches the expected values
    assert config["base"]["setting.with.dot"] == "baseval3"
    assert config["config"]["setting1"] == "newval1"
    assert config["config"]["setting2"] == "baseval2"
    assert config["config"]["setting2_incl"] == "baseval2"
    assert config["config"]["setting3"]["setting1"] == "baseval"
    assert config["config"]["setting3"]["setting2"] == "baseval2"
    assert config["config"]["setting3"]["setting.with.dot"] == "baseval3"

    assert config["config"]["extra"]["root"]["a"] == 3
    assert config["config"]["extra"]["root"]["b"] == 4
    assert config["config"]["extra"]["root"]["inner"]["c"] == 5
    assert config["config"]["extra"]["root"]["inner"]["d"] == 6
    assert config["config"]["extra"]["additional_settings"]["setting3"] == "additional_value3"
    assert config["config"]["home"] == "test_var_1"
    assert config["config"]["a_list"] == ["item1", "item2", "item3", "item4"]

    assert config["config"]["new_with.dot"] == "baseval3"

    assert config["other_base"]["default_settings"]["param1"] == "value1_overriden"
    assert config["other_base"]["default_settings"]["setting1"] == "default_value1"
    assert config["other_base"]["default_settings"]["setting2"] == "default_value2"
    assert config["other_base"]["default_settings"]["again"]["setting2"] == "value_params_2"
    assert (
        config["other_base"]["default_settings"]["just_simple"]["setting3"] == "additional_value3"
    )
    assert config["other_base"]["default_settings"]["just_simple"]["setting_list"] == [
        "item_lol",
        3,
        "item_lol",
    ]

    assert config["new_simple"]["root"] == {"a": "new_a"}
    assert config["new_simple"]["additional_settings"]["setting_list"] == [
        "item_lol",
        3,
        "item_lol",
    ]

    assert config["other_base"]["scalar"] == "hello"


def get_config(config_path):
    loader = DraconLoader(enable_interpolation=True)
    compres = compose_from_include_str(loader, f"pkg:{config_path}")
    config = loader.load_composition_result(compres)
    return config


def test_main_config_composition():
    config = get_config(main_config_path)
    main_config_ok(config)


def test_copy_composition_result():
    loader = DraconLoader()
    composition = compose_from_include_str(loader, f"pkg:{main_config_path}")

    # Copy the composition result and the loader
    comp_copy = deepcopy(composition)
    loader_copy = deepcopy(loader)

    origconf = loader.load_composition_result(composition)
    confcopy = loader_copy.load_composition_result(comp_copy)

    assert origconf == confcopy


def test_simple_config_inclusion():
    config = get_config(simple_config_path)

    assert 'root' in config
    assert 'inner' in config['root']
    assert 'a' in config['root']
    assert 'b' in config['root']
    assert 'c' in config['root']['inner']
    assert 'd' in config['root']['inner']

    # Check if the extra configuration is composed correctly
    assert config["root"]["a"] == 3
    assert config["root"]["b"] == 4
    assert config["root"]["inner"]["c"] == 5
    assert config["root"]["inner"]["d"] == 6
    assert config["additional_settings"]["setting3"] == "additional_value3"
    assert config["additional_settings"]["setting_list"] == ["item_lol", 3, "item_lol"]


def test_params_config():
    config = get_config(params_config_path)

    # Check if the params configuration is composed correctly
    assert config["param1"] == "value1_overriden"
    assert config["param2"] == "value2"
    assert config["simple_params"]["root"]["a"] == 3
    assert config["simple_params"]["additional_settings"]["setting_list"] == [
        "item_lol",
        3,
        "item_lol",
    ]

    assert config["list2"] == [7, 8, 9]


def test_include_contexts():
    loader = DraconLoader(enable_interpolation=True)
    config_path = "pkg:dracon:tests/configs/incl_contexts.yaml"
    compres = compose_from_include_str(loader, config_path)
    print(f"Composition result: {compres}")
    config = loader.load_composition_result(compres)
    print(f"Config: {config}")

    assert config.fstem_basedir == "incl_contexts"
    assert config.fstem_subdir.fstem_here == "subincl"
    assert config.fstem_subdir.fstem_above.here == "fstem"

    assert config.avar_from_sub == 3
    assert config.bvar_from_sub == 2

    assert config.vars.a == 5
    assert config.vars.b == 2


def test_composition_through_interpolation():
    loader = DraconLoader(enable_interpolation=True)
    config = loader.load(f"pkg:{interp_config_path}")

    assert "default_settings" in config["base"]
    assert "param1" in config["base"]["default_settings"]
    assert "setting1" in config["base"]["default_settings"]

    assert config.base.file_stem == "interpolation"
    assert config.base.interpolated_addition == 4

    assert config.loaded_base.default_settings.param1 == "value1_overriden"

    assert type(config.int4) is int
    assert config.int4 == 4
    assert config.floatstr == 'float'

    assert config.nested_int4 == 4

    assert isinstance(config.tag_interp, float)
    assert config.tag_interp == 4.0

    assert config.interp_later == 5
    assert type(config.interp_later) is int

    assert config.interp_later_tag == 5.0
    assert type(config.interp_later_tag) is float

    # assert config.base.fstem == "fstem"


def test_override():
    loader = DraconLoader()
    config = loader.load(f"pkg:{override_config_path}")

    assert config["default_settings"]["setting1"] == "override_value1"
    assert config["default_settings"]["setting2"] == "default_value2"
    assert config["default_settings"]["setting3"] == "override_value3"
    assert config["default_settings"]["setting_list"] == ["override_item1", 3, "item_lol", "item4"]


class Person(BaseModel):
    name: str
    age: int


class WithResolvable(BaseModel):
    ned: Resolvable[Person]


def test_resolvable():
    loader = DraconLoader(
        enable_interpolation=True, context={"Person": Person, "WithResolvable": WithResolvable}
    )
    config = loader.load(f"pkg:{resolvable_config_path}")

    assert type(config.ned) is Resolvable
    ned = config.ned.resolve()
    assert type(ned) is Person
    assert ned.name == "Eddard"
    assert ned.age == 40


def test_include_interpolation():
    config = get_config('dracon:tests/configs/include_interpolations.yaml')
    # check that config.base is the base config:
    assert config.base.default_settings.setting1 == "default_value1"
    assert config.base.default_settings.simple_params.root.a == 3
    assert config.just_simple.setting3 == "additional_value3"
    assert config.just_simple.setting_list == ["item_lol", 3, "item_lol"]


if __name__ == "__main__":
    pytest.main([__file__])

================
File: dracon/tests/test_deepcopy.py
================
import pytest
from dataclasses import dataclass
from types import ModuleType, FunctionType
import sys
import marshal

# Import the _deepcopy function and its helpers
# Assuming they're in a file called deepcopy_utils.py
from dracon.utils import _deepcopy, dict_like, list_like


# Test Classes and Fixtures
@dataclass
class SimpleDataClass:
    x: int
    y: str


class CustomDeepCopy:
    def __init__(self, value):
        self.value = value

    def __deepcopy__(self, memo):
        return CustomDeepCopy(self.value * 2)


class CircularReference:
    def __init__(self):
        self.ref = None


class UncopiableObject:
    def __init__(self):
        self._module = sys

    def __deepcopy__(self, memo):
        raise NotImplementedError("Cannot deep copy this object")


@pytest.fixture
def circular_ref():
    obj1 = CircularReference()
    obj2 = CircularReference()
    obj1.ref = obj2
    obj2.ref = obj1
    return obj1


# Basic Type Tests
def test_simple_types():
    """Test copying of basic Python types."""
    assert _deepcopy(42) == 42
    assert _deepcopy(3.14) == 3.14
    assert _deepcopy("hello") == "hello"
    assert _deepcopy(True) == True
    assert _deepcopy(None) is None
    assert _deepcopy(complex(1, 2)) == complex(1, 2)


def test_marshalable_types():
    """Test if marshalable types are handled correctly."""
    data = {
        'list': [1, 2, 3],
        'tuple': (4, 5, 6),
        'dict': {'a': 1, 'b': 2},
        'set': {7, 8, 9},
        'frozenset': frozenset([10, 11, 12]),
    }

    copied = _deepcopy(data)
    assert copied == data
    assert copied is not data
    assert all(list(copied[k]) is not data[k] for k in data)


def test_nested_structures():
    """Test copying of nested data structures."""
    original = {'a': [1, 2, {'b': (3, 4, [5, 6])}], 'c': {7, 8, frozenset([9, 10])}}

    copied = _deepcopy(original)
    assert copied == original
    assert copied is not original
    assert copied['a'] is not original['a']
    assert copied['a'][2] is not original['a'][2]
    assert copied['a'][2]['b'] is not original['a'][2]['b']

    original['a'][2]['b'] = (0, 0, [0, 0])
    assert copied['a'][2]['b'] == (3, 4, [5, 6])


# Custom Object Tests
def test_dataclass_copy():
    """Test copying of dataclasses."""
    original = SimpleDataClass(x=1, y="test")
    copied = _deepcopy(original)

    assert copied == original
    assert copied is not original
    assert copied.x == 1
    assert copied.y == "test"


def test_custom_deepcopy_method():
    """Test that objects with __deepcopy__ are handled correctly."""
    original = CustomDeepCopy(5)
    copied = _deepcopy(original)

    assert copied is not original
    assert copied.value == 10  # Value should be doubled as per __deepcopy__ implementation


def test_circular_references(circular_ref):
    """Test handling of circular references."""
    copied = _deepcopy(circular_ref)

    assert copied is not circular_ref
    assert copied.ref is not circular_ref.ref
    assert copied.ref.ref is copied  # Circular reference should be preserved


# Edge Cases and Special Types
def test_module_type():
    """Test that ModuleType objects are returned as-is."""
    module = sys
    assert _deepcopy(module) is module


def test_function_type():
    """Test that function objects are returned as-is."""

    def test_func():
        pass

    assert _deepcopy(test_func) is test_func


def test_type_objects():
    """Test that type objects are returned as-is."""
    assert _deepcopy(str) is str
    assert _deepcopy(int) is int


# Performance Tests
def test_large_structure_performance():
    """Test performance with large data structures."""
    large_dict = {i: list(range(100)) for i in range(1000)}

    import time

    start_time = time.time()
    copied = _deepcopy(large_dict)
    end_time = time.time()

    assert copied == large_dict
    assert end_time - start_time < 1.0  # Should complete within 1 second


# Memory Tests
def test_memory_usage():
    """Test memory usage doesn't grow with repeated copies."""
    import psutil
    import os

    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss

    # Perform multiple copies
    data = {'a': [1] * 1000}
    for _ in range(1000):
        copied = _deepcopy(data)

    final_memory = process.memory_info().rss
    memory_increase = final_memory - initial_memory

    # Memory increase should be reasonable (less than 10MB)
    assert memory_increase < 10 * 1024 * 1024


if __name__ == '__main__':
    pytest.main([__file__])

================
File: dracon/tests/test_deferred.py
================
## {{{                          --     imports     --
import re
import pytest
import weakref
import types
from dracon import dump, loads
from dracon.loader import DraconLoader
from dracon.deferred import DeferredNode, make_deferred
from dracon.dracontainer import Dracontainer, Mapping, Sequence, resolve_all_lazy
from dracon.interpolation import InterpolationError, InterpolationMatch
from dracon.include import compose_from_include_str
from dracon.tests.test_config_composition import get_config, main_config_ok
from typing import Generic, TypeVar, Any, Optional, Annotated, cast, List
from pydantic import (
    BaseModel,
    field_validator,
    BeforeValidator,
    WrapValidator,
    AfterValidator,
    ConfigDict,
    Field,
)
import concurrent.futures
import threading
from dracon.interpolation import outermost_interpolation_exprs
from dracon.lazy import LazyInterpolable

from pydantic.dataclasses import dataclass
from dracon.keypath import KeyPath
from typing import Any, Dict, Callable, Optional, Tuple, List
import copy
from dracon.interpolation_utils import find_field_references
from dracon.utils import node_repr
from asteval import Interpreter

import pickle
import multiprocessing
import operator

##────────────────────────────────────────────────────────────────────────────}}}


# Session-scoped pool fixture
@pytest.fixture(scope="session")
def process_pool():
    """Create a process pool that's reused across tests"""
    pool = multiprocessing.Pool(processes=3)
    yield pool
    pool.close()
    pool.join()


class ClassA(BaseModel):
    index: int
    name: str = ''

    @property
    def name_index(self):
        return f"{self.index}: {self.name}"


def get_index(obj):
    return obj.index


def test_deferred_file():
    loader = DraconLoader(enable_interpolation=True, context={'var_a': 2})
    compres = compose_from_include_str(loader, "pkg:dracon:tests/configs/deferred.yaml")
    config = loader.load_composition_result(compres)
    assert config.a == 2

    assert type(config.main_content) is DeferredNode
    main_content = config.main_content.construct()
    main_config_ok(main_content)
    assert type(config.simple_merge) is DeferredNode

    sm = config.simple_merge.copy()

    simple_merge = sm.construct()
    assert simple_merge.root.a == 2
    assert simple_merge.additional_settings.setting_list[1] == 3

    sm2 = config.simple_merge.copy()
    simple_merge2 = sm2.construct(context={'var_a': 42})
    assert simple_merge2.root.a == 42
    assert simple_merge2.additional_settings.setting_list[1] == 3

    assert type(config.deferred_root) is DeferredNode
    dr = config.deferred_root.copy()
    deferred_root = dr.construct()
    assert deferred_root.ayy == "lmao"
    assert deferred_root.a == 2
    assert deferred_root.base.file_stem == "interpolation"
    instructs = deferred_root.instructs
    assert len(instructs.things) == 3
    instructs.things = [t.construct() for t in instructs.things]
    assert instructs.things[0].a == 1
    assert instructs.things[1].a == 2
    assert instructs.things[2].a == 3
    assert instructs.things[0].b == 2
    assert instructs.things[1].elt == 3
    assert instructs.things[2].fstem.here == "fstem"

    dr2 = config.deferred_root.copy()
    deferred_root2 = dr2.construct(context={'var_a': 42})
    assert deferred_root2.ayy == "lmao"
    assert deferred_root2.a == 42


def test_deferred_file_with_paths():
    config = get_config('dracon:tests/configs/deferred.yaml')
    assert type(config.deferred_root) is DeferredNode
    defroot_node = config.deferred_root.copy()
    deferred_root = defroot_node.construct(deferred_paths=['/loaded_base.default_settings'])
    assert deferred_root.ayy == "lmao"

    assert isinstance(deferred_root.loaded_base.default_settings, DeferredNode)
    deferred_settings = deferred_root.loaded_base.default_settings.construct()
    assert deferred_settings.simple_params.additional_settings.setting_list[1] == 3
    deferred_settings2 = deferred_root.loaded_base.default_settings.construct()
    assert deferred_settings2.simple_params.additional_settings.setting_list[1] == 3

    defroot_node2 = config.deferred_root.copy()
    deferred_root2 = defroot_node2.construct(deferred_paths=['/loaded_base.default_settings'])
    assert deferred_root2.ayy == "lmao"
    assert isinstance(deferred_root2.loaded_base.default_settings, DeferredNode)
    deferred_settings3 = deferred_root2.loaded_base.default_settings.construct()
    assert deferred_settings3.simple_params.additional_settings.setting_list[1] == 3
    deferred_settings4 = deferred_root2.loaded_base.default_settings.construct()
    assert deferred_settings4.simple_params.additional_settings.setting_list[1] == 3


def test_deferred_with_instructs():
    config = get_config('dracon:tests/configs/deferred.yaml')
    defroot_node = config.deferred_root.copy()
    deferred_root = defroot_node.construct(deferred_paths=['/instructs.things.*'])

    deferred_things = deferred_root.instructs.things
    assert len(deferred_things) == 3
    assert all(isinstance(t, DeferredNode) for t in deferred_things)

    defroot_node = config.deferred_root.copy()
    deferred_root = defroot_node.construct(
        deferred_paths=['/instructs.things.*'], context={'elements': [0, 1]}
    )

    deferred_things = deferred_root.instructs.things
    assert len(deferred_things) == 2
    assert all(isinstance(t, DeferredNode) for t in deferred_things)

    for j, thing in enumerate(deferred_things):
        t = thing.copy().construct(deferred_paths=['/fstem'], context={'elt_value': 42})
        assert t.a == j
        assert t.b == 2
        assert t.elt == 43
        assert isinstance(t.fstem, DeferredNode)
        fs = t.fstem.construct()
        assert fs.here == "fstem"


def test_deferred_context_1():
    yaml_content = """
    !set_default start: 3
    !set_default N : 2
    node: 
        !deferred
        !define some_var: ${[start + i for i in range(N)]}
        content:
            !each(val) ${some_var}:
                - !deferred
                  !define v: ${val}
                  value: ${val}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    node = config.node.copy()

    n1 = node.construct()
    # print(f"n1: {node_repr(node, context_paths=['some_var', 'val', 'v'])}")
    assert len(n1.content) == 2
    assert isinstance(n1.content[0], DeferredNode)
    assert isinstance(n1.content[1], DeferredNode)
    c0 = n1.content[0].construct()
    c1 = n1.content[1].construct()
    assert c0.value == 3
    assert c1.value == 4

    node = config.node.copy()
    n2 = node.construct(context={'start': 5, 'N': 3})
    assert len(n2.content) == 3
    c0 = n2.content[0].construct()
    c1 = n2.content[1].construct()
    c2 = n2.content[2].construct()
    assert c0.value == 5
    assert c1.value == 6
    assert c2.value == 7


def test_deferred_context_2():
    yaml_content = """
    !set_default var : 0
    value: ${var}
    deferred_node: !deferred
        value: ${var}
    """

    loader = DraconLoader(enable_interpolation=True, context={'var': 42})
    config = loader.loads(yaml_content)
    assert config.value == 42
    n = config.deferred_node.construct()
    assert n.value == 42


def test_deferred_context_3():
    yaml_content = """
    !set_default var : 0
    val: ${var}
    deferred_node: !deferred::clear_ctx=var
        !set_default var : 1
        val: ${var}
    """
    loader = DraconLoader(enable_interpolation=True, context={'var': 42})
    config = loader.loads(yaml_content)
    print(node_repr(config.deferred_node, context_paths=['/*'], enable_colors=True))
    assert config.val == 42
    n = config.deferred_node.construct()
    assert n.val == 1


def test_deferred_each_ctx():
    yaml_content = """
    !set_default varlist : ['value1', 'value2']
    list_content:
      !each(var) ${varlist}:
        - !deferred
          val: ${var}
          valist: ${varlist}
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    assert len(config.list_content) == 2
    assert all(isinstance(c, DeferredNode) for c in config.list_content)
    assert config.list_content[0].context['varlist'] is config.list_content[1].context['varlist']
    for i, c in enumerate(config.list_content):
        c = c.construct()
        assert c.val == f"value{i+1}"
        assert c.valist == ['value1', 'value2']


def test_deferred_context_4():
    yaml_content = """
    !set_default varlist : ['value1', 'value2']
    list_content:
      !each(var) ${varlist}:
        - !deferred::clear_ctx=varlist
          !set_default varlist : ['value3']
          val: ${var}
          valist: ${varlist}
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    print(node_repr(config, context_paths=['/*'], enable_colors=True))
    for i, c in enumerate(config.list_content):
        assert isinstance(c, DeferredNode)
        c = c.construct()
        assert c.val == f"value{i+1}"
        assert c.valist == ['value3']


def test_deferred_basic():
    yaml_content = """
    !define i42 : !int 42

    a_obj: !ClassA
        index: &aid ${i42}
        name: oldname
        <<{<+}: 
            name: "new_name ${&aid}"

    nested: !deferred
        !define aid: ${get_index(construct(&/a_obj))}
        a_index: ${aid}
        aname: ${&/a_obj.name}
        constructed_nameindex: ${construct(&/a_obj).name_index}

    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    loader.yaml.representer.full_module_path = False
    loader.context['get_index'] = get_index
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert isinstance(config.a_obj, ClassA)
    assert config['a_obj'].index == 42
    assert config['a_obj'].name == "new_name 42"

    assert type(config['nested']) is DeferredNode

    nested = config.nested.construct()

    assert nested.a_index == 42
    assert nested.aname == "new_name 42"
    assert nested.constructed_nameindex == "42: new_name 42"


def test_deferred_explicit():
    yaml_content = """
    !define i42 : !int 42

    a_obj: !ClassA &ao
        index: &aid ${i42}
        name: oldname
        <<{<+}: 
            name: "new_name ${&aid}"


    b_obj: !deferred:ClassA &bo
        index: &bid ${int(i42) - 10}
        name: oldname
        <<{<+}: 
            name: "new_name ${&bid}"

    nested:
        !define aid: ${get_index(construct(&/a_obj)) + $CONSTANT}
        a_index: ${aid}
        aname: ${&/a_obj.name}
        constructed_nameindex: ${construct(&/a_obj).name_index}
        !define ao: ${&/a_obj}
        !define bo: ${&/b_obj} # required to go through a reference when pointing to a deferred node
        obj2:
            <<: !include ao
        obj3: !include $ao
        obj4: !include $bo


    """

    loader = DraconLoader(
        enable_interpolation=True, context={'ClassA': ClassA}, deferred_paths=['/nested']
    )
    loader.yaml.representer.full_module_path = False
    config = loader.loads(yaml_content)
    resolve_all_lazy(config)

    assert isinstance(config.a_obj, ClassA)
    assert config['a_obj'].index == 42
    assert config['a_obj'].name == "new_name 42"

    assert type(config['nested']) is DeferredNode

    config.nested.update_context({'get_index': get_index, '$CONSTANT': 10})
    nested = config.nested.construct()
    resolve_all_lazy(nested)

    assert nested.a_index == 52
    assert nested.aname == "new_name 42"
    assert nested.constructed_nameindex == "42: new_name 42"

    assert isinstance(config.b_obj, DeferredNode)
    b_obj = config.b_obj.construct()
    resolve_all_lazy(b_obj)
    assert isinstance(b_obj, ClassA)
    assert b_obj.index == 32
    assert b_obj.name == "new_name 32"

    print(f"{config.a_obj=}, {nested.obj2=}")
    # here, nested.obj2 is a mapping... it should be a ClassA instance

    assert nested.obj2 == config.a_obj
    assert nested.obj3 == config.a_obj
    assert isinstance(nested.obj4, DeferredNode)
    assert nested.obj4.construct() == b_obj


def process_deferred_node(node_data: Dict[str, Any]) -> Any:
    """Helper function for multiprocessing tests"""
    pickled_node, context = node_data
    node = pickle.loads(pickled_node)
    if context:
        node.update_context(context)
    return node.construct()


def test_deferred_node_pickling():
    """Test pickling and unpickling of DeferredNode"""
    yaml_content = """
    !define i42 : !int 42

    nested: !deferred
        !define aid: ${get_index(construct(&/a_obj))}
        a_index: ${aid}
        aname: ${&/a_obj.name}
        constructed_nameindex: ${construct(&/a_obj).name_index}

    a_obj: !ClassA
        index: &aid ${i42}
        name: oldname
        <<{<+}: 
            name: "new_name ${&aid}"
    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    loader.context['get_index'] = get_index
    config = loader.loads(yaml_content)

    # Pickle the entire config
    pickled_config = pickle.dumps(config)
    unpickled_config = pickle.loads(pickled_config)

    # Verify the deferred node behavior is preserved
    unpickled_config.resolve_all_lazy()
    nested = unpickled_config.nested.construct()

    assert nested.a_index == 42
    assert nested.aname == "new_name 42"
    assert nested.constructed_nameindex == "42: new_name 42"


def doublex(x):
    return x * 2


def double(x: int) -> int:
    return x * 2


def add_ten(x: int) -> int:
    return x + 10


def square(x: int) -> int:
    return x**2


def test_deferred_node_context_pickling():
    """Test pickling DeferredNode with context updates"""
    yaml_content = """
    nested: !deferred
        value: ${VALUE}
        computed: ${COMPUTE(10)}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)

    # Pickle the node
    pickled_node = pickle.dumps(config.nested)
    unpickled_node = pickle.loads(pickled_node)

    # Update context after unpickling
    context = {'VALUE': 42, 'COMPUTE': doublex}
    unpickled_node.update_context(context)

    result = unpickled_node.construct()
    assert result.value == 42
    assert result.computed == 20


def test_parallel_deferred_node_processing(process_pool):
    """Test processing multiple deferred nodes in parallel"""
    yaml_content = """
    nodes:
        node1: !deferred
            value: ${VALUE + 1}
        node2: !deferred
            value: ${VALUE + 2}
        node3: !deferred
            value: ${VALUE + 3}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)

    nodes = [
        (pickle.dumps(config.nodes.node1), {'VALUE': 10}),
        (pickle.dumps(config.nodes.node2), {'VALUE': 20}),
        (pickle.dumps(config.nodes.node3), {'VALUE': 30}),
    ]

    results = process_pool.map(process_deferred_node, nodes)

    assert [r.value for r in results] == [11, 22, 33]


def construct_deferred(node: DeferredNode) -> Any:
    print(f"Constructing {node}")
    print(f"{node.context=}")
    return node.construct()


def test_parallel_deferred_nodes(process_pool):
    """Test processing multiple deferred nodes in parallel"""
    yaml_content = """
    nodes:
        !each(val) ${VALUES}:
            - !deferred
              value: ${val + 1}
              ainst: !ClassA
                index: ${int(val) + 1}
                name: "Item ${int(val) + 1}"
    """

    loader = DraconLoader(
        enable_interpolation=True,
        context={
            'VALUES': [10, 20, 30],
            'ClassA': ClassA,
        },
    )
    config = loader.loads(yaml_content)

    nodes = config.nodes
    print(f"{nodes=}")

    assert len(nodes) == 3

    results = process_pool.map(construct_deferred, nodes)

    assert [r.value for r in results] == [11, 21, 31]
    assert all(isinstance(r.ainst, ClassA) for r in results)


def test_complex_deferred_node_pickling():
    """Test pickling complex deferred nodes with cross-references"""
    yaml_content = """
    !define i42 : !int 42

    a_obj: !ClassA &ao
        index: &aid ${i42}
        name: oldname
        <<{<+}: 
            name: "new_name ${&aid}"

    b_obj: !deferred:ClassA &bo
        index: &bid ${int(i42) - 10}
        name: oldname
        <<{<+}: 
            name: "new_name ${&bid}"

    nested:
        !define aid: ${get_index(construct(&/a_obj)) + $CONSTANT}
        a_index: ${aid}
        aname: ${&/a_obj.name}
        constructed_nameindex: ${construct(&/a_obj).name_index}
        !define ao: ${&/a_obj}
        !define bo: ${&/b_obj}
        obj2: !include ao
        obj3: !include $ao
        obj4: !include $bo
    """

    loader = DraconLoader(
        enable_interpolation=True, context={'ClassA': ClassA}, deferred_paths=['/nested']
    )
    config = loader.loads(yaml_content)

    # Pickle entire config
    pickled_config = pickle.dumps(config)
    unpickled_config = pickle.loads(pickled_config)

    # Update context and resolve
    unpickled_config.nested.update_context({'get_index': get_index, '$CONSTANT': 10})

    nested = unpickled_config.nested.construct()
    resolve_all_lazy(nested)

    assert nested.a_index == 52
    assert nested.aname == "new_name 42"
    assert nested.constructed_nameindex == "42: new_name 42"

    b_obj = unpickled_config.b_obj.construct()
    resolve_all_lazy(b_obj)
    assert isinstance(b_obj, ClassA)
    assert b_obj.index == 32
    assert b_obj.name == "new_name 32"


def test_parallel_deferred_class_instantiation(process_pool):
    """Test parallel instantiation of deferred class objects"""
    yaml_content = """
    listitems:
        obj1: !deferred:ClassA
            index: ${BASE + 1}
            name: "Item ${BASE + 1}"
        obj2: !deferred:ClassA
            index: ${BASE + 2}
            name: "Item ${BASE + 2}"
        obj3: !deferred:ClassA
            index: ${BASE + 3}
            name: "Item ${BASE + 3}"
    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    config = loader.loads(yaml_content)

    # Prepare nodes for parallel processing
    nodes = [
        (pickle.dumps(node), {'BASE': i * 10})
        for i, node in enumerate(
            [config.listitems.obj1, config.listitems.obj2, config.listitems.obj3]
        )
    ]

    results = process_pool.map(process_deferred_node, nodes)

    for i, result in enumerate(results):
        assert isinstance(result, ClassA)
        base = i * 10
        assert result.index == base + (i + 1)
        assert result.name == f"Item {base + (i + 1)}"


def test_deferred_node_thread_safety():
    """Test thread-safe processing of deferred nodes"""
    yaml_content = """
    node: !deferred
        counter: ${COUNTER}
        value: ${VALUE}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)

    # Use threading.Lock for thread-safe counter increment
    counter_lock = threading.Lock()
    counter = 0

    def process_node(value):
        nonlocal counter
        with counter_lock:
            counter += 1
            current_counter = counter

        node = pickle.loads(pickle.dumps(config.node))
        node.update_context({'COUNTER': current_counter, 'VALUE': value})
        return node.construct()

    # Process in multiple threads
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(process_node, i) for i in range(10)]
        results = [f.result() for f in futures]

    # Verify results
    counters = set(r.counter for r in results)
    assert len(counters) == 10, f"Expected 10 unique counters, got {len(counters)}: {counters}"
    assert min(counters) == 1
    assert max(counters) == 10
    assert sorted(r.value for r in results) == list(range(10))


def process_counter_node(args: Tuple[int, int]) -> Any:
    """Process a node with counter and value"""
    value, counter = args
    yaml_content = """
    node: !deferred
        counter: ${COUNTER}
        value: ${VALUE}
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    node = config.node
    node.update_context({'COUNTER': counter, 'VALUE': value})
    return node.construct()


def test_deferred_node_process_safety(process_pool):
    """Test process-safe processing of deferred nodes using a shared counter"""
    # Process in multiple processes with pre-assigned counters
    args = [(i, i + 1) for i in range(10)]  # (value, counter)
    results = process_pool.map(process_counter_node, args)

    # Verify results
    counters = set(r.counter for r in results)
    assert len(counters) == 10, f"Expected 10 unique counters, got {len(counters)}: {counters}"
    assert min(counters) == 1
    assert max(counters) == 10
    assert sorted(r.value for r in results) == list(range(10))


def test_process_pool_reuse(process_pool):
    """Test reusing process pool for multiple deferred node operations"""
    yaml_content = """
    node: !deferred
        operation_name: ${OPERATION_NAME}
        input: ${INPUT}
        result: ${OPERATION(INPUT)}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)

    # Use named functions instead of lambdas
    operations = [(double, "double"), (add_ten, "add_ten"), (square, "square")]

    def create_node_data(op_func, op_name, input_value):
        return (
            pickle.dumps(config.node),
            {'OPERATION': op_func, 'INPUT': input_value, 'OPERATION_NAME': op_name},
        )

    # First batch
    nodes_data = [create_node_data(op_func, op_name, 5) for op_func, op_name in operations]
    results1 = process_pool.map(process_deferred_node, nodes_data)

    # Second batch with different input
    nodes_data = [create_node_data(op_func, op_name, 10) for op_func, op_name in operations]
    results2 = process_pool.map(process_deferred_node, nodes_data)

    # Verify first batch
    assert results1[0].result == 10  # 5 * 2
    assert results1[1].result == 15  # 5 + 10
    assert results1[2].result == 25  # 5 ** 2

    # Verify second batch
    assert results2[0].result == 20  # 10 * 2
    assert results2[1].result == 20  # 10 + 10
    assert results2[2].result == 100  # 10 ** 2


def unpack_call(func, args):
    return func(*args)


def test_complex_operations(process_pool):
    """Test processing deferred nodes with more complex operations"""
    yaml_content = """
    node: !deferred
        input: ${INPUT}
        operation: ${OPERATION_NAME}
        args: ${ARGS}
        result: ${unpack_call(OPERATION, ARGS)}
    """

    loader = DraconLoader(enable_interpolation=True, context={'unpack_call': unpack_call})
    config = loader.loads(yaml_content)

    # Test data using built-in functions and operators
    test_cases = [
        (operator.add, "add", (5, 3)),
        (operator.mul, "multiply", (4, 6)),
        (operator.truediv, "divide", (10, 2)),
        (max, "max", (7, 12, 3)),
        (min, "min", (8, 2, 5)),
    ]

    def create_node_data(op_func, op_name, args):
        return (
            pickle.dumps(config.node),
            {
                'OPERATION': op_func,
                'OPERATION_NAME': op_name,
                'ARGS': args,
                'INPUT': args[0],  # First arg as input for reference
            },
        )

    nodes_data = [create_node_data(op_func, op_name, args) for op_func, op_name, args in test_cases]
    results = process_pool.map(process_deferred_node, nodes_data)

    # Verify results
    assert results[0].result == 8  # 5 + 3
    assert results[1].result == 24  # 4 * 6
    assert results[2].result == 5.0  # 10 / 2
    assert results[3].result == 12  # max(7, 12, 3)
    assert results[4].result == 2  # min(8, 2, 5)


def test_builtin_operations(process_pool):
    """Test processing deferred nodes with built-in operations"""
    yaml_content = """
    node: !deferred
        input: ${INPUT}
        processed: ${PROCESS(INPUT)}
        constant: constant
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)

    # Use built-in functions that are always picklable
    processors = [
        (abs, "abs", -5),
        (str.upper, "upper", "hello"),
        (len, "len", [1, 2, 3]),
        (sorted, "sorted", [3, 1, 4, 1, 5]),
        (bool, "bool", 1),
    ]

    def create_node_data(proc_func, proc_name, test_input):
        return (pickle.dumps(config.node), {'PROCESS': proc_func, 'INPUT': test_input})

    nodes_data = [
        create_node_data(proc_func, proc_name, test_input)
        for proc_func, proc_name, test_input in processors
    ]
    results = process_pool.map(process_deferred_node, nodes_data)

    # Verify results
    assert results[0].processed == 5  # abs(-5)
    assert results[1].processed == "HELLO"  # "hello".upper()
    assert results[2].processed == 3  # len([1, 2, 3])
    assert results[3].processed == [1, 1, 3, 4, 5]  # sorted([3, 1, 4, 1, 5])
    assert results[4].processed == True  # bool(1)def test_large_parallel_processing():

    assert all(r.constant == "constant" for r in results)


def test_large_parallel_processing(process_pool):
    """Test processing a large number of deferred nodes in parallel"""
    yaml_content = """
    node: !deferred
        input: ${INPUT}
        result: ${INPUT * 2}
        batch: ${BATCH}
        classAinst: !ClassA
            index: ${INPUT}
            name: "Item ${INPUT}"
    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    config = loader.loads(yaml_content)

    # Create a large number of nodes
    num_nodes = 100
    batch_size = 10
    nodes_data = []

    for batch in range(num_nodes // batch_size):
        for i in range(batch_size):
            value = batch * batch_size + i
            nodes_data.append((pickle.dumps(config.node), {'INPUT': value, 'BATCH': batch}))

    results = process_pool.map(process_deferred_node, nodes_data, chunksize=10)

    # Verify results
    for i, result in enumerate(results):
        assert result.input == i
        assert result.result == i * 2
        assert result.batch == i // batch_size
        assert isinstance(result.classAinst, ClassA)
        assert result.classAinst.index == i
        assert result.classAinst.name == f"Item {i}"


def test_make_deferred():
    inode = make_deferred(42)
    assert inode.construct() == 42

    snode = make_deferred("hello")
    assert snode.construct() == "hello"


def test_deferred_node_method_pickling():
    """Test that bound methods of DeferredNode can be pickled and unpickled."""
    # Create a deferred node
    node = make_deferred(42)

    try:
        # Try to pickle the whole node with its methods
        pickled_node = pickle.dumps(node)
        unpickled_node = pickle.loads(pickled_node)

        # This is likely where it will fail
        result = unpickled_node.construct()
        assert result == 42
    except Exception as e:
        pytest.fail(f"Failed to pickle/unpickle DeferredNode methods: {e}")


def test_check_weakrefs_in_deferred_node():
    """Check if DeferredNode contains weakref objects that could cause serialization issues."""
    node = make_deferred(42)

    def find_weakrefs(obj, path="obj", seen=None):
        if seen is None:
            seen = set()

        # Skip if we've seen this object or it's None
        if id(obj) in seen or obj is None:
            return []

        seen.add(id(obj))
        weakrefs_found = []

        # Check if this object is a weakref
        if isinstance(obj, weakref.ReferenceType):
            weakrefs_found.append((path, obj))

        # Check if object is a bound method (which often contain weakrefs)
        if isinstance(obj, types.MethodType):
            for attr_name in ['__self__', '__func__']:
                if hasattr(obj, attr_name):
                    attr_value = getattr(obj, attr_name)
                    weakrefs_found.extend(find_weakrefs(attr_value, f"{path}.{attr_name}", seen))

        # Check other attributes
        if hasattr(obj, "__dict__"):
            for attr_name, attr_value in obj.__dict__.items():
                if attr_name.startswith("__"):
                    continue
                weakrefs_found.extend(find_weakrefs(attr_value, f"{path}.{attr_name}", seen))

        # Check elements of sequences
        if isinstance(obj, (list, tuple)) and not isinstance(obj, str):
            for i, item in enumerate(obj):
                weakrefs_found.extend(find_weakrefs(item, f"{path}[{i}]", seen))

        # Check keys and values of dictionaries
        if isinstance(obj, dict):
            for k, v in obj.items():
                key_str = str(k)[:20]  # Truncate long keys
                weakrefs_found.extend(find_weakrefs(v, f"{path}['{key_str}']", seen))

        return weakrefs_found

    weakrefs = find_weakrefs(node)

    # Print all found weakrefs for debugging
    if weakrefs:
        for path, ref in weakrefs:
            print(f"Found weakref at {path}: {ref}")

    # The test should fail if weakrefs are found
    assert not weakrefs, f"Found {len(weakrefs)} weakref objects in DeferredNode"


def test_full_composition_serialization():
    """Test that _full_composition can be properly serialized and deserialized."""
    node = make_deferred(42)

    # Test serializing the full composition
    try:
        # First check if _full_composition exists
        assert node._full_composition is not None, "Node has no _full_composition"

        # Try to pickle just the _full_composition
        pickled_comp = pickle.dumps(node._full_composition)
        unpickled_comp = pickle.loads(pickled_comp)

        # Check if essential attributes were preserved
        assert hasattr(unpickled_comp, 'root')
    except Exception as e:
        print(f"Failed to serialize _full_composition: {e}")
        # This might be expected to fail
        pass


def test_loader_serialization():
    """Test that _loader can be properly serialized and deserialized."""
    node = make_deferred(42)

    # Test serializing the loader
    try:
        # First check if _loader exists
        assert node._loader is not None, "Node has no _loader"

        # Try to pickle just the _loader
        pickled_loader = pickle.dumps(node._loader)
        unpickled_loader = pickle.loads(pickled_loader)

        # Check if essential methods were preserved
        assert hasattr(unpickled_loader, 'load')
    except Exception as e:
        print(f"Failed to serialize _loader: {e}")
        # This might be expected to fail
        pass


def test_large_context_not_duplicated():
    """Test that large context objects aren't duplicated."""
    from dracon.deferred import DeferredNode, make_deferred
    from dracon.nodes import DraconMappingNode, DraconScalarNode
    from dracon.keypath import ROOTPATH
    import sys

    large_data = [i for i in range(100)]
    ref_count_before = sys.getrefcount(large_data)

    context = {"large_data": large_data}

    nodes = []
    for i in range(10):
        node = make_deferred(f"test{i}", context=context)
        nodes.append(node)

    for node in nodes:
        assert node.context["large_data"] is large_data

    ref_count_after = sys.getrefcount(large_data)
    assert ref_count_after <= ref_count_before + len(nodes) + 3  # Allow a few extra references

    result = nodes[0].construct()

    assert large_data[0] == 0
    assert large_data[-1] == 99

================
File: dracon/tests/test_edge_cases.py
================
from dracon import DraconLoader, resolve_all_lazy


def test_edge_cases():
    loader = DraconLoader(enable_interpolation=True)
    config = loader.load('dracon/tests/configs/edge_cases.yaml')

    print(f"Config: {config}")
    resolve_all_lazy(config)

    assert config["dotted.keys"]["nested.value"] == "simple_value"
    assert config["dotted.keys"]["another.dotted.key"] == "another_value"

    assert config.value == "not_an_internal_value"
    assert config.context == "not_an_internal_context"
    assert config.tag == "not_an_internal_tag"
    assert config.anchor == "not_an_internal_anchor"

    assert config.each_with_dots["item.1"] == "value_1"
    assert config.each_with_dots["item.2"] == "value_2"
    assert config.each_with_dots["item.3"] == "value_3"
    assert config.each_with_dots["nested.item.1"] == "nested_value_1"
    assert config.each_with_dots["nested.item.2"] == "nested_value_2"
    assert config.each_with_dots["nested.item.3"] == "nested_value_3"

    assert config.nested.level1["dotted.key"] == "deep_value"
    assert config.nested.array[0]["key.with.dots"] == "array_value1"
    assert config.nested.array[1]["key.with.dots"] == "array_value2"

    assert config.reference_test.simple_ref == "simple_value"
    assert config.reference_test.nested_ref == "deep_value"

    assert config["interpolated.keys.dynamic"].value == "interpolated_value"

    assert config["base.with.dots"].key1 == "base_value1"
    assert config["base.with.dots"].key2 == "override_value2"
    assert config["base.with.dots"]["nested.key"] == "override_nested"

    # deferred_node = config["deferred.node"]
    # print(f"Deferred type: {type(deferred_node)}")
    # constructed = deferred_node.construct()
    # print(f"Constructed: {constructed}")
    # assert constructed["dotted.key"] == "deferred_value"
    # assert constructed.reference == "simple_value"

    assert config.complex["first.level"]["inner.value"] == "complex_inner_value"
    # assert config.complex["first.level"].reference == "complex_inner_value"

================
File: dracon/tests/test_interpolate.py
================
## {{{                          --     imports     --
from dracon.loader import DraconLoader
from dracon.dracontainer import Mapping
from pydantic import BaseModel
from dracon.interpolation import outermost_interpolation_exprs
from dracon.lazy import LazyInterpolable
from dracon.keypath import KeyPath
import copy
from dracon.interpolation_utils import find_field_references
from dracon.include import compose_from_include_str
import pytest
from dracon.keypath import ROOTPATH
from dracon.dracontainer import resolve_all_lazy
##────────────────────────────────────────────────────────────────────────────}}}


class ClassA(BaseModel):
    index: int
    name: str = ''

    @property
    def name_index(self):
        return f"{self.index}: {self.name}"


class ClassB(BaseModel):
    attr1: str
    attr2: int
    attrA: ClassA


def test_dict():
    kp = find_field_references(
        r"@/name.greeting..back+2 + / @path.to.list[2] = @haha./../p\[3{]"
    )  # -> [ @/name.greeting..back ,  @path.to.list , @haha./../p\[3 ]

    assert len(kp) == 3
    assert kp[0].start == 0
    assert kp[0].end == 21
    assert kp[0].expr == "/name.greeting..back"

    assert kp[1].start == 28
    assert kp[1].end == 41
    assert kp[1].expr == "path.to.list"

    assert kp[2].start == 47
    assert kp[2].end == 61
    assert kp[2].expr == r"haha./../p[3"

    test_expr2 = "${@/name\\.greeting..back+2 + / @path.${'haha' + @inner.match }to.list\\[2] } = @haha./../p\\[3{] + ${2+2}"
    test_expr_paren = "$(@/name\\.greeting..back+2 + / @path.${'haha' + @inner.match }to.list\\[2] ) = @haha./../p\\[3{] + ${2+2}"

    interp_matches = outermost_interpolation_exprs(test_expr2)
    assert len(interp_matches) == 2
    assert interp_matches[0].start == 0

    interp_matches = outermost_interpolation_exprs(test_expr_paren)
    assert len(interp_matches) == 2
    assert interp_matches[0].start == 0

    obj = {
        "name": "John",
        "n": 5,
        "greetingroot": "Hello, ${@name}!",
        'nested': {
            'inner': {
                'greeting': 'greetings, ${@/name}!',
                'list': '${[@/name + "_" + str(i) for i in range(@/n)]}',
                'ref': '${@/greetingroot}',
            }
        },
    }

    dump = DraconLoader().dump(obj)
    loader = DraconLoader(enable_interpolation=True)
    loaded = loader.loads(dump)
    loaded.resolve_all_lazy()

    assert loaded.name == 'John'
    assert loaded.greetingroot == 'Hello, John!'
    assert loaded.nested.inner.greeting == 'greetings, John!'
    assert loaded.nested.inner.ref == 'Hello, John!'
    assert loaded.nested.inner.list == ['John_0', 'John_1', 'John_2', 'John_3', 'John_4']


def test_lazy():
    obj = {
        "name": "John",
        "n": 5,
        "greetingroot": "Hello, ${@name}!",
        "quatre": "${2+2}",
        'nested': {
            'inner': {
                'greeting': 'greetings, ${"dear "+  @/name}!',
                'list': '${[@/name + "_" + str(i) for i in range(@....n)]}',
                'ref': '${@/greetingroot}',
            }
        },
    }

    loader = DraconLoader(enable_interpolation=True)
    ymldump = loader.dump(obj)

    loaded = loader.loads(ymldump)
    loaded_copy = copy.copy(loaded)
    loaded_deepcopy = copy.deepcopy(loaded)

    assert isinstance(loaded, Mapping)
    assert isinstance(loaded_copy, Mapping)
    assert isinstance(loaded_deepcopy, Mapping)

    assert loaded.name == 'John'
    assert loaded.quatre == 4

    assert isinstance(loaded._data['greetingroot'], LazyInterpolable)
    assert loaded.greetingroot == 'Hello, John!'
    assert isinstance(loaded._data['greetingroot'], str)
    assert isinstance(loaded_copy._data['greetingroot'], str)
    assert isinstance(loaded_deepcopy._data['greetingroot'], LazyInterpolable)

    assert loaded.nested.inner.greeting == 'greetings, dear John!'
    assert loaded.nested.inner.ref == 'Hello, John!'
    assert loaded.nested.inner.list == ['John_0', 'John_1', 'John_2', 'John_3', 'John_4']

    assert loaded.nested.inner._dracon_current_path == KeyPath('/nested.inner')

    newstr = '${@/name + " " + @/nested.inner.greeting}'
    loaded.nested.inner['new'] = newstr
    assert loaded.nested.inner.new == newstr
    loaded.nested.inner['new'] = LazyInterpolable(loaded.nested.inner['new'])
    assert loaded.nested.inner.new == 'John greetings, dear John!'


def test_ampersand_interpolation_simple():
    yaml_content = """
    base: &base_anchor
      key1: value1
      key2: value2

    config:
      key3: ${&/base}
      key4: ${&/base.key1}
      full: ${&base_anchor}
      key1_amp: ${&base_anchor.key1}
      key1_at: ${@/base.key1}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)

    config_copy = copy.deepcopy(config)

    assert config['config']['key3'] == {'key1': 'value1', 'key2': 'value2'}
    assert config['config']['full'] == {'key1': 'value1', 'key2': 'value2'}
    assert config['config']['key4'] == 'value1'
    assert config['config']['key1_amp'] == 'value1'
    assert config['config']['key1_at'] == 'value1'

    config_copy.base.key1 = 'new_value1'
    assert config_copy['config']['key3'] == {'key1': 'value1', 'key2': 'value2'}
    assert config_copy['config']['full'] == {'key1': 'value1', 'key2': 'value2'}
    assert config_copy['config']['key4'] == 'value1'
    assert config_copy['config']['key1_amp'] == 'value1'
    assert config_copy['config']['key1_at'] == 'new_value1'


# 6.5
# removed deepcopy in merge -> 4.6


@pytest.mark.parametrize('n', range(4))
def test_recursive_interpolation(n):
    yaml_content = """
    base: &base_anchor
        key1: value1
        key2: ${@key1}
        key3: ${&key2}
        key4: ${@key3}
        key5: ${&key4}
        key6: ${&key5}
        key7: ${&base_anchor.key6}
        key8: ${@/base.key7}

    base2: ${&base_anchor}
    base3: ${&base2}
    base4: ${&/base3}
    base5: ${@base4}
    base6: ${@/base}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert config['base'] == {
        'key1': 'value1',
        'key2': 'value1',
        'key3': 'value1',
        'key4': 'value1',
        'key5': 'value1',
        'key6': 'value1',
        'key7': 'value1',
        'key8': 'value1',
    }

    print(f"config: {config}")

    assert config['base2'] == config['base']
    assert config['base3'] == config['base']
    assert config['base4'] == config['base']
    assert config['base5'] == config['base']
    assert config['base6'] == config['base']


@pytest.mark.parametrize('n', range(15))
def test_ampersand_interpolation_complex(n):
    yaml_content = """
        __dracon__:
          simple_obj: &smpl
            index: ${i + 1}
            name: "Name ${&index}"

        all_objs: ${[&/__dracon__.simple_obj:i=j for j in range(5)]}
        all_objs_by_anchor: ${[&smpl:i=i for i in range(5)]}
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()
    assert '__dracon__' not in config
    print(f"config: {config}")

    assert config['all_objs'] == [
        {'index': 1, 'name': 'Name 1'},
        {'index': 2, 'name': 'Name 2'},
        {'index': 3, 'name': 'Name 3'},
        {'index': 4, 'name': 'Name 4'},
        {'index': 5, 'name': 'Name 5'},
    ]

    assert config['all_objs_by_anchor'] == config['all_objs']


@pytest.mark.parametrize('n', range(10))
def test_ampersand_interpolation_complex_copy(n):
    yaml_content = """
        __dracon__:
          simple_obj: &smpl
            index: ${i + 1}
            name: "Name ${&index}"

        all_objs: ${[&/__dracon__.simple_obj:i=j for j in range(5)]}
        all_objs_by_anchor: ${[&smpl:i=i for i in range(5)]}
    """

    loader = DraconLoader(enable_interpolation=True)
    comp = loader.compose_config_from_str(yaml_content)

    from dracon.utils import deepcopy

    loader_copy = deepcopy(loader)
    comp_copy = deepcopy(comp)

    config = loader.load_composition_result(comp)
    config_copy = loader_copy.load_composition_result(comp_copy)

    config.resolve_all_lazy()
    config_copy.resolve_all_lazy()

    print(f"config: {config}")

    assert '__dracon__' not in config
    assert '__dracon__' not in config_copy
    expected = [
        {'index': 1, 'name': 'Name 1'},
        {'index': 2, 'name': 'Name 2'},
        {'index': 3, 'name': 'Name 3'},
        {'index': 4, 'name': 'Name 4'},
        {'index': 5, 'name': 'Name 5'},
    ]

    assert config['all_objs'] == expected
    assert config_copy['all_objs'] == expected

    assert config['all_objs_by_anchor'] == expected
    assert config_copy['all_objs_by_anchor'] == expected


def test_obj_references():
    yaml_content = """
    __dracon__:
        simple_obj: &smpl !ClassA
            index: ${i + 1}
            name: "Name ${@index}"

    obj4: &o4 ${&smpl:i=3}
    prop4: ${@obj4.name_index}

    as_ampersand_anchor: ${[&smpl:i=i for i in range(5)]}
    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    loader.yaml.representer.full_module_path = False
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert '__dracon__' not in config
    assert isinstance(config['obj4'], ClassA)
    assert config['obj4'].index == 4
    assert config['obj4'].name == 'Name 4'
    assert config['prop4'] == '4: Name 4'

    assert config['as_ampersand_anchor'] == [
        ClassA(index=1, name='Name 1'),
        ClassA(index=2, name='Name 2'),
        ClassA(index=3, name='Name 3'),
        ClassA(index=4, name='Name 4'),
        ClassA(index=5, name='Name 5'),
    ]


def test_instruction_define():
    yaml_content = """
    !define i : ${4}

    a: ${i + 2}
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert config.a == 6


def test_instruction_each_simple():
    yaml_content = """
    ilist:
        !each(e) ${list(range(5))}:
            - ${e}
    """
    loader = DraconLoader(enable_interpolation=True)
    composed = loader.compose_config_from_str(yaml_content)

    config = loader.loads(yaml_content)

    assert '__dracon__' not in config
    assert len(config['ilist']) == 5

    config.resolve_all_lazy()


def test_instruction_if_true():
    yaml_content = """
    !if 1:
      a: 1
      b: 2
      !if ${True}:
        c: 3
        !if null:
            d: 4
        !if 1:
            e: 5
        !if true:
            f: 6
        !if false:
            g: 7
        !if ${False}:
            h: 8
        !if ${True }:
            i: 9
        !if 0:
            j: 10
        !if 2:
            k: 11
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    print(config)
    assert config == {
        'a': 1,
        'b': 2,
        'c': 3,
        'e': 5,
        'f': 6,
        'i': 9,
        'k': 11,
    }


def test_instruction_if_false():
    yaml_content = """
    !if ${False}:
      a: 1
      b: 2
    c: 3
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert 'a' not in config
    assert 'b' not in config
    assert 'c' in config
    assert config.c == 3


def test_instruction_if_inside_each():
    yaml_content = """
    numbers:
      !each(n) ${list(range(5))}:
        - !if ${(n % 2) == 0}:
            number: ${n}
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    expected_numbers = [{'number': 0}, {'number': 2}, {'number': 4}]
    assert config.numbers == expected_numbers


def test_instruction_if_sequence():
    yaml_content = """
    !define threshold: ${10}
    !define val: ${15}
    list:
        - !if ${val > threshold}: "greater"
        - !if ${val > threshold}:
            a: 1
            !if 1:
                b: 2
        - !if ${val <= threshold}: "lessthan"
        - other
    """

    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert config.list == [
        "greater",
        {'a': 1, 'b': 2},
        "other",
    ]


def test_instruction_if_complex_expression_true():
    yaml_content = """
    !define threshold: ${10}
    !define value: ${15}
    !if ${value > threshold}:
      result: "greater"
    !if ${value <= threshold}:
      result: "lessthan"
    """
    loader = DraconLoader(enable_interpolation=True)
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert 'result' in config
    assert config.result == "greater"


def test_instruction_if_with_external_function():
    yaml_content = """
    !define is_even: ${is_even_function(4)}
    !if ${is_even}:
      number_type: "Even"
    !if ${not is_even}:
      number_type: "Odd"
    """

    def is_even_function(n):
        return n % 2 == 0

    loader = DraconLoader(enable_interpolation=True, context={'is_even_function': is_even_function})
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert 'number_type' in config
    assert config.number_type == "Even"


def test_obj_references_instruct():
    yaml_content = """
    __dracon__:
        simple_obj: &smpl !ClassA
            index: ${i + 1}
            name: "Name ${@index}"

    obj4: &o4 ${&smpl:i=3}
    prop4: ${@obj4.name_index}

    # using each + define
    as_ampersand_anchor:
        !each(i) ${range(5)}:
            - ${&smpl:i=i}

    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    loader.yaml.representer.full_module_path = False
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert '__dracon__' not in config
    assert isinstance(config['obj4'], ClassA)
    assert config['obj4'].index == 4
    assert config['obj4'].name == 'Name 4'
    assert config['prop4'] == '4: Name 4'

    manual_list = [ClassA(index=i + 1, name=f"Name {i+1}") for i in range(5)]
    assert config['as_ampersand_anchor'] == manual_list


def test_instruct_on_nodes():
    yaml_content = """
    a_list: &alist
     - !ClassA
       index: 42
     - !ClassA
       index: 43
     - !ClassA
       index: 44

    !define i42 : 42

    list42:
        !each(elt) ${&alist}:
            - <<: *$elt
              <<{+}: {name: "new_name ${@index}"}
              <<{<+}:
                index: *$i42

    other_list:
        !each(elt) ${&alist}:
            - <<: *$elt
              <<{+}: {name: "new_name ${@index}"}
              <<{<+}: 
                index: !include $elt@index

    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    loader.yaml.representer.full_module_path = False
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert config['list42'] == [
        ClassA(index=42, name='new_name 42'),
        ClassA(index=42, name='new_name 42'),
        ClassA(index=42, name='new_name 42'),
    ]

    assert config['other_list'] == [
        ClassA(index=42, name='new_name 42'),
        ClassA(index=43, name='new_name 43'),
        ClassA(index=44, name='new_name 44'),
    ]


def test_each_with_mapping():
    yaml_content = """
    # Test with dynamic keys
    simple_map:
        !each(i) ${range(3)}:
            key_${i}: value_${i}

    # Test with object references
    __dracon__:
        obj: &obj !ClassA
            index: ${i}
            name: "Name ${i}"

    objects:
        !each(i) ${range(2)}:
            obj_${i}:
                <<: *obj
                <<{+}: 
                    name: "Modified ${i}"
    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    # Check simple map with dynamic keys
    assert config['simple_map'] == {'key_0': 'value_0', 'key_1': 'value_1', 'key_2': 'value_2'}

    # Check objects map
    assert isinstance(config['objects']['obj_0'], ClassA)
    assert config['objects']['obj_0'].index == 0
    assert config['objects']['obj_0'].name == 'Modified 0'
    assert isinstance(config['objects']['obj_1'], ClassA)
    assert config['objects']['obj_1'].index == 1
    assert config['objects']['obj_1'].name == 'Modified 1'


def test_defines():
    yaml_content = """
    !define i42 : !int 42

    expr42: !int ${i42}
    inc42: *$i42

    !define compint: ${4 + 4}
    compint_expr: ${compint}
    compint_inc: !include $compint

    !define runtimeval : ${func(1,2)}
    runtimeval_expr: ${runtimeval}

    !define recursive_def: ${&runtimeval_expr}
    recursive: ${recursive_def.evaluate()}

    a_obj: !ClassA
        index: &aid ${i42}
        name: oldname
        <<{<+}: 
            name: "new_name ${&aid}"

    nested:
        !define aid: ${get_index(construct(&/a_obj))}
        a_index: ${aid}
        aname: ${&/a_obj.name}
        constructed_name: ${construct(&/a_obj).name}
        constructed_nameindex: ${construct(&/a_obj).name_index}

    """

    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    loader.yaml.representer.full_module_path = False
    loader.context['func'] = lambda x, y: x + y
    loader.context['get_index'] = lambda obj: obj.index
    config = loader.loads(yaml_content)
    config.resolve_all_lazy()

    assert config['expr42'] == 42
    assert config['inc42'] == 42
    assert config['compint_expr'] == 8
    assert config['compint_inc'] == 8
    assert config['runtimeval_expr'] == 3

    assert config['recursive'] == 3

    assert isinstance(config.a_obj, ClassA)
    assert config['a_obj'].index == 42
    assert config['a_obj'].name == "new_name 42"

    assert config['nested']['a_index'] == config['a_obj'].index
    assert config['nested']['aname'] == config['a_obj'].name

    assert config['nested']['constructed_name'] == config['a_obj'].name
    assert config['nested']['constructed_nameindex'] == config['a_obj'].name_index


def test_include():
    loader = DraconLoader(enable_interpolation=True, context={'ClassA': ClassA})
    loader.context['get_index'] = lambda obj: obj.index
    loader.context['get_nameindex'] = lambda obj: obj.name_index
    compres = compose_from_include_str(loader, 'pkg:dracon:tests/configs/interp_include.yaml')
    config = loader.load_composition_result(compres)
    config.resolve_all_lazy()
    assert config.nested.a_index == 2

    assert isinstance(config.nested.a_nested, ClassA)
    assert config.nested.a_nested.index == 3
    assert config.nested.oldname == 'oldname 2'

    assert config.nested.a_nested.name == 'newer_name 3'

    assert config.nested.nameindex == '3: oldname 3'
    assert config.nested.nameindex_2 == '3: oldname 3'

    assert config.nested.alist == [ClassA(index=1, name='name 1'), ClassA(index=2, name='name 2')]

    assert config.other.a == 3
    assert config.other.var_b_value == 15


def test_each_ctx_is_shallow():
    yaml_content = """
    !set_default varlist : ['value1', 'value2']
    list_content:
      !each(var) ${varlist}:
        - val: ${var}
          valist: ${varlist}
    """
    loader = DraconLoader(enable_interpolation=True, base_dict_type=dict, base_list_type=list)
    config = loader.loads(yaml_content)
    assert isinstance(config, dict)
    assert isinstance(config['list_content'], list)
    assert len(config['list_content']) == 2
    assert (
        config['list_content'][0]['val'].context['varlist']
        is config['list_content'][1]['val'].context['varlist']
    )

    resolve_all_lazy(config)

    for i, c in enumerate(config['list_content']):
        assert c['val'] == f"value{i+1}"
        assert c['valist'] == ['value1', 'value2']

================
File: dracon/tests/test_keypath.py
================
import pytest
from dracon.keypath import KeyPath, KeyPathToken
from dracon.nodes import DraconMappingNode


# Tests for parsing without simplification
def test_parse_string_root():
    kp = KeyPath("/", simplify=False)
    assert kp.parts == [KeyPathToken.ROOT]


def test_parse_string_simple():
    kp = KeyPath("a.b.c", simplify=False)
    assert kp.parts == ["a", "b", "c"]


def test_parse_int_simple():
    kp = KeyPath(1, simplify=False)
    assert kp.parts == ['1']


def test_with_dots():
    kp = KeyPath("a.b\\.c.d", simplify=False)
    assert kp.parts == ["a", "b.c", "d"]


def test_parse_string_with_root():
    kp = KeyPath("/a.b.c", simplify=False)
    assert kp.parts == [KeyPathToken.ROOT, "a", "b", "c"]


def test_parse_string_with_up():
    kp = KeyPath("a.b..c", simplify=False)
    assert kp.parts == ["a", "b", KeyPathToken.UP, "c"]


def test_parse_string_multiple_up():
    kp = KeyPath("a.b...c", simplify=False)
    assert kp.parts == ["a", "b", KeyPathToken.UP, KeyPathToken.UP, "c"]


def test_parse_string_up_at_start():
    kp = KeyPath("..a.b", simplify=False)
    assert kp.parts == [KeyPathToken.UP, "a", "b"]


def test_parse_string_up_at_end():
    kp = KeyPath("a.b..", simplify=False)
    assert kp.parts == ["a", "b", KeyPathToken.UP]


def test_parse_string_with_integers():
    kp = KeyPath("a.0.b.1", simplify=False)
    assert kp.parts == ["a", '0', "b", '1']


def test_parse_string_mixed():
    kp = KeyPath("/a.0..b.1...", simplify=False)
    assert kp.parts == [
        KeyPathToken.ROOT,
        "a",
        '0',
        KeyPathToken.UP,
        "b",
        '1',
        KeyPathToken.UP,
        KeyPathToken.UP,
    ]


def test_parse_empty_string():
    kp = KeyPath("", simplify=False)
    assert kp.parts == []


# Tests for full construction with simplification
def test_simplify_root():
    kp = KeyPath("/")
    assert str(kp) == "/"


def test_simplify_simple_0():
    kp = KeyPath("a")
    assert str(kp) == "a"


def test_simplify_simple():
    kp = KeyPath("a.b.c")
    assert str(kp) == "a.b.c"


def test_simplify_with_up():
    kp = KeyPath("a.b..c")
    assert str(kp) == "a.c"


def test_simplify_multiple_up():
    kp = KeyPath("a.b...c", simplify=False)
    assert kp.parts == ["a", "b", KeyPathToken.UP, KeyPathToken.UP, "c"]
    assert str(kp) == "a.b...c"
    kp.simplify()
    assert str(kp) == "c"


def test_dont_simplify_up_at_start():
    kp = KeyPath("..a.b", simplify=False)
    assert kp.parts == [KeyPathToken.UP, "a", "b"]
    assert str(kp) == "..a.b"


def test_simplify_up_at_end():
    kp = KeyPath("a.b.")
    assert str(kp) == "a.b"


def test_simplify_with_root():
    kp = KeyPath("/a.b..c")
    assert str(kp) == "/a.c"


def test_simplify_to_root():
    kp = KeyPath("/a.b...")
    assert str(kp) == "/"


def test_simplify_beyond_root():
    kp = KeyPath("/a.b....")
    assert str(kp) == "/"


def test_simplify_with_integers():
    kp = KeyPath("a.0.b.1")
    assert str(kp) == "a.0.b.1"


def test_simplify_mixed():
    kp = KeyPath("/a.0..b.1")
    assert str(kp) == "/a.b.1"


def test_simplify_empty():
    kp = KeyPath("")
    assert str(kp) == ""


def test_simplify_only_up():
    kp = KeyPath("...")
    assert kp.parts == [KeyPathToken.UP, KeyPathToken.UP]
    assert str(kp) == "..."


def test_simplify_only_up_to_root():
    kp = KeyPath("/....", simplify=False)
    assert kp.parts == [KeyPathToken.ROOT, KeyPathToken.UP, KeyPathToken.UP, KeyPathToken.UP]
    assert str(kp) == "/...."
    kp.simplify()
    assert str(kp) == "/"


def test_root_anywhere():
    kp = KeyPath("a.b.c/..d.e")
    assert str(kp) == "/d.e"


def test_multuple_root():
    kp = KeyPath("/a.b.c//..d.e/..f.g.d..")
    assert str(kp) == "/f.g"


def test_addition():
    a = KeyPath("a.b")
    assert str(a) == "a.b"
    b = KeyPath("c.d")
    assert str(b) == "c.d"
    c = a + b
    assert str(a) == "a.b"
    assert str(b) == "c.d"
    assert str(c) == "a.b.c.d"
    d = a + KeyPath("e")
    assert str(d) == "a.b.e"
    e = KeyPath("f") + b
    assert str(e) == "f.c.d"

    f = KeyPath("..f")
    assert str(f) == "..f"
    assert (a + f).simplified() == KeyPath("a.f")

    aroot = KeyPath("/a.b")
    assert str(aroot) == "/a.b"
    broot = KeyPath("/c.d")
    assert str(broot) == "/c.d"
    croot = aroot + broot
    assert str(croot) == "/a.b/c.d"
    assert str(croot.simplified()) == "/c.d"


# test get on a dictionary
D = {
    "a": {"b": {"c": 1}},
    "d": 2,
    "e": 3,
    "f": {"g": {"h": [4, 5, {"i": 6, "j": [7, 8, 9]}]}},
}


def test_get():
    assert KeyPath("d").get_obj(D) == 2
    assert KeyPath("/f.g.h.1").get_obj(D) == 5
    assert KeyPath("f.g.h.2.j.1").get_obj(D) == 8
    assert KeyPath("a.b.c/a").get_obj(D) == {"b": {"c": 1}}
    assert KeyPath("a.b.c/a.b").get_obj(D) == {"c": 1}
    assert KeyPath("a.b.c....d").get_obj(D) == 2


class DummyNode:
    def __init__(self, value):
        self.value = value
        self.tag = ''


    def __eq__(self, other):
        return self.value == other.value


B = DraconMappingNode(
    tag='',
    value=[(DummyNode('b'), DraconMappingNode(tag='', value=[(DummyNode('c'), 1)]))],
)
F = DraconMappingNode(
    tag='',
    value=[
        (
            DummyNode('g'),
            DraconMappingNode(
                tag='',
                value=[
                    (
                        DummyNode('h'),
                        [4, 5, DraconMappingNode(tag='', value=[(DummyNode('i'), 6)])],
                    ),
                ],
            ),
        )
    ],
)

M = DraconMappingNode(
    tag='',
    value=[
        (
            DummyNode('a'),
            B,
        ),
        (DummyNode('d'), 2),
        (DummyNode('e'), 3),
        (DummyNode('f'), F),
    ],
)


def test_mappingkey():
    # test normal value keypaths:
    assert KeyPath("a.b.c").get_obj(M) == 1
    assert KeyPath("d").get_obj(M) == 2
    # test mapping key keypaths:
    mk = KeyPath("a.b") + KeyPathToken.MAPPING_KEY + KeyPath("c")
    assert mk.is_mapping_key()
    assert mk.get_obj(M) == DummyNode('c')
    mk = KeyPath("a.b") + KeyPathToken.MAPPING_KEY
    with pytest.raises(ValueError):
        mk.get_obj(M)

    mk = KeyPath("a") + KeyPathToken.MAPPING_KEY + KeyPath("b")
    assert mk.get_obj(M) == DummyNode('b')

    newmk = mk.copy() + KeyPath("c")
    with pytest.raises(ValueError):
        newmk.get_obj(M)

    newmk = mk.copy().up()
    assert not newmk.is_mapping_key()
    assert newmk.get_obj(M) == B

    newmk = mk.copy() + [KeyPathToken.UP, KeyPathToken.UP]
    assert newmk.get_obj(M) == M

    newmk = mk.copy() + [
        KeyPathToken.UP,
        KeyPathToken.UP,
        KeyPathToken.MAPPING_KEY,
        "d",
        KeyPathToken.UP,
        KeyPathToken.MAPPING_KEY,
        KeyPathToken.UP,
        "f",
        "g",
        KeyPathToken.MAPPING_KEY,
        "h",
    ]
    assert newmk.get_obj(M) == DummyNode('h')

    assert newmk.is_mapping_key()
    valuek = newmk.removed_mapping_key() + "1"
    assert newmk.is_mapping_key()
    assert not valuek.is_mapping_key()
    assert valuek.get_obj(M) == 5


def test_match_exact():
    pattern = KeyPath("a.b.c")
    target = KeyPath("a.b.c")
    assert pattern.match(target)


def test_match_single_wildcard():
    pattern = KeyPath("a.*.c")
    target1 = KeyPath("a.b.c")
    target2 = KeyPath("a.xyz.c")
    target3 = KeyPath("a.b.d")
    assert pattern.match(target1)
    assert pattern.match(target2)
    assert not pattern.match(target3)


def test_match_multi_wildcard():
    pattern = KeyPath("a.**.d")
    target1 = KeyPath("a.b.c.d")
    target2 = KeyPath("a.d")
    target3 = KeyPath("a.x.y.z.d")
    target4 = KeyPath("a.b.c.e")
    assert pattern.match(target1)
    assert pattern.match(target2)
    assert pattern.match(target3)
    assert not pattern.match(target4)


def test_match_partial_segment():
    pattern = KeyPath("a.b*.d")
    target1 = KeyPath("a.b.d")
    target2 = KeyPath("a.bcd.d")
    target3 = KeyPath("a.bc123.d")
    target4 = KeyPath("a.c.d")
    assert pattern.match(target1)
    assert pattern.match(target2)
    assert pattern.match(target3)
    assert not pattern.match(target4)


def test_match_mixed_wildcards_and_partial():
    pattern = KeyPath("a.*.b*.**.c*d")
    target1 = KeyPath("a.x.by.cd")
    target2 = KeyPath("a.x.bz.y.z.cd")
    target3 = KeyPath("a.x.by.z.czzd")
    target4 = KeyPath("a.x.cy.z.d")
    assert pattern.match(target1)
    assert pattern.match(target2)
    assert pattern.match(target3)
    assert not pattern.match(target4)


def test_match_with_root():
    pattern = KeyPath("/a.b*.c")
    target1 = KeyPath("/a.b.c")
    target2 = KeyPath("/a.bxyz.c")
    target3 = KeyPath("a.b.c")
    assert pattern.match(target1)
    assert pattern.match(target2)
    assert not pattern.match(target3)


def test_match_only_wildcards():
    pattern = KeyPath("*.**.*")
    target1 = KeyPath("a.b.c")
    target2 = KeyPath("a.b.c.d.e")
    target3 = KeyPath("a")
    assert pattern.match(target1)
    assert pattern.match(target2)
    assert not pattern.match(target3)


def test_match_empty():
    pattern = KeyPath("")
    target1 = KeyPath("")
    target2 = KeyPath("a")
    assert pattern.match(target1)
    assert not pattern.match(target2)


def test_match_complex_pattern():
    pattern = KeyPath("/a.*.b*.**.c*.*d")
    target1 = KeyPath("/a.x.by.z.cz.yd")
    target2 = KeyPath("/a.x.b.c.d")
    target3 = KeyPath("/a.x.by.z.c")
    target4 = KeyPath("a.x.by.z.cz.yd")
    assert pattern.match(target1)
    assert pattern.match(target2)
    assert not pattern.match(target3)
    assert not pattern.match(target4)


def test_match_with_integers():
    pattern = KeyPath("a.*.b*.**")
    target1 = KeyPath("a.0.b1.2.3")
    target2 = KeyPath("a.x.by.z")
    assert pattern.match(target1)
    assert pattern.match(target2)


def test_match_with_escaped_dots():
    pattern = KeyPath("a.*.b\\.c*.d")
    target1 = KeyPath("a.x.b.c.d")
    target2 = KeyPath("a.x.b\\.c123.d")
    assert not pattern.match(target1)
    assert pattern.match(target2)

================
File: dracon/tests/test_merge.py
================
from dracon.merge import merged, MergeKey, MergeMode, MergePriority
from dracon.utils import ShallowDict


def test_basic_merge():
    d1 = {"a": 1, "b": 2}
    d2 = {"b": 3, "c": 4}
    mk = MergeKey(raw="<<{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": 1, "b": 2, "c": 4}


def test_merge_with_new_priority():
    d1 = {"a": 1, "b": 2}
    d2 = {"b": 3, "c": 4}
    mk = MergeKey(raw="<<{<}")
    result = merged(d1, d2, mk)
    assert result == {"a": 1, "b": 3, "c": 4}


def test_merge_nested_dicts():
    d1 = {"a": 1, "b": {"x": 10, "y": 20}}
    d2 = {"b": {"y": 30, "z": 40}, "c": 5}
    mk = MergeKey(raw="<<{+>}")
    result = merged(d1, d2, mk)
    assert result == {"a": 1, "b": {"x": 10, "y": 20, "z": 40}, "c": 5}


def test_merge_replace_mode():
    d1 = {"a": 1, "b": {"x": 10, "y": 20}}
    d2 = {"b": {"z": 30}, "c": 5}
    mk = MergeKey(raw="<<{~>}")
    result = merged(d1, d2, mk)
    assert result == {"a": 1, "b": {"x": 10, "y": 20}, "c": 5}


def test_merge_lists_append_mode():
    d1 = {"a": [1, 2], "b": 3}
    d2 = {"a": [3, 4], "c": 5}
    mk = MergeKey(raw="<<[+]{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": [1, 2, 3, 4], "b": 3, "c": 5}


def test_merge_lists_replace_mode():
    d1 = {"a": [1, 2], "b": 3}
    d2 = {"a": [3, 4], "c": 5}
    mk = MergeKey(raw="<<[~]{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": [1, 2], "b": 3, "c": 5}


def test_merge_lists_with_priority():
    d1 = {"a": [1, 2], "b": 3}
    d2 = {"a": [3, 4], "c": 5}
    mk = MergeKey(raw="<<[~<]{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": [3, 4], "b": 3, "c": 5}


def test_merge_mixed_types():
    d1 = {"a": [1, 2], "b": {"x": 10}}
    d2 = {"a": 3, "b": [4, 5]}
    mk = MergeKey(raw="<<[+]{+<}")
    result = merged(d1, d2, mk)
    assert result == {"a": 3, "b": [4, 5]}


def test_merge_with_none_values():
    d1 = {"a": 1, "b": None}
    d2 = {"b": 2, "c": None}
    mk = MergeKey(raw="<<{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": 1, "b": None, "c": None}


def test_merge_empty_dicts():
    d1 = {}
    d2 = {"a": 1}
    mk = MergeKey(raw="<<{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": 1}


def test_merge_identical_dicts():
    d1 = {"a": 1, "b": 2}
    d2 = {"a": 1, "b": 2}
    mk = MergeKey(raw="<<{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": 1, "b": 2}


def test_merge_nested_lists():
    d1 = {"a": [1, [2, 3]], "b": 4}
    d2 = {"a": [5, [6, 7]], "c": 8}
    mk = MergeKey(raw="<<[+]{>}")
    result = merged(d1, d2, mk)
    assert result == {"a": [1, [2, 3], 5, [6, 7]], "b": 4, "c": 8}


def test_merge_nested_dicts_with_lists():
    d1 = {"a": {"x": [1, 2]}, "b": 3}
    d2 = {"a": {"x": [3, 4], "y": 5}, "c": 6}
    mk = MergeKey(raw="<<[+]{+<}")
    result = merged(d1, d2, mk)
    assert result == {"a": {"x": [1, 2, 3, 4], "y": 5}, "b": 3, "c": 6}


def test_context_merge_with_large_objects():
    large_data1 = [i for i in range(100000)]
    large_data2 = [i for i in range(100000, 200000)]

    context1 = ShallowDict({"data1": large_data1, "common_key": "value1"})
    context2 = ShallowDict({"data2": large_data2, "common_key": "value2"})

    merged_context_existing = merged(context1, context2, MergeKey(raw="{+>}"))
    merged_context_new = merged(context1, context2, MergeKey(raw="{+<}"))

    # large objects are preserved as references, not copies
    assert merged_context_existing["data1"] is large_data1
    assert merged_context_existing["data2"] is large_data2
    assert merged_context_new["data1"] is large_data1
    assert merged_context_new["data2"] is large_data2

    # merge priority works correctly
    assert merged_context_existing["common_key"] == "value1"  # Existing preserved
    assert merged_context_new["common_key"] == "value2"  # New took priority

================
File: dracon/tests/test_mergekey.py
================
import pytest
from pydantic import ValidationError
from dracon.merge import MergeKey, MergeMode, MergePriority


def test_merge_key_initialization():
    mk = MergeKey(raw="<<{+<}")
    assert mk.dict_mode == MergeMode.APPEND
    assert mk.dict_priority == MergePriority.NEW
    assert mk.list_mode == MergeMode.REPLACE
    assert mk.list_priority == MergePriority.EXISTING


def test_merge_key_is_merge_key():
    assert MergeKey.is_merge_key("<<{+<}")
    assert MergeKey.is_merge_key("<<[+]{<}")
    assert not MergeKey.is_merge_key("normal_key")


def test_merge_key_dict_mode_and_priority():
    mk = MergeKey(raw="<<{+<}")
    assert mk.dict_mode == MergeMode.APPEND
    assert mk.dict_priority == MergePriority.NEW

    mk = MergeKey(raw="<<{~>}")
    assert mk.dict_mode == MergeMode.REPLACE
    assert mk.dict_priority == MergePriority.EXISTING


def test_merge_key_list_mode_and_priority():
    mk = MergeKey(raw="<<[+<]")
    assert mk.list_mode == MergeMode.APPEND
    assert mk.list_priority == MergePriority.NEW

    mk = MergeKey(raw="<<[~>]")
    assert mk.list_mode == MergeMode.REPLACE
    assert mk.list_priority == MergePriority.EXISTING


def test_merge_key_depth():
    mk = MergeKey(raw="<<{+2<}")
    assert mk.dict_depth == 2
    assert mk.list_depth is None

    mk = MergeKey(raw="<<[+3]{+2<}")
    assert mk.dict_depth == 2
    assert mk.list_depth == 3


def test_merge_key_combined_options():
    mk = MergeKey(raw="<<[+<]{~>}")
    assert mk.dict_mode == MergeMode.REPLACE
    assert mk.dict_priority == MergePriority.EXISTING
    assert mk.list_mode == MergeMode.APPEND
    assert mk.list_priority == MergePriority.NEW


def test_merge_key_invalid_combinations():
    with pytest.raises(ValidationError):
        MergeKey(raw="<<{+~}")

    with pytest.raises(ValidationError):
        MergeKey(raw="<<{<>}")


@pytest.mark.parametrize(
    "raw,expected_dict_mode,expected_dict_priority,expected_list_mode,expected_list_priority",
    [
        ("<<{+<}", MergeMode.APPEND, MergePriority.NEW, MergeMode.REPLACE, MergePriority.EXISTING),
        (
            "<<[~]{>}",
            MergeMode.APPEND,
            MergePriority.EXISTING,
            MergeMode.REPLACE,
            MergePriority.EXISTING,
        ),
        (
            "<<[+]{~<}",
            MergeMode.REPLACE,
            MergePriority.NEW,
            MergeMode.APPEND,
            MergePriority.EXISTING,
        ),
        ("<<", MergeMode.APPEND, MergePriority.EXISTING, MergeMode.REPLACE, MergePriority.EXISTING),
    ],
)

def test_merge_key_various_combinations(
    raw, expected_dict_mode, expected_dict_priority, expected_list_mode, expected_list_priority
):
    mk = MergeKey(raw=raw)
    assert mk.dict_mode == expected_dict_mode
    assert mk.dict_priority == expected_dict_priority
    assert mk.list_mode == expected_list_mode
    assert mk.list_priority == expected_list_priority


def test_merge_key_empty_options():
    mk = MergeKey(raw="<<")
    assert mk.dict_mode == MergeMode.APPEND
    assert mk.dict_priority == MergePriority.EXISTING
    assert mk.list_mode == MergeMode.REPLACE
    assert mk.list_priority == MergePriority.EXISTING


def test_merge_key_only_dict_options():
    mk = MergeKey(raw="<<{+<}")
    assert mk.dict_mode == MergeMode.APPEND
    assert mk.dict_priority == MergePriority.NEW
    assert mk.list_mode == MergeMode.REPLACE
    assert mk.list_priority == MergePriority.EXISTING


def test_merge_key_only_list_options():
    mk = MergeKey(raw="<<[~>]")
    assert mk.dict_mode == MergeMode.APPEND
    assert mk.dict_priority == MergePriority.EXISTING
    assert mk.list_mode == MergeMode.REPLACE
    assert mk.list_priority == MergePriority.EXISTING


def test_merge_key_multiple_depth_specifications():
    mk = MergeKey(raw="<<[+2]{+3<}")
    assert mk.dict_depth == 3
    assert mk.list_depth == 2


def test_merge_key_ignore_invalid_depth():
    mk = MergeKey(raw="<<{+invalid<}")
    assert mk.dict_depth is None
    assert mk.dict_mode == MergeMode.APPEND
    assert mk.dict_priority == MergePriority.NEW

================
File: dracon/tests/test_picklable.py
================
import pickle
import pytest
from ruamel.yaml import YAML
from dracon.yaml import PicklableYAML
import io
import multiprocessing
from pathlib import Path
from dracon.loader import DraconLoader
from pydantic import BaseModel
from dracon.resolvable import Resolvable
from dracon.deferred import DeferredNode
from dracon.lazy import LazyInterpolable
from dracon.keypath import ROOTPATH
from dracon.include import compose_from_include_str
import os

# Set a dummy environment variable for testing purposes
os.environ["TESTVAR1"] = "test_var_1"
os.environ["TESTVAR2"] = "test_var_2"

# Test file paths
simple_config_path = 'dracon:tests/configs/simple.yaml'

main_config_path = 'dracon:tests/configs/main.yaml'
params_config_path = 'dracon:tests/configs/params.yaml'
base_config_path = 'dracon:tests/configs/base.yaml'
interp_config_path = 'dracon:tests/configs/interpolation.yaml'
resolvable_config_path = 'dracon:tests/configs/resolvable.yaml'
override_config_path = 'dracon:tests/configs/override.yaml'


# Move CustomType to module level
class CustomType:
    """A custom type for YAML serialization testing"""

    yaml_tag = '!custom'

    def __init__(self, value):
        self.value = int(value)

    def __eq__(self, other):
        return isinstance(other, CustomType) and self.value == other.value

    @classmethod
    def from_yaml(cls, constructor, node):
        return cls(int(constructor.construct_scalar(node)))

    @classmethod
    def to_yaml(cls, representer, data):
        return representer.represent_scalar(cls.yaml_tag, str(data.value))


# Main test fixtures
@pytest.fixture
def yaml_instance():
    yaml = PicklableYAML()
    yaml.indent = 4
    yaml.width = 80
    yaml.preserve_quotes = True
    yaml.default_flow_style = False
    return yaml


@pytest.fixture
def test_data():
    return {
        'string': 'test',
        'number': 42,
        'list': [1, 2, 3],
        'nested': {'a': 1, 'b': 2},
        'multiline': """line1
        line2
        line3""",
    }


def test_basic_pickle_unpickle(yaml_instance):
    """Test basic pickling and unpickling of the YAML instance"""
    pickled = pickle.dumps(yaml_instance)
    unpickled = pickle.loads(pickled)

    assert unpickled.old_indent == yaml_instance.old_indent
    assert unpickled.width == yaml_instance.width
    assert unpickled.preserve_quotes == yaml_instance.preserve_quotes
    assert unpickled.default_flow_style == yaml_instance.default_flow_style


def test_pickle_with_dump_load(yaml_instance, test_data):
    """Test that dumping and loading work correctly after pickling"""
    output_before = io.StringIO()
    yaml_instance.dump(test_data, output_before)

    pickled = pickle.dumps(yaml_instance)
    unpickled = pickle.loads(pickled)

    output_after = io.StringIO()
    unpickled.dump(test_data, output_after)

    assert output_before.getvalue() == output_after.getvalue()


def test_pickle_with_custom_types(yaml_instance):
    """Test pickling with custom registered types"""
    # Register the custom type
    yaml_instance.register_class(CustomType)

    # Test data with custom type
    test_data = {'custom': CustomType(42)}

    # Pickle and unpickle
    pickled = pickle.dumps(yaml_instance)
    unpickled = pickle.loads(pickled)

    # Test dumping and loading with custom type
    output = io.StringIO()
    unpickled.dump(test_data, output)

    # Load and verify
    input_stream = io.StringIO(output.getvalue())
    loaded_data = unpickled.load(input_stream)
    assert isinstance(loaded_data['custom'], CustomType)
    assert loaded_data['custom'].value == 42


def test_pickle_with_different_configurations():
    """Test pickling with different YAML configurations"""
    configs = [
        {'typ': 'safe'},
        {'typ': 'rt'},
        {'typ': 'base'},
    ]

    for config in configs:
        yaml = PicklableYAML(**config)
        pickled = pickle.dumps(yaml)
        unpickled = pickle.loads(pickled)
        assert unpickled.typ == yaml.typ


def test_deep_pickle_state():
    """Test that internal state and buffers are properly pickled"""
    yaml = PicklableYAML()

    test_data = {'test': 'value'}
    output = io.StringIO()
    yaml.dump(test_data, output)

    pickled = pickle.dumps(yaml)
    unpickled = pickle.loads(pickled)

    assert not hasattr(unpickled, '_reader')
    assert not hasattr(unpickled, '_scanner')
    assert unpickled.allow_unicode == yaml.allow_unicode
    assert unpickled.encoding == yaml.encoding


def process_yaml(pickled_yaml):
    """Process YAML in a separate process"""
    yaml = pickle.loads(pickled_yaml)
    output = io.StringIO()
    yaml.dump({'test': 'value'}, output)
    return output.getvalue()


def test_pickle_cross_process():
    """Test pickling and unpickling across processes"""
    import multiprocessing

    yaml = PicklableYAML()
    pickled = pickle.dumps(yaml)

    with multiprocessing.Pool(1) as pool:
        result = pool.apply(process_yaml, (pickled,))

    assert isinstance(result, str)
    assert 'test: value' in result


#####################################################
#            PICKLING A LOADER
#####################################################


# Helper function for multiprocessing tests - must be at module level
def load_config_in_process(config_path):
    """Load a config in a separate process"""
    loader = DraconLoader()
    config = loader.load(f"pkg:{config_path}")
    return config


def pickle_unpickle(obj):
    """Helper to pickle and unpickle an object"""
    pickled = pickle.dumps(obj)
    return pickle.loads(pickled)


def test_loader_pickling():
    """Test that DraconLoader can be pickled and unpickled"""
    loader = DraconLoader()
    loader.enable_interpolation = True

    # Pickle and unpickle the loader
    unpickled_loader = pickle_unpickle(loader)

    # Check if attributes are preserved
    assert unpickled_loader.enable_interpolation == loader.enable_interpolation

    # Test that the unpickled loader can still load configs
    config = unpickled_loader.load(f"pkg:{simple_config_path}")
    assert config["root"]["a"] == 3
    assert config["root"]["inner"]["d"] == 6


def test_loaded_config_pickling():
    """Test that loaded configurations can be pickled and unpickled"""
    loader = DraconLoader()
    config = loader.load(f"pkg:{main_config_path}")

    # Pickle and unpickle the config
    unpickled_config = pickle_unpickle(config)

    # Verify the unpickled config matches the original
    assert unpickled_config["config"]["setting1"] == "newval1"
    assert unpickled_config["config"]["setting2"] == "baseval2"
    assert unpickled_config["config"]["extra"]["root"]["inner"]["d"] == 6
    assert unpickled_config["config"]["a_list"] == ["item1", "item2", "item3", "item4"]


def test_composition_result_pickling():
    """Test that composition results can be pickled and unpickled"""
    loader = DraconLoader()
    compres = compose_from_include_str(loader, f"pkg:{main_config_path}")

    # Pickle and unpickle the composition result
    unpickled_compres = pickle_unpickle(compres)

    # Load the unpickled composition result
    config = loader.load_composition_result(unpickled_compres)

    # Verify the config loaded from unpickled composition is correct
    assert config["config"]["setting1"] == "newval1"
    assert config["config"]["extra"]["root"]["inner"]["d"] == 6


def test_multiprocess_loading():
    """Test that configs can be loaded in separate processes"""
    with multiprocessing.Pool(2) as pool:
        # Load multiple configs in parallel
        configs = pool.map(load_config_in_process, [simple_config_path, params_config_path])

    # Verify the configs loaded correctly
    simple_config, params_config = configs

    # Check simple config
    assert simple_config["root"]["a"] == 3
    assert simple_config["root"]["inner"]["d"] == 6

    # Check params config
    assert params_config["param1"] == "value1_overriden"
    assert params_config["param2"] == "value2"


def test_pickle_with_interpolation():
    """Test pickling configs with interpolation enabled"""
    loader = DraconLoader(enable_interpolation=True)
    config = loader.load(f"pkg:{interp_config_path}")

    # Store any values we want to compare after unpickling
    pre_pickle_values = {
        'file_stem': config.base.file_stem,
        'interpolated_addition': config.base.interpolated_addition
        if hasattr(config.base, 'interpolated_addition')
        else None,
    }

    # Pickle and unpickle
    unpickled_config = pickle_unpickle(config)

    # Verify the structure is preserved
    assert hasattr(unpickled_config, 'base')

    # Compare values that should be preserved
    for key, value in pre_pickle_values.items():
        if value is not None:
            assert getattr(unpickled_config.base, key) == value


def test_lazy_interpolable_pickling():
    """Test pickling of individual LazyInterpolable objects"""
    # Create a simple LazyInterpolable
    lazy = LazyInterpolable(
        value="${2+2}",
        name="test",
        current_path=ROOTPATH,
        permissive=False,
        context={"some_context": "value"},
    )

    # Pickle and unpickle
    unpickled = pickle_unpickle(lazy)

    # Verify the basic attributes are preserved
    assert unpickled.value == "${2+2}"
    assert unpickled.name == "test"
    assert unpickled.permissive is False
    assert unpickled.context == {"some_context": "value"}


def test_lazy_interpolable_with_validator():
    """Test pickling of LazyInterpolable with validator"""

    def simple_validator(x):
        return int(x)

    # Create LazyInterpolable with validator
    lazy = LazyInterpolable(value="${2+2}", validator=simple_validator, name="test")

    # Pickle and unpickle
    unpickled = pickle_unpickle(lazy)

    # Reattach validator
    unpickled.reattach_validator(simple_validator)

    # Verify it still works
    resolved = unpickled.resolve()
    assert isinstance(resolved, int)
    assert resolved == 4


def load_with_interpolation(config_path):
    loader = DraconLoader(enable_interpolation=True)
    return loader.load(f"pkg:{config_path}")


def test_multiprocess_with_interpolation():
    """Test multiprocess loading with interpolation enabled"""

    with multiprocessing.Pool(1) as pool:
        config = pool.apply(load_with_interpolation, (interp_config_path,))

    assert config.base.file_stem == "interpolation"


def test_large_config_pickling():
    """Test pickling with a large nested configuration"""
    loader = DraconLoader()
    config = loader.load(f"pkg:{main_config_path}")

    # Create a large nested structure
    large_config = {
        "original": config,
        "nested": {
            "level1": {"level2": {"level3": config.copy()}},
            "array": [config.copy() for _ in range(10)],
        },
    }

    # Pickle and unpickle
    unpickled = pickle_unpickle(large_config)

    # Verify deep nested structures
    assert unpickled["original"]["config"]["setting1"] == "newval1"
    assert unpickled["nested"]["level1"]["level2"]["level3"]["config"]["setting1"] == "newval1"
    assert unpickled["nested"]["array"][5]["config"]["setting1"] == "newval1"


class ConfigModel(BaseModel):
    setting1: str
    setting2: str


def test_pydantic_model_pickling():
    """Test pickling when using Pydantic models"""

    loader = DraconLoader()
    config = loader.load(f"pkg:{simple_config_path}")
    model = ConfigModel(setting1="test1", setting2="test2")

    config_with_model = {"config": config, "model": model}

    # Pickle and unpickle
    unpickled = pickle_unpickle(config_with_model)

    # Verify both config and model are preserved
    assert unpickled["config"]["root"]["a"] == 3
    assert isinstance(unpickled["model"], ConfigModel)
    assert unpickled["model"].setting1 == "test1"

================
File: dracon/tests/test_serde.py
================
from dracon import dump, loads
from dracon.loader import DraconLoader
from pydantic import BaseModel, PlainSerializer
from typing import Annotated, List, get_type_hints
import pytest
from dracon.nodes import ContextNode
import gc
import sys


class ClassA(BaseModel):
    attr3: float = 0


class ClassB(BaseModel):
    attr1: str
    attr2: int
    attrA: ClassA


TypeWithSer = Annotated[
    str,
    PlainSerializer(lambda x: f'custom_{x}'),
]


def test_context_shallow_copy():
    # Create a large object that would be expensive to deepcopy
    large_data = [i for i in range(1000000)]
    initial_ref_count = sys.getrefcount(large_data)

    # Create a context with this large object
    context = {"large_data": large_data}

    # Create nodes with this context
    nodes = [ContextNode(value=f"test{i}", context=context) for i in range(10)]

    # Verify all nodes reference the same large_data object
    for node in nodes:
        assert node.context["large_data"] is large_data

    # Verify reference count increased by expected amount
    # (one for each node's context dict plus other references from the test)
    expected_increase = len(nodes)
    assert sys.getrefcount(large_data) <= initial_ref_count + expected_increase + 3

    # Copy a node and verify the large data is not duplicated
    copied_node = nodes[0].copy()
    assert copied_node.context["large_data"] is large_data


class ClassC(BaseModel):
    attr1: List[TypeWithSer]
    attrB: ClassB


def test_simple():
    conf = """
        !ClassB
        attr1: hello
        attr2: 42
        attrA: !ClassA
            attr3: 3.14
    """

    loader = DraconLoader(context={"ClassA": ClassA, "ClassB": ClassB})
    loader.yaml.representer.full_module_path = False

    obj = loader.loads(conf)
    assert isinstance(obj, ClassB)
    assert obj.attr1 == "hello"
    assert obj.attr2 == 42
    assert isinstance(obj.attrA, ClassA)
    assert obj.attrA.attr3 == 3.14


def test_dump():
    loader = DraconLoader()
    loader.yaml.representer.full_module_path = False
    obj = ClassB(attr1="hello", attr2=42, attrA=ClassA(attr3=3.14))
    conf = loader.dump(obj)
    assert conf == "!ClassB\nattr1: hello\nattr2: 42\nattrA: !ClassA\n  attr3: 3.14\n"


def test_complex():
    a = ClassA(attr3=3.14)
    b = ClassB(attr1="hello", attr2=42, attrA=a)
    c = ClassC(attr1=["hello", "world"], attrB=b)

    loader = DraconLoader(context={"ClassA": ClassA, "ClassB": ClassB, "ClassC": ClassC})
    loader.yaml.representer.full_module_path = False
    conf = loader.dump(c)
    assert (
        conf
        == "!ClassC\nattr1:\n- custom_hello\n- custom_world\nattrB: !ClassB\n  attr1: hello\n  attr2: 42\n  attrA: !ClassA\n    attr3: 3.14\n"
    )


class ClassEx(BaseModel):
    attr: float = 0


def test_empty():
    conf = """
        emptyd: !ClassEx {}
        """
    loader = DraconLoader(context={"ClassEx": ClassEx})
    loader.yaml.representer.full_module_path = False
    obj = loader.loads(conf)
    assert isinstance(obj.emptyd, ClassEx)
    assert obj.emptyd.attr == 0

================
File: dracon/__init__.py
================
from .utils import node_repr
# from .merge import *
from .loader import *
from .lazy import resolve_all_lazy
# from .composer import *
# from .keypath import *
# from .resolvable import Resolvable
# from .draconstructor import Draconstructor
# from .composer import DraconMappingNode, DraconSequenceNode
# import typing

================
File: dracon/.repomixignore
================
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/

================
File: dracon/commandline.py
================
from pydantic import BaseModel, ValidationError, ConfigDict
from pydantic_core import PydanticUndefined
from typing import List, Dict, Any
from dracon import DraconLoader
from dracon.composer import DRACON_UNSET_VALUE
import sys
from rich.console import Console
from rich.box import ROUNDED
from rich.text import Text
from rich.panel import Panel
from typing import (
    Optional,
    Annotated,
    Any,
    TypeVar,
    Generic,
    Callable,
    ForwardRef,
    Union,
)
from dracon.lazy import resolve_all_lazy
from dracon.resolvable import Resolvable, get_inner_type
from dracon.deferred import DeferredNode
from dracon.keypath import KeyPath
from dracon.loader import DEFAULT_LOADERS
import traceback
import logging


logger = logging.getLogger(__name__)

B = TypeVar("B", bound=BaseModel)

ProgramType = ForwardRef("Program")


class Arg:
    def __init__(
        self,
        real_name: Optional[str] = None,
        short: Optional[str] = None,
        long: Optional[str] = None,
        help: Optional[str] = None,
        arg_type: Optional[type] = None,
        expand_help: Optional[bool] = False,
        action: Optional[Callable[[ProgramType, Any], Any]] = None,
        positional: Optional[bool] = False,
        resolvable: Optional[bool] = False,
        is_file: Optional[bool] = False,
    ):
        self.real_name = real_name
        self.short = short
        self.long = long
        self.help = help
        self.arg_type = arg_type
        self.expand_help = expand_help
        self.action = action
        self.positional = positional
        self.resolvable = resolvable
        self.is_file = is_file

    def merge(self, other):
        arg = Arg(
            real_name=self.real_name,
            short=self.short if self.short else other.short,
            long=self.long if self.long else other.long,
            help=self.help if self.help else other.help,
            arg_type=self.arg_type if self.arg_type else other.arg_type,
            action=self.action if self.action else other.action,
            positional=self.positional if self.positional else other.positional,
            resolvable=self.resolvable if self.resolvable else other.resolvable,
            is_file=self.is_file if self.is_file else other.is_file,
        )
        return arg


def getArg(name, field):
    arg = Arg(real_name=name)
    for m in field.metadata:
        if isinstance(m, Arg):
            arg = arg.merge(m)

    if not arg.long:
        arg.long = name

    if not arg.arg_type:
        arg.arg_type = field.annotation

    if arg.arg_type is Resolvable:
        arg.resolvable = True

    assert arg.real_name is not None
    return arg


T = TypeVar("T")


## {{{                        --     print help     --

console = Console()


def format_type_str(arg_type: type) -> str:
    if arg_type is None:
        return ""
    if hasattr(arg_type, "__origin__"):
        if (
            arg_type.__origin__ is Annotated
            or arg_type.__origin__ is Resolvable
            or arg_type.__origin__ is DeferredNode
        ):
            return format_type_str(arg_type.__args__[0])
        elif arg_type.__origin__ is Union:
            types = [t for t in arg_type.__args__ if t is not type(None)]
            if len(types) == 1:
                return format_type_str(types[0])
            return arg_type.__origin__.__name__.upper()
        return format_type_str(arg_type.__args__[0])
    return arg_type.__name__.upper()


def format_default_value(value: Any) -> str:
    if value is PydanticUndefined:
        return None
    if isinstance(value, str):
        return f'"{value}"'
    return str(value)


def is_optional_field(field) -> bool:
    return (
        (field.default is not None and field.default is not PydanticUndefined)
        or field.default_factory is not None
        or (
            hasattr(field.annotation, "__origin__")
            and field.annotation.__origin__ is Union
            and type(None) in field.annotation.__args__
        )
    )


def format_type_display(name: str, arg_type: str, text: Text) -> None:
    text.append(f"  {name}", style="yellow")
    if arg_type:
        text.append(f" ")
        text.append(arg_type, style="blue")
    text.append("\n")


def print_help(prg: "Program", _) -> None:
    positionals = []
    options = []
    flags = []

    for arg in prg._args:
        if arg.positional:
            positionals.append(arg)
        elif arg.arg_type is bool:
            flags.append(arg)
        else:
            options.append(arg)

    content = Text()

    if prg.description:
        content.append("\n" + prg.description + "\n\n", style="italic")
        content.append("─" * min(console.width - 4, 80) + "\n\n", style="bright_black")

    usage = [prg.name or "command"]
    if options or flags:
        usage.append("[OPTIONS]")
    for pos in positionals:
        usage.append(pos.real_name.upper())

    content.append("Usage: ", style="bold")
    content.append(" ".join(usage) + "\n\n", style="yellow")

    if positionals:
        content.append("Arguments:\n", style="bold green")
        for arg in positionals:
            name = arg.real_name.upper()
            help_text = arg.help or ""
            arg_type = format_type_str(arg.arg_type)

            field = prg.conf_type.model_fields.get(arg.real_name)
            default = None
            required = False

            if field:
                if field.default is not None and field.default is not PydanticUndefined:
                    default = format_default_value(field.default)
                elif field.default_factory is not None:
                    default = "<factory>"
                else:
                    required = not is_optional_field(field)

            content.append(f"  {name}\n", style="yellow")
            content.append(f"    type: ", style="bright_black")
            content.append(f"{arg_type}\n", style="blue")
            if required:
                content.append("    REQUIRED\n", style="red")
            elif default:
                content.append("    default: ", style="bright_black")
                content.append(f"{default}\n", style="dim")
            if help_text:
                content.append(f"    {help_text}\n", style="default")
            content.append("\n")

    if options or flags:
        content.append("Options:\n", style="bold green")
        for arg in options + flags:
            parts = []
            if arg.short:
                parts.append(f"-{arg.short}")
            if arg.long:
                parts.append(f"--{arg.long}")

            option_str = ", ".join(parts)
            if not arg.arg_type is bool:
                name = option_str
                type_str = format_type_str(arg.arg_type)
                format_type_display(name, type_str, content)
            else:
                content.append(f"  {option_str}\n", style="yellow")

            help_text = arg.help or ""
            field = prg.conf_type.model_fields.get(arg.real_name)
            default = None
            required = False

            if field:
                if field.default is not None and field.default is not PydanticUndefined:
                    default = format_default_value(field.default)
                elif field.default_factory is not None:
                    default = "<factory>"
                else:
                    required = not is_optional_field(field)

            if help_text:
                content.append(f"    {help_text}\n")
            if required:
                content.append("    REQUIRED\n", style="red")
            elif default:
                content.append("    default: ", style="bright_black")
                content.append(f"{default}\n", style="dim")
            content.append("\n")

    title = Text()
    title.append(prg.name if prg.name else "Command", style="bold cyan")
    if prg.version:
        title.append(f" (v{prg.version})", style="cyan")

    console.print(
        Panel(content, title=title, box=ROUNDED, border_style="bright_black", expand=False)
    )
    sys.exit(0)


##────────────────────────────────────────────────────────────────────────────}}}


class ArgParseError(Exception):
    pass


class Program(BaseModel, Generic[T]):
    conf_type: type[T]

    name: Optional[str] = None
    version: Optional[str] = None
    description: Optional[str] = None

    model_config = ConfigDict(arbitrary_types_allowed=True, extra="allow")

    def model_post_init(self, *args, **kwargs):
        super().model_post_init(*args, **kwargs)
        self._args = [getArg(name, f) for name, f in self.conf_type.model_fields.items()]
        self._args.append(
            Arg(
                real_name="help",
                short="h",
                long="help",
                help="Print this help message",
                action=print_help,
            )
        )

    def parse_args(self, argv: List[str], **kwargs) -> tuple[Optional[T], Dict[str, Any]]:
        self._positionals = [arg for arg in self._args if arg.positional]
        self._positionals.reverse()
        self._arg_map = {f'-{arg.short}': arg for arg in self._args if arg.short} | {
            f'--{arg.long}': arg for arg in self._args if arg.long
        }

        logger.debug(f"Positional args: {self._positionals}")
        logger.debug(f"Arg map: {self._arg_map}")
        logger.debug(f"Args: {self._args}")

        args, defined_vars, actions, confs_to_merge = {}, {}, [], []

        i = 0
        while i < len(argv):
            i = self._parse_single_arg(argv, i, args, defined_vars, actions, confs_to_merge)

        logger.debug(f"Defined vars: {defined_vars}")

        conf = self.generate_config(args, defined_vars, confs_to_merge, **kwargs)
        if conf is not None:
            for action in actions:
                action(self, conf)
        return conf, args

    def _parse_single_arg(
        self,
        argv: List[str],
        i: int,
        args: Dict,
        defined_vars: Dict,
        actions: List,
        confs_to_merge: List,
    ) -> int:
        argstr = argv[i]

        if argstr.startswith('--define.'):  # a define statement
            return self._handle_define(argv, i, defined_vars)

        elif argstr.startswith('+'):  # conf merge
            confs_to_merge.append(argv[i][1:])
            return i + 1

        elif not argstr.startswith('-'):  # positional argument
            return self._handle_positional(argv, i, args)

        else:  # regular optionnal argument
            return self._handle_option(argv, i, args, actions)

    def _handle_define(self, argv: List[str], i: int, defined_vars: Dict) -> int:
        var_name = argv[i][9:]
        var_value, i = self._read_value(argv, i)
        defined_vars[var_name] = var_value
        return i

    def _handle_positional(self, argv: List[str], i: int, args: Dict) -> int:
        if not self._positionals:
            raise ArgParseError(f"Unexpected positional argument {argv[i]}")
        arg_obj = self._positionals.pop()
        args[arg_obj.real_name] = argv[i]
        return i + 1

    def _handle_option(self, argv: List[str], i: int, args: Dict, actions: List) -> int:
        logger.debug(f"Handling option {argv[i]}")
        argstr = argv[i]
        if argstr not in self._arg_map:
            raise ArgParseError(f"Unknown argument {argstr}")

        arg_obj = self._arg_map[argstr]

        if arg_obj.action is not None:
            actions.append(arg_obj.action)
            logger.debug(f"Adding action {arg_obj.action} to the list of actions")
            return i + 1

        if arg_obj.arg_type is bool:
            args[arg_obj.real_name] = True
            logger.debug(f"Setting {arg_obj.real_name} to True")
            return i + 1

        modifier = (lambda x: f"*file:{x}") if arg_obj.is_file else (lambda x: x)
        v, i = self._read_value(argv, i)
        modified_v = modifier(v)
        args[arg_obj.real_name] = modified_v
        logger.debug(f"Setting {arg_obj.real_name} to {modified_v}")
        return i

    def _read_value(self, argv: List[str], i: int) -> tuple[str, int]:
        i += 1
        if i >= len(argv) or argv[i].startswith('-'):
            raise ArgParseError(f"Expected value for argument {argv[i-1]}")
        return argv[i], i + 1

    def make_merge_str(self, confs_to_merge):
        DEFAULT_MERGE_ARGS = "{~<}[~<]"
        for i, conf in enumerate(confs_to_merge):
            key = f"<<{DEFAULT_MERGE_ARGS}_merge{i}"
            if conf.startswith(tuple(DEFAULT_LOADERS.keys())):
                yield f"{key}: !include \"{conf}\""
            else:  # assume it's a file
                yield f"{key}: !include \"file:{conf}\""

    def generate_config(
        self,
        args: dict[str, str],
        defined_vars: dict[str, str],
        confs_to_merge: list[str],
        **kwargs,
    ) -> Optional[T]:
        def make_override(argname, value):
            argname = argname.lstrip('-')
            if '@' in argname:
                return f"<<{argname}: {value}"
            return f"<<@{argname}: {value}"

        override_str = "\n".join([make_override(k, v) for k, v in args.items()])
        loader = DraconLoader(
            enable_interpolation=True,
            base_dict_type=dict,
            base_list_type=list,
            **kwargs,
        )
        loader.update_context(defined_vars)

        empty_model = self.conf_type.model_construct()

        as_dict = empty_model.model_dump()

        for field_name, field in self.conf_type.model_fields.items():
            if not hasattr(empty_model, field_name):
                as_dict[field_name] = DRACON_UNSET_VALUE

        dmp = loader.dump(as_dict)

        merge_str = "\n".join(list(self.make_merge_str(confs_to_merge)))

        if merge_str:
            dmp += '\n' + merge_str
        dmp += '\n' + override_str

        logger.debug(f"Parsed all args passed to commandline prog: {args}")
        logger.debug(f"Defined vars: {defined_vars}")
        logger.debug(f"Going to parse generated config:\n{dmp}\n")

        try:
            comp = loader.compose_config_from_str(dmp)
            real_name_map = {arg.real_name: arg for arg in self._args}
            # then we wrap all resolvable args in a !Resolvable[...] tag
            for field_name, field in self.conf_type.model_fields.items():
                if field_name in real_name_map:
                    arg = real_name_map[field_name]
                    if arg.resolvable:
                        field_t = get_inner_type(field.annotation)
                        if field_t is Any:
                            field_t = field.annotation
                        field_path = KeyPath(f'/{field_name}')
                        resolvable_node = field_path.get_obj(comp.root)
                        new_tag = f"!Resolvable[{field_t.__name__}]"
                        resolvable_node.tag = new_tag

            res = loader.load_composition_result(comp)
            resolve_all_lazy(res)
            res = self.conf_type(**res)
            if not isinstance(res, self.conf_type):
                raise ArgParseError(f"Expected {self.conf_type} but got {type(res)}")
            return res
        except ValidationError as e:
            # Intercept the validation error
            print()
            for error in e.errors():
                if error['type'] == 'missing':
                    print(f"Error: '{error['loc'][0]}' is required but was not provided.")
                else:
                    print(f"Validation Error: {error['loc']} - {error['msg']} - {error['type']}")
                    if 'ctx' in error:
                        print(f"Context: {error['ctx']}")
            print()

            print_help(self, None)
            print()


def make_program(conf_type: type, **kwargs):
    if not issubclass(conf_type, BaseModel):
        raise ValueError("make_program requires a BaseModel subclass")
    return Program[conf_type](conf_type=conf_type, **kwargs)

================
File: dracon/composer.py
================
## {{{                          --     imports     --{{{}}}
from ruamel.yaml.composer import Composer
from ruamel.yaml.nodes import Node, MappingNode, SequenceNode, ScalarNode

from dracon.utils import ftrace, deepcopy
from dracon.nodes import (
    DraconScalarNode,
    DraconMappingNode,
    DraconSequenceNode,
    IncludeNode,
    MergeNode,
    UnsetNode,
    DRACON_UNSET_VALUE,
)

from ruamel.yaml.events import (
    AliasEvent,
    ScalarEvent,
    SequenceStartEvent,
    MappingStartEvent,
)

from dracon.keypath import KeyPath, ROOTPATH, MAPPING_KEY
from pydantic import BaseModel, ConfigDict
from typing import Any, Hashable, Callable
from typing import Optional, List, Literal, Final

from dracon.interpolation import InterpolableNode
from dracon.interpolation_utils import outermost_interpolation_exprs
##────────────────────────────────────────────────────────────────────────────}}}

## {{{                   --     CompositionResult    --

SpecialNodeCategory = Literal['include', 'merge', 'instruction', 'interpolable']
INCLUDENODE: Final = 'include'
MERGENODE: Final = 'merge'
INTERPOLABLE: Final = 'interpolable'
INSTRUCTION: Final = 'instruction'

INCLUDE_TAG = '!include'


class CompositionResult(BaseModel):
    root: Node
    special_nodes: dict[SpecialNodeCategory, list[KeyPath]] = {}
    anchor_paths: Optional[dict[str, KeyPath]] = None
    node_map: Optional[dict[KeyPath, Node]] = None

    def __deepcopy__(self, memo=None):
        cr = CompositionResult(
            root=deepcopy(self.root, memo),
            special_nodes={},
            anchor_paths=deepcopy(self.anchor_paths, memo),
        )
        cr.make_map()
        return cr

    def __hash__(self):
        return hash(self.root)

    def model_post_init(self, *args, **kwargs):
        super().model_post_init(*args, **kwargs)
        for category in SpecialNodeCategory.__args__:
            self.special_nodes.setdefault(category, [])
        if self.node_map is None:
            self.make_map()
        if self.anchor_paths is None:
            self.find_anchors()

    def make_map(self):
        self.node_map = {}

        def _callback(node, path):
            self.node_map[path] = node  # type: ignore

        walk_node(self.root, _callback, start_path=ROOTPATH)

    def update_paths(self):
        # update the path attribute of all nodes
        assert self.node_map is not None
        for path, node in self.node_map.items():
            if hasattr(node, 'path'):
                node.path = path  # type: ignore

    def rerooted(self, new_root_path: KeyPath):
        cr = CompositionResult(root=new_root_path.get_obj(self.root))
        cr.make_map()
        cr.update_paths()
        return cr

    def set_at(self, at_path: KeyPath, new_node: Node):
        if at_path == ROOTPATH:
            self.root = new_node
        else:
            parent_node = at_path.parent.get_obj(self.root)

            if isinstance(parent_node, DraconMappingNode):
                key = at_path[-1]
                parent_node[key] = new_node
            elif isinstance(parent_node, DraconSequenceNode):
                idx = int(at_path[-1])  # type: ignore
                parent_node[idx] = new_node
            else:
                raise ValueError(f'Invalid parent node type: {type(parent_node)}')
        self.update_map_at(at_path)

    def update_map_at(self, at_path: KeyPath):
        if self.node_map is None:
            self.node_map = {}
        node = at_path.get_obj(self.root)
        self.node_map[at_path] = node

        def _callback(node, path):
            assert self.node_map is not None
            self.node_map[path] = node

        walk_node(node, _callback, start_path=at_path)

    def merge_composition_at(self, at_path: KeyPath, new_comp: 'CompositionResult', reuse_map=True):
        # new_comp.parent_path = self.parent_path + at_path[1:]
        new_node = new_comp.root
        self.set_at(at_path, new_node)
        # if reuse_map:
        # for path, node in new_comp.node_map.items():
        # self.node_map[at_path + path] = node

    def pop_all_special(self, category: SpecialNodeCategory):
        while self.special_nodes.get(category):
            yield self.special_nodes[category].pop()

    def sort_special_nodes(self, category: SpecialNodeCategory, reverse=False):
        nodes = self.special_nodes.get(category, [])
        self.special_nodes[category] = sorted(nodes, key=len, reverse=reverse)

    def walk_no_path(
        self,
        callback: Callable[[Node], None],
    ):
        assert self.node_map is not None
        for _, node in self.node_map.items():
            callback(node)

    def walk(
        self,
        callback: Callable[[Node, KeyPath], None],
    ):
        assert self.node_map is not None
        for path, node in self.node_map.items():
            callback(node, path)

    def find_special_nodes(
        self,
        category: SpecialNodeCategory,
        is_special: Callable[[Node], bool],
    ):
        special_nodes = []
        assert self.node_map is not None

        for path, node in self.node_map.items():
            if is_special(node):
                special_nodes.append(path)

        self.special_nodes[category] = special_nodes

    def find_anchors(self):
        assert self.node_map is not None

        def is_anchor(node):
            return hasattr(node, 'anchor') and (node.anchor is not None)

        self.anchor_paths = {}
        for path, node in self.node_map.items():
            if is_anchor(node):
                self.anchor_paths[node.anchor] = path

    def remove_from_context(self, ctx_key: str | list[str]):
        if isinstance(ctx_key, str):
            ctx_key = [ctx_key]
        for key in ctx_key:
            self.walk_no_path(
                lambda node: node.context.pop(key, None) if hasattr(node, 'context') else None
            )

    def print_context_keys(self):
        # print path: key1, key2, ...
        for path, node in self.node_map.items():
            if hasattr(node, 'context'):
                print(f'{path}: {", ".join(node.context.keys())}')

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def __repr__(self):
        return f'CompositionResult:{self.root}'

    def __str__(self):
        return f'CompositionResult:{self.root}'


def walk_node(node, callback, start_path=None):
    def __walk_node_no_path(node):
        callback(node)
        if isinstance(node, DraconMappingNode):
            for k_node, v_node in node.value:
                __walk_node_no_path(k_node)
                __walk_node_no_path(v_node)
        elif isinstance(node, DraconSequenceNode):
            for v in node.value:
                __walk_node_no_path(v)

    def __walk_node(node, path):
        callback(node, path)
        path = path.removed_mapping_key()
        if isinstance(node, DraconMappingNode):
            for k_node, v_node in node.value:
                __walk_node(k_node, path.with_added_parts(MAPPING_KEY, k_node.value))
                __walk_node(v_node, path.with_added_parts(k_node.value))
        elif isinstance(node, DraconSequenceNode):
            for i, v in enumerate(node.value):
                __walk_node(v, path.with_added_parts(str(i)))

    if start_path is not None:
        __walk_node(node, start_path)
    else:
        __walk_node_no_path(node)


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                      --     DraconComposer     --


class DraconComposer(Composer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.special_nodes: dict[SpecialNodeCategory, list[KeyPath]] = {}
        self.anchor_paths: dict[str, KeyPath] = {}
        self.interpolation_enabled = True
        self.merging_enabled = True
        self.root_node = None

    def get_result(self) -> CompositionResult:
        if self.root_node is not None:
            root_node = self.root_node
        else:
            # create an empty root node
            root_node = DraconMappingNode(value=[], tag='')

        return CompositionResult(
            root=root_node,
            special_nodes=self.special_nodes,
        )

    def add_special_node(self, category: SpecialNodeCategory, path: KeyPath):
        if category not in self.special_nodes:
            self.special_nodes[category] = []
        self.special_nodes[category].append(path.copy())

    def compose_node(self, parent, index):
        if self.parser.check_event(AliasEvent):  # *anchor
            node = self.compose_alias_event()
        else:
            event = self.parser.peek_event()

            self.resolver.descend_resolver(parent, index)
            if self.parser.check_event(ScalarEvent):
                if event.ctag == INCLUDE_TAG:
                    node = self.compose_include_node()
                elif event.style is None and is_merge_key(event.value) and self.merging_enabled:
                    node = self.compose_merge_node()
                else:
                    node = self.compose_scalar_node()
            elif self.parser.check_event(SequenceStartEvent):
                node = self.compose_sequence_node(event.anchor)
            elif self.parser.check_event(MappingStartEvent):
                node = self.compose_mapping_node(event.anchor)
            else:
                raise RuntimeError(f'Not a valid node event: {event}')
            self.resolver.ascend_resolver()

        node = self.wrapped_node(node)

        if parent is None:
            self.root_node = node

        return node

    def wrapped_node(self, node: Node) -> Node:
        if isinstance(node, MappingNode):
            return DraconMappingNode(
                tag=node.tag,
                value=node.value,
                start_mark=node.start_mark,
                end_mark=node.end_mark,
                flow_style=node.flow_style,
                comment=node.comment,
                anchor=node.anchor,
            )
        elif isinstance(node, SequenceNode):
            return DraconSequenceNode(
                tag=node.tag,
                value=node.value,
                start_mark=node.start_mark,
                end_mark=node.end_mark,
                flow_style=node.flow_style,
                comment=node.comment,
                anchor=node.anchor,
            )
        elif isinstance(node, (IncludeNode, MergeNode, InterpolableNode)):
            return node
        elif isinstance(node, ScalarNode):
            if node.value == DRACON_UNSET_VALUE:
                return UnsetNode()
            return DraconScalarNode(
                tag=node.tag,
                value=node.value,
                start_mark=node.start_mark,
                end_mark=node.end_mark,
                comment=node.comment,
                anchor=node.anchor,
            )
        else:
            raise NotImplementedError(f'Node type {type(node)} not supported')

    def compose_alias_event(self):
        event = self.parser.get_event()
        return IncludeNode(
            value=event.anchor,
            start_mark=event.start_mark,
            end_mark=event.end_mark,
            comment=event.comment,
        )

    def compose_scalar_node(self, anchor=None) -> Node:
        event = self.parser.get_event()
        tag = event.ctag

        if tag is None or str(tag) == '!':
            tag = self.resolver.resolve(ScalarNode, event.value, event.implicit)
            assert not isinstance(tag, str)

        node = ScalarNode(
            tag,
            event.value,
            event.start_mark,
            event.end_mark,
            style=event.style,
            comment=event.comment,
            anchor=event.anchor,
        )

        node = self.handle_interpolation(node)

        if node.anchor is not None:
            self.anchors[node.anchor] = node

        return node

    def handle_interpolation(self, node) -> Node:
        if self.interpolation_enabled:
            tag_iexpr = outermost_interpolation_exprs(node.tag)
            value_iexpr = (
                outermost_interpolation_exprs(node.value) if isinstance(node.value, str) else None
            )

            if tag_iexpr or value_iexpr:
                return InterpolableNode(
                    value=node.value,
                    start_mark=node.start_mark,
                    end_mark=node.end_mark,
                    tag=node.tag,
                    anchor=node.anchor,
                    comment=node.comment,
                    init_outermost_interpolations=value_iexpr,
                )
        return node

    def compose_include_node(self) -> Node:
        normal_node = self.compose_scalar_node()
        node = IncludeNode(
            value=normal_node.value,
            start_mark=normal_node.start_mark,
            end_mark=normal_node.end_mark,
            comment=normal_node.comment,
            anchor=normal_node.anchor,
        )
        return node

    def compose_merge_node(self) -> Any:
        event = self.parser.get_event()
        tag = event.ctag
        if tag is None or str(tag) == '!':
            tag = self.resolver.resolve(ScalarNode, event.value, event.implicit)
            assert not isinstance(tag, str)
        assert is_merge_key(event.value), f'Invalidly routed to merge node: {event.value}'
        node = MergeNode(
            value=event.value,
            tag=tag,
            start_mark=event.start_mark,
            end_mark=event.end_mark,
            comment=event.comment,
            anchor=event.anchor,
        )
        return node


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                           --     utils     --


def is_merge_key(value: str) -> bool:
    return value.startswith('<<')


def delete_unset_nodes(comp_res: CompositionResult):
    # when we delete an unset node, we have to check if the parent is a mapping node
    # and if we just made it empty. If so, we have to replace it with an UnsetNode
    # and so on, until we reach the root
    has_changed = False

    def _delete_unset_nodes(node: Node, parent: Optional[Node], key: Optional[Hashable]) -> Node:
        nonlocal has_changed
        if isinstance(node, DraconMappingNode):
            new_value = []
            for k, v in node.value:
                if isinstance(v, UnsetNode):
                    has_changed = True
                    continue
                new_value.append((k, _delete_unset_nodes(v, node, k)))
            if not new_value and not node.tag.startswith('!'):
                has_changed = True
                return UnsetNode()
            return DraconMappingNode(
                tag=node.tag,
                value=new_value,
                start_mark=node.start_mark,
                end_mark=node.end_mark,
                flow_style=node.flow_style,
                comment=node.comment,
                anchor=node.anchor,
            )
        elif isinstance(node, DraconSequenceNode):
            new_value = []
            for v in node.value:
                if isinstance(v, UnsetNode):
                    has_changed = True
                    continue
                new_value.append(_delete_unset_nodes(v, node, None))
            return DraconSequenceNode(
                tag=node.tag,
                value=new_value,
                start_mark=node.start_mark,
                end_mark=node.end_mark,
                flow_style=node.flow_style,
                comment=node.comment,
                anchor=node.anchor,
            )
        else:
            return node

    comp_res.root = _delete_unset_nodes(comp_res.root, None, None)

    return comp_res, has_changed


##────────────────────────────────────────────────────────────────────────────}}}

================
File: dracon/deferred.py
================
## {{{                          --     imports     --
from typing import Optional, Any, List, Dict, TypeVar, Generic, Type
import dracon.utils as utils
from dracon.utils import ftrace, deepcopy, ser_debug, node_repr
from dracon.utils import ShallowDict
from dracon.composer import (
    CompositionResult,
    walk_node,
)
from ruamel.yaml.nodes import Node
from dracon.nodes import (
    DraconScalarNode,
    ContextNode,
    context_node_hash,
)


from dracon.keypath import KeyPath, ROOTPATH
from dracon.merge import add_to_context, merged, MergeKey, reset_context

from functools import partial
from dracon.nodes import make_node
import logging

logger = logging.getLogger(__name__)

##────────────────────────────────────────────────────────────────────────────}}}

## {{{                         --     DeferredNode     --


T = TypeVar('T')


class DeferredNode(ContextNode, Generic[T]):
    """
    Allows to "pause" the composition of the contained node until construct is called
    All of dracons tree walking functions see this node as a leaf, i.e. it will not
    be traversed further.
    """

    def __init__(
        self,
        value: Node | T,
        path=ROOTPATH,
        obj_type: Optional[Type[T]] = None,
        clear_ctx: Optional[List[str] | bool] = None,
        loader=None,
        context=None,
        comp=None,
        **kwargs,
    ):
        from dracon.loader import DraconLoader

        if not isinstance(value, Node):
            value = make_node(value, **kwargs)

        self._clear_ctx = []

        if isinstance(clear_ctx, str):
            clear_ctx = [clear_ctx]

        if context is None or clear_ctx is True:
            context = ShallowDict()

        if isinstance(clear_ctx, list):
            self._clear_ctx = clear_ctx

        super().__init__(tag='', value=value, context=context, **kwargs)

        self.obj_type = obj_type

        for key in self._clear_ctx:
            if key in self.context:
                del self.context[key]

        if loader is None:
            self._loader = DraconLoader()
        else:
            self._loader = loader

        self.path = path

    def __getstate__(self):
        state = DraconScalarNode.__getstate__(self)
        state['path'] = self.path
        state['context'] = self.context
        state['obj_type'] = self.obj_type
        state['_loader'] = self._loader
        state['_full_composition'] = self._full_composition
        state['_clear_ctx'] = self._clear_ctx
        return state

    def __setstate__(self, state):
        DraconScalarNode.__setstate__(self, state)
        self.path = state['path']
        self.context = state['context']
        self.obj_type = state['obj_type']
        self._loader = state['_loader']
        self._clear_ctx = state['_clear_ctx']
        self._full_composition = state['_full_composition']

    @ftrace(watch=[])
    def update_context(self, context):
        add_to_context(context, self)

    @ftrace(watch=[])
    def compose(
        self,
        context: Optional[Dict[str, Any]] = None,
        deferred_paths: Optional[list[KeyPath | str]] = None,
        use_original_root: bool = False,
    ) -> Node:
        from dracon.loader import DraconLoader

        if self._loader is None:
            self._loader = DraconLoader()

        assert self._loader
        assert self._full_composition

        assert isinstance(self.path, KeyPath)
        assert isinstance(self.value, Node)

        deferred_paths = [KeyPath(p) if isinstance(p, str) else p for p in deferred_paths or []]

        logger.debug(f"Composing deferred node at {self.path}. deferred_paths={deferred_paths}")
        if not use_original_root:
            deferred_paths = [self.path + p[1:] for p in deferred_paths]

        self._loader.deferred_paths = deferred_paths

        composition = self._full_composition
        value = self.value

        ser_debug(context, operation='deepcopy')
        ser_debug(self.context, operation='deepcopy')

        merged_context = merged(self.context, context or {}, MergeKey(raw="{<~}[<~]"))
        merged_context = ShallowDict(merged_context)

        composition.set_at(self.path, value)

        composition.walk_no_path(
            callback=partial(
                add_to_context, self._loader.context, merge_key=MergeKey(raw='{>~}[>~]')
            )
        )

        walk_node(
            node=self.path.get_obj(composition.root),
            callback=partial(reset_context),
        )

        walk_node(
            node=self.path.get_obj(composition.root),
            callback=partial(add_to_context, merged_context, merge_key=MergeKey(raw='{<~}[<~]')),
        )

        compres = self._loader.post_process_composed(composition)

        return self.path.get_obj(compres.root)

    @ftrace(watch=[])
    def construct(self, **kwargs) -> T:  # type: ignore
        compres = self.compose(**kwargs)
        return self._loader.load_node(compres)

    @property
    def keypath_passthrough(self):
        # a deferred node should be transparent (we should be able to traverse it with a keypath)
        # A node that is not yet resolved, just a wrapper to another node
        return self.value

    def dracon_dump_to_node(self, representer) -> Node:
        val = deepcopy(self.value)
        if len(val.tag):
            val.tag = '!deferred:' + val.tag
        else:
            val.tag = '!deferred'
        return val

    def __hash__(self):
        return context_node_hash(self)

    def copy(self, clear_context=False, reroot=False, deepcopy_composition=True):
        """Create a copy with optional context clearing."""

        value_copy = deepcopy(self.value)
        context = {} if clear_context else self.context.copy()

        new_obj = DeferredNode(
            value=value_copy,
            path=deepcopy(self.path),
            obj_type=self.obj_type,
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            anchor=self.anchor,
            comment=self.comment,
            context=context,
        )
        new_obj._loader = self._loader.copy() if self._loader else None
        if not reroot:
            new_obj._full_composition = self._full_composition
            if deepcopy_composition:
                new_obj._full_composition = deepcopy(new_obj._full_composition)
        else:
            # new_comp = self._full_composition.rerooted(self.path)
            # new_obj._full_composition = new_comp
            # new_obj.path = ROOTPATH
            new_comp = CompositionResult(root=new_obj)
            new_obj._full_composition = new_comp
            new_obj.path = ROOTPATH


        return new_obj


def make_deferred(
    value: Any,
    loader=None,
    context=None,
    comp=None,
    path=ROOTPATH,
    clear_ctx=None,
    reroot=False,
    **kwargs,
) -> DeferredNode:
    from dracon.utils import ShallowDict

    if context is None or clear_ctx is True:
        context = ShallowDict()

    n = DeferredNode(
        value=make_node(value, **kwargs),
        context=context,
        path=path,
        clear_ctx=clear_ctx,
    )

    if comp is None:
        comp = CompositionResult(root=n)
    n._full_composition = comp

    n._loader = loader


    if reroot:
        n.path = ROOTPATH
        n._full_composition = comp.rerooted(n.path)
    else:
        n._full_composition = comp

    return n


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                     --     process deferred     --


def parse_query_params(query_string: str) -> Dict[str, Any]:
    """
    Parse URI-style query parameters into a dictionary with type conversion and list support
    For list values, the key should be repeated with the same name.
    For nested values, the key should be separated by a dot.
    """

    from urllib.parse import parse_qsl

    params = {}
    if not query_string:
        return params

    for key, value in parse_qsl(query_string, keep_blank_values=True):
        if value.lower() == "true":
            value = True
        elif value.lower() == "false":
            value = False
        elif value.lower() == "null" or value.lower() == "none":
            value = None
        else:
            try:
                if value.isdigit() or (value.startswith("-") and value[1:].isdigit()):
                    value = int(value)
                else:
                    value = float(value)
            except (ValueError, TypeError):
                pass

        if "." in key:
            parts = key.split(".")
            current = params
            for part in parts[:-1]:
                if part not in current:
                    current[part] = {}
                elif not isinstance(current[part], dict):
                    current[part] = {"_value": current[part]}
                current = current[part]

            last_part = parts[-1]
            if last_part in current:
                if isinstance(current[last_part], list):
                    current[last_part].append(value)
                else:
                    current[last_part] = [current[last_part], value]
            else:
                current[last_part] = value
        elif key in params:
            if isinstance(params[key], list):
                params[key].append(value)
            else:
                params[key] = [params[key], value]
        else:
            params[key] = value

    return params


@ftrace(watch=[])
def process_deferred(comp: CompositionResult, force_deferred_at: List[KeyPath | str] | None = None):
    """
    Wraps in a DeferredNode any node with a tag starting with '!deferred', or in a path that matches any in force_deferred_at
    """

    from dracon.nodes import reset_tag

    force_deferred_at = force_deferred_at or []
    force_deferred_at = [KeyPath(p) if isinstance(p, str) else p for p in force_deferred_at]
    deferred_nodes = []

    def find_deferred_nodes(node, path: KeyPath):
        if (
            not isinstance(node, DeferredNode)
            and node.tag.startswith('!deferred')
            or any(p.match(path) for p in force_deferred_at)  # type: ignore
        ):
            deferred_nodes.append((node, path))

    comp.walk(find_deferred_nodes)
    deferred_nodes = sorted(deferred_nodes, key=lambda x: len(x[1]), reverse=True)

    for node, path in deferred_nodes:
        qparams = {}
        if isinstance(node, DeferredNode):
            continue

        node_context = {}
        if hasattr(node, 'context'):
            node_context = node.context

        if node.tag.startswith('!deferred'):
            node.tag = node.tag[len('!deferred') :]
            if node.tag.startswith('::'):
                end = node.tag[2:].find(':')
                if end == -1:
                    query_string = node.tag[2:]
                else:
                    query_string = node.tag[2:end]
                qparams = parse_query_params(query_string)
                node.tag = node.tag[end + 1 :]

            if node.tag.startswith(':'):
                node.tag = '!' + node.tag[1:]
        else:
            assert any(
                p.match(path) for p in force_deferred_at
            ), f"node at path {path} is not deferred"

        if node.tag == "":
            reset_tag(node)

        new_node = make_deferred(
            value=node,
            path=path,
            context=node_context,
            comp=comp,
            **qparams,
        )
        comp.set_at(path, new_node)

    return comp


##────────────────────────────────────────────────────────────────────────────}}}

================
File: dracon/draconstructor.py
================
from ruamel.yaml.constructor import Constructor
import sys
import importlib
from ruamel.yaml.nodes import MappingNode, SequenceNode
from ruamel.yaml.constructor import ConstructorError
from typing import Dict, Any, Mapping, List
from dracon.merge import merged, MergeKey
import pydantic
import types
import pickle
from dracon.keypath import KeyPath, ROOTPATH

from dracon.interpolation import InterpolationError
from pydantic import (
    TypeAdapter,
    PydanticSchemaGenerationError,
)

import typing
import inspect
from dracon.utils import ShallowDict, ftrace, deepcopy
from dracon import dracontainer
from dracon.dracontainer import Dracontainer
from dracon.interpolation import outermost_interpolation_exprs, InterpolableNode
from dracon.lazy import LazyInterpolable, resolve_all_lazy, is_lazy_compatible
from dracon.resolvable import Resolvable, get_inner_type
from dracon.deferred import DeferredNode
from dracon.nodes import reset_tag


from typing import (
    Optional,
    Hashable,
    Type,
    Any,
    ForwardRef,
    List,
    get_origin,
)
from functools import partial
import logging

logger = logging.getLogger("dracon")

## {{{                        --     type utils     --


def pydantic_validate(tag, value, localns=None, root_obj=None, current_path=None):
    tag_type = resolve_type(tag, localns=localns or {})

    if not is_lazy_compatible(tag_type) and tag_type is not Any:
        resolve_all_lazy(value)
    try:
        return TypeAdapter(tag_type).validate_python(value)
    except PydanticSchemaGenerationError as e:
        instance = tag_type(value)  # we try a simple construction
        return instance


def resolve_type(
    type_str: str,
    localns: Optional[dict] = None,
    available_module_names: Optional[List[str]] = None,
) -> Type:
    if not type_str.startswith('!'):
        return Any

    type_str = type_str[1:]

    if available_module_names is None:
        available_module_names = ["__main__"]
    localns = localns or {}

    # Attempt regular import
    module_name, type_name = type_str.rsplit(".", 1) if "." in type_str else ("", type_str)
    if module_name:
        available_module_names = [module_name] + available_module_names

    for module_name in available_module_names:
        try:
            module = sys.modules.get(module_name) or importlib.import_module(module_name)
            if hasattr(module, type_name):
                return getattr(module, type_name)
        except ImportError:
            continue

    # Fall back to _eval_type
    if '.' in type_str:
        module_name, cname = type_str.rsplit('.', 1)
        try:
            module = importlib.import_module(module_name)
            localns[module_name] = module
            localns[type_str] = getattr(module, cname)
        except (ImportError, AttributeError):
            pass

    try:
        from typing import _eval_type

        return _eval_type(ForwardRef(type_str), globals(), localns)
    except NameError as e:
        raise ValueError(f"Failed to resolve type {type_str}. {e}") from None
    except Exception:
        return Resolvable if type_str.startswith('Resolvable[') else Any


def get_origin_type(t):
    orig = get_origin(t)
    if orig is None:
        return t
    return orig


def get_all_types(items):
    return {
        name: obj
        for name, obj in items.items()
        if isinstance(
            obj,
            (
                type,
                typing._GenericAlias,
                typing._SpecialForm,
                typing._SpecialGenericAlias,
            ),
        )
    }


def get_all_types_from_module(module):
    if isinstance(module, str):
        try:
            module = importlib.import_module(module)
        except ImportError:
            print(f"WARNING: Could not import module {module}")
            return {}
    return get_all_types(module.__dict__)


def get_globals_up_to_frame(frame_n):
    frames = inspect.stack()
    globalns = {}

    for frame_id in range(min(frame_n, len(frames) - 1), 0, -1):
        frame = frames[frame_id]
        globalns.update(frame.frame.f_globals)

    return globalns


def parse_resolvable_tag(tag):
    if tag.startswith('!'):
        tag = tag[1:]
    if tag.startswith('Resolvable['):
        inner = tag[11:-1]
        return inner
    return Any


def collect_all_types(modules, capture_globals=True, globals_at_frame=15):
    types = {}
    for module in modules:
        types.update(get_all_types_from_module(module))
    if capture_globals:
        globalns = get_globals_up_to_frame(globals_at_frame)
        types.update(get_all_types(globalns))
    return types


DEFAULT_TYPES = {
    'Any': Any,
    'Resolvable': Resolvable,
    'DraconResolvable': Resolvable,
}

DEFAULT_MODULES_FOR_TYPES = [
    'pydantic',
    'typing',
    'dracon',
    'numpy',
]

##────────────────────────────────────────────────────────────────────────────}}}


class Draconstructor(Constructor):
    def __init__(
        self,
        preserve_quotes=None,
        loader=None,
        reference_nodes=None,
        resolve_interpolations=False,
        capture_globals=False,
    ):
        Constructor.__init__(self, preserve_quotes=preserve_quotes, loader=loader)
        self.preserve_quotes = preserve_quotes
        self.yaml_base_dict_type = dracontainer.Mapping
        self.yaml_base_sequence_type = dracontainer.Sequence

        self.localns = collect_all_types(
            DEFAULT_MODULES_FOR_TYPES,
            capture_globals=capture_globals,
        )
        self.localns.update(get_all_types_from_module('__main__'))

        self.referenced_nodes = reference_nodes or {}
        self._depth = 0
        self._root_node = None
        self._current_path = ROOTPATH
        self.resolve_interpolations = resolve_interpolations
        self.context = None

    def base_construct_object(self, node: Any, deep: bool = False) -> Any:
        """deep is True when creating an object/mapping recursively,
        in that case want the underlying elements available during construction
        """
        if node in self.constructed_objects:
            return self.constructed_objects[node]
        if deep:
            old_deep = self.deep_construct
            self.deep_construct = True
        if node in self.recursive_objects:
            return self.recursive_objects[node]
        self.recursive_objects[node] = None
        data = self.construct_non_recursive_object(node)

        self.constructed_objects[node] = data
        try:
            del self.recursive_objects[node]
        except KeyError as e:
            msg = f"Failed to delete {node} from recursive objects: {e}"
            msg += f"\navailable = \n{self.recursive_objects}"
            logger.error(msg)

        if deep:
            self.deep_construct = old_deep
        return data

    # def construct_scalar(self, node):
    # if isinstance(node.value, str):
    # # Convert the string value to a raw string interpretation
    # rawstr = rf"{node.value}"
    # return rawstr
    # return super().construct_scalar(node)
    # @ftrace()
    def construct_object(self, node, deep=True):
        assert self.context is not None, "Context must be set before constructing objects"

        self.localns.update(DEFAULT_TYPES)
        self.localns.update(get_all_types(self.context))

        is_root = False
        if self._depth == 0:
            self._root_node = node
            is_root = True
            self._current_path = ROOTPATH
        self._depth += 1
        tag = node.tag

        try:
            tag_type = resolve_type(tag, localns=self.localns)

            if issubclass(get_origin_type(tag_type), Resolvable):
                return self.construct_resolvable(node, tag_type)

            if isinstance(node, DeferredNode):
                return node

            if isinstance(node, InterpolableNode):
                return self.construct_interpolable(node)

            if tag.startswith('!'):
                reset_tag(node)
            obj = self.base_construct_object(node, deep=True)

            node.tag = tag  # we don't want to permanently change the tag of the node because it might be referenced elsewhere

            obj = pydantic_validate(
                tag,
                obj,
                self.localns,
                root_obj=self._root_node,
                current_path=self._current_path,
            )

            if self.resolve_interpolations and is_root:
                resolve_all_lazy(obj)

            return obj

        finally:
            self._depth -= 1

    # @ftrace(watch=[])
    def construct_resolvable(self, node, tag_type):
        newnode = deepcopy(node)
        inner_type = get_inner_type(tag_type)
        if inner_type is Any:
            inner_type = parse_resolvable_tag(newnode.tag)
        if inner_type is Any:
            reset_tag(newnode)
        else:
            # check if it's a string or a type:
            if isinstance(inner_type, str):
                newnode.tag = f"!{inner_type}"
            else:
                newnode.tag = f"!{inner_type.__name__}"
        res = Resolvable(node=newnode, ctor=self, inner_type=inner_type)
        return res

    # @ftrace(watch=[])
    def construct_interpolable(self, node):
        node_value = node.value
        init_outermost_interpolations = node.init_outermost_interpolations
        validator = partial(pydantic_validate, node.tag, localns=self.localns)
        tag_iexpr = outermost_interpolation_exprs(node.tag)
        if tag_iexpr:  # tag is an interpolation itself
            # we can make a combo interpolation that evaluates
            # to a tuple of the resolved tag and value
            node_value = "${('" + str(node.tag) + "', " + str(node.value) + ")}"
            init_outermost_interpolations = outermost_interpolation_exprs(node_value)

            def validator_f(value, localns=self.localns):
                tag, value = value
                return pydantic_validate(tag, value, localns=localns)

            validator = partial(validator_f)

        context = ShallowDict(merged(self.context, node.context, MergeKey(raw='{<+}')))
        context['__DRACON_NODES'] = {
            i: Resolvable(node=n, ctor=self.copy()) for i, n in self.referenced_nodes.items()
        }

        lzy = LazyInterpolable(
            value=node_value,
            init_outermost_interpolations=init_outermost_interpolations,
            validator=validator,
            current_path=self._current_path,
            root_obj=self._root_node,
            context=context,
        )

        return lzy

    def copy(self):
        ctor = Draconstructor(
            preserve_quotes=self.preserve_quotes,
            loader=self.loader,
            reference_nodes=self.referenced_nodes,
        )
        ctor.context = self.context.copy()

        return ctor

    def __deepcopy__(self, memo):
        return self.copy()

    def construct_mapping(self, node: Any, deep: bool = False) -> Any:
        if not isinstance(node, MappingNode):
            raise ConstructorError(
                None,
                None,
                f"expected a mapping node, but found {node.id!s}",
                node.start_mark,
            )
        mapping = self.yaml_base_dict_type()
        for key_node, value_node in node.value:
            if key_node.tag == '!noconstruct' or value_node.tag == '!noconstruct':
                continue
            key = self.construct_object(key_node, deep=True)
            if not isinstance(key, Hashable):
                if isinstance(key, list):
                    key = tuple(key)
            if not isinstance(key, Hashable):
                raise ConstructorError(
                    "while constructing a mapping",
                    node.start_mark,
                    "found unhashable key",
                    key_node.start_mark,
                )
            if self._depth == 1:  # This is the root mapping node
                if isinstance(key, str) and key.startswith('__dracon__'):
                    continue

            value = self.construct_object(value_node, deep=deep)
            mapping[key] = value

        return mapping

================
File: dracon/dracontainer.py
================
from typing import Any, Dict, List, Union, TypeVar, Generic, Optional, Set
from collections.abc import MutableMapping, MutableSequence
from dracon.keypath import ROOTPATH, KeyPath
from dracon.utils import DictLike, ListLike, deepcopy, dict_like, list_like
from dracon.lazy import (
    Lazy,
    recursive_update_lazy_container,
    resolve_all_lazy,
)


K = TypeVar('K')
V = TypeVar('V')


class Tag:
    def __init__(self, name: str):
        self.name = name

    def __eq__(self, other):
        if isinstance(other, Tag):
            return self.name == other.name
        return False

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.name


INTERPOLABLE = Tag("intrp")


class Dracontainer:
    def __init__(self):
        self._auto_interp = True
        self._inplace_interp = True
        self._metadata = None
        self._per_item_metadata: Dict[Any, Any] = {}
        self._dracon_root_obj = self
        self._dracon_current_path = ROOTPATH
        self._dracon_lazy_resolve = True
        self._data = None

    def cleanup(self):
        """Clear internal references and caches"""
        if self._data:
            self._data.clear()
        if hasattr(self, '_metadata'):
            if self._metadata:
                self._metadata.clear()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.cleanup()

    def __deepcopy__(self, memo):
        new_obj = self.__class__()
        new_obj._auto_interp = self._auto_interp
        new_obj._inplace_interp = self._inplace_interp
        new_obj._metadata = deepcopy(self._metadata, memo)
        new_obj._per_item_metadata = deepcopy(self._per_item_metadata, memo)
        new_obj._dracon_root_obj = self._dracon_root_obj
        new_obj._dracon_current_path = self._dracon_current_path
        new_obj._dracon_lazy_resolve = self._dracon_lazy_resolve
        new_obj._data = deepcopy(self._data, memo)

        if self._dracon_root_obj is self:
            new_obj._dracon_root_obj = new_obj
        recursive_update_lazy_container(
            new_obj,
            root_obj=new_obj._dracon_root_obj,
            current_path=new_obj._dracon_current_path,
        )

        return new_obj

    def __copy__(self):
        new_obj = self.__class__()
        new_obj._auto_interp = self._auto_interp
        new_obj._inplace_interp = self._inplace_interp
        new_obj._metadata = self._metadata
        new_obj._per_item_metadata = self._per_item_metadata
        new_obj._dracon_root_obj = self._dracon_root_obj
        new_obj._dracon_current_path = self._dracon_current_path
        new_obj._dracon_lazy_resolve = self._dracon_lazy_resolve
        new_obj._data = self._data

        if self._dracon_root_obj is self:
            new_obj._dracon_root_obj = new_obj
        recursive_update_lazy_container(
            new_obj,
            root_obj=new_obj._dracon_root_obj,
            current_path=new_obj._dracon_current_path,
        )

        return new_obj

    def set_metadata(self, metadata):
        self._metadata = metadata

    def get_metadata(self):
        return self._metadata

    def set_item_metadata(self, key, metadata):
        self._per_item_metadata[key] = metadata

    def get_item_metadata(self, key):
        return self._per_item_metadata.get(key)

    def __setitem__(self, key, value):
        raise NotImplementedError

    def __getitem__(self, key):
        raise NotImplementedError

    def __iter__(self):
        raise NotImplementedError

    def __setattr__(self, key, value):
        if key.startswith('_'):
            super().__setattr__(key, value)
        else:
            self[key] = value

    def _handle_lazy(self, name, value):
        if isinstance(value, Lazy) and self._dracon_lazy_resolve:
            value.name = name
            newval = value.get(self, setval=True)
            return newval
        return value

    @classmethod
    def create(cls, data: Union[DictLike[K, V], ListLike[V], None] = None):
        if dict_like(data):
            return Mapping(data)
        elif list_like(data):
            return Sequence(data)
        else:
            raise ValueError("Input must be either a dict or a list")

    def _to_dracontainer(self, value: Any, key: Any):
        newval = value
        if isinstance(value, (Mapping, Sequence)):
            newval = value
        elif isinstance(value, dict):
            newval = Mapping(value)
        elif isinstance(value, list) and not isinstance(value, str):
            newval = Sequence(value)

        recursive_update_lazy_container(
            newval,
            root_obj=self._dracon_root_obj,
            current_path=self._dracon_current_path + KeyPath(str(key)),
        )

        return newval

    def set_lazy_resolve(self, value, recursive=True):
        self._dracon_lazy_resolve = value
        if recursive:
            for item in self:
                if isinstance(item, Dracontainer):
                    item.set_lazy_resolve(value, recursive=True)

    def resolve_all_lazy(self):
        resolve_all_lazy(self)


class Mapping(Dracontainer, MutableMapping[K, V], Generic[K, V]):
    def __init__(self, data: Optional[DictLike[K, V]] = None):
        super().__init__()
        self._data: Dict[K, V] = {}
        if data:
            for key, value in data.items():
                self[key] = value

    def __getattr__(self, key):
        try:
            _data = object.__getattribute__(self, '_data')
            if key in _data:
                return self[key]
        except AttributeError:
            pass
        raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{key}'")

    def __getitem__(self, key):
        element = self._data[key]
        return self._handle_lazy(key, element)

    def __setitem__(self, key: K, value: V):
        self._data[key] = self._to_dracontainer(value, key)

    def update(self, other):
        for key, value in other.items():
            self._data[key] = self._to_dracontainer(value, key)

    def __delitem__(self, key):
        del self._data[key]
        if key in self._per_item_metadata:
            del self._per_item_metadata[key]

    def __iter__(self):
        return iter(self._data)

    def __len__(self):
        return len(self._data)

    def __repr__(self):
        return f"{self.__class__.__name__}({self._data})"

    def __contains__(self, key):
        return key in self._data

    def items(self):
        return self._data.items()

    def keys(self):
        return self._data.keys()

    def values(self):
        return self._data.values()

    def copy(self):
        # shallow copy
        return Mapping(self._data.copy())


class Sequence(Dracontainer, MutableSequence[V]):
    def __init__(self, data: Optional[ListLike[V]] = None):
        super().__init__()
        self._data: List[V] = []
        if data:
            for item in data:
                self.append(item)

    def __setitem__(self, index, value):
        index = int(index)
        self._data[index] = self._to_dracontainer(value, index)  # type: ignore

    def __delitem__(self, index):
        del self._data[index]
        if index in self._per_item_metadata:
            del self._per_item_metadata[index]

    def __getitem__(self, index):  # type: ignore
        index = int(index)
        element = self._data[index]
        return self._handle_lazy(str(index), element)

    def __len__(self):
        return len(self._data)

    def __repr__(self):
        return f"{self.__class__.__name__}({self._data})"

    def __contains__(self, value):
        return value in self._data

    def __iter__(self):
        return iter(self._data)

    def __reversed__(self):
        return reversed(self._data)

    def clear(self):
        self._data.clear()

    def insert(self, index: int, value: V):
        self._data.insert(index, self._to_dracontainer(value, index))

    def append(self, value: V):
        self._data.append(self._to_dracontainer(value, key=len(self._data)))

    def __append__(self, value):
        self.append(value)

    def extend(self, values):
        for value in values:
            self.append(value)

    def __add__(self, other: 'Sequence[V]') -> 'Sequence[V]':
        new_data = deepcopy(self._data)
        new_data.extend(other)
        return Sequence(new_data)  # type: ignore

    def __eq__(self, other):
        if isinstance(other, Sequence):
            return self._data == other._data
        elif isinstance(other, List):
            return self._data == other
        return False


def create_dracontainer(
    data: Union[Dict, List],
) -> Union[Mapping, Sequence]:
    return Dracontainer.create(data)

================
File: dracon/include.py
================
from dataclasses import dataclass
from typing import Any, Optional, Dict, Tuple, Callable
import dracon.utils as utils
from functools import partial
import re
from dracon.keypath import KeyPath, ROOTPATH
from dracon.composer import (
    IncludeNode,
    CompositionResult,
)
from dracon.interpolation_utils import resolve_interpolable_variables
from dracon.interpolation import evaluate_expression
from dracon.merge import merged, MergeKey
from dracon.utils import deepcopy, ftrace
from dracon.deferred import DeferredNode

from dracon.merge import add_to_context
from dracon.loaders.file import read_from_file
from dracon.loaders.pkg import read_from_pkg
from dracon.loaders.env import read_from_env

DEFAULT_LOADERS: Dict[str, Callable] = {
    'file': read_from_file,
    'pkg': read_from_pkg,
    'env': read_from_env,
}


@dataclass
class IncludeComponents:
    """Represents the parsed components of an include string."""

    main_path: str
    key_path: str

    @property
    def path(self) -> str:
        return KeyPath(self.main_path) + KeyPath(self.key_path)


@dataclass
class LoaderResult:
    """Represents the result of a loader operation."""

    result: Any
    context: dict


def parse_include_str(include_str: str) -> IncludeComponents:
    """Parse an include string into its main path and key path components."""
    if '@' in include_str:
        main_path, key_path = re.split(r'(?<!\\)@', include_str, maxsplit=1)
    else:
        main_path, key_path = include_str, ''
    return IncludeComponents(main_path, key_path)


def handle_in_memory_include(
    name: str, node: 'IncludeNode', key_path: str, dump_to_node_fn: Callable
) -> CompositionResult:
    """Handle an in-memory include (starting with $)."""
    if name not in node.context:
        raise ValueError(f'Invalid in-memory include: {name} not found')

    incl_node = node.context[name]
    incl_node = dump_to_node_fn(incl_node)
    if key_path:
        incl_node = KeyPath(key_path).get_obj(incl_node)

    return CompositionResult(root=incl_node)


def handle_absolute_path(
    main_path: str, composition_result: CompositionResult
) -> CompositionResult:
    return composition_result.rerooted(KeyPath(main_path))


def handle_relative_path(
    main_path: str, include_node_path: KeyPath, composition_result: CompositionResult
) -> CompositionResult:
    comb_path = include_node_path.parent.down(KeyPath(main_path))
    return composition_result.rerooted(comb_path)


def handle_anchor_path(
    components: IncludeComponents,
    anchors: Dict[str, KeyPath],
    composition_result: CompositionResult,
) -> CompositionResult:
    return composition_result.rerooted(
        composition_result.anchor_paths[components.main_path] + components.key_path
    )


@ftrace(watch=[])
def compose_from_include_str(
    draconloader,
    include_str: str,
    include_node_path: KeyPath = ROOTPATH,
    composition_result: Optional[CompositionResult] = None,
    custom_loaders: dict = DEFAULT_LOADERS,
    node: Optional[IncludeNode] = None,  #
) -> Any:
    context = draconloader.context if not node else node.context

    # there are 2 syntaxes for include string interpolations:
    # first is any variable that starts with $, like $DIR (which is set by the file loader)
    # second is the usual interpolation syntax, like ${some_expression}.
    # we resolve both of them here.
    include_str = evaluate_expression(
        include_str,
        current_path=include_node_path,
        root_obj=composition_result.root if composition_result else None,
        context=context,
    )
    include_str = resolve_interpolable_variables(include_str, context)  # type: ignore

    components = parse_include_str(include_str)
    result = None
    file_context = {}

    try:
        if composition_result is not None:
            assert isinstance(composition_result.anchor_paths, dict)

            if components.main_path.startswith('$'):
                assert node is not None
                result = handle_in_memory_include(
                    components.main_path[1:], node, components.key_path, draconloader.dump_to_node
                )

            elif components.main_path.startswith('/'):
                assert not components.key_path, 'Invalid key path for relative path include'
                result = handle_absolute_path(components.main_path, composition_result)

            elif components.main_path.startswith('@') or components.main_path.startswith('.'):
                assert not components.key_path, 'Invalid key path for relative path include'
                result = handle_relative_path(
                    components.main_path, include_node_path, composition_result
                )
            elif components.main_path in composition_result.anchor_paths:
                result = handle_anchor_path(
                    components, composition_result.anchor_paths, composition_result
                )

            if result is not None:
                result.root = deepcopy(result.root)
                return result

            assert (
                ':' in components.main_path
            ), f'Invalid include path: anchor {components.main_path} not found in document'

        assert (
            ':' in components.main_path
        ), f'Invalid include path: {components.main_path}. No loader specified.'

        loader_name, path = components.main_path.split(':', 1)
        if loader_name not in custom_loaders:
            raise ValueError(f'Unknown loader: {loader_name}')

        result, new_context = custom_loaders[loader_name](path)
        file_context = new_context
        draconloader.update_context(new_context)

        if not isinstance(result, CompositionResult):
            if not isinstance(result, str):
                raise ValueError(f"Invalid result type from loader '{loader_name}': {type(result)}")
            new_loader = draconloader.copy()
            if node is not None:
                merged_context = merged(node.context, new_context, MergeKey(raw="{<~}[<~]"))
                add_to_context(merged_context, new_loader)

            result = new_loader.compose_config_from_str(result)
        if components.key_path:
            result = result.rerooted(KeyPath(components.key_path))
        return result

    finally:
        if isinstance(result, CompositionResult) and node is not None:
            result.make_map()
            merged_context = merged(
                node.context, file_context, MergeKey(raw="{<~}[~<]")
            )  # Changed to +>
            result.walk_no_path(
                callback=partial(add_to_context, merged_context, merge_key=MergeKey(raw='{>~}[~>]'))
            )

================
File: dracon/instructions.py
================
## {{{                          --     imports     --
from typing import Optional, Any
import re
import time
from pydantic import BaseModel
from enum import Enum
from dracon.utils import dict_like, DictLike, ListLike, ftrace, deepcopy, node_repr, ser_debug
from dracon.composer import (
    CompositionResult,
    walk_node,
    DraconMappingNode,
    DraconSequenceNode,
    IncludeNode,
)
from dracon.utils import ShallowDict
from ruamel.yaml.nodes import Node
from dracon.keypath import KeyPath, ROOTPATH
from dracon.merge import merged, MergeKey, add_to_context
from dracon.interpolation import evaluate_expression, InterpolableNode
from dracon.deferred import DeferredNode, make_deferred
from functools import partial
from dracon.nodes import DraconScalarNode
import logging

logger = logging.getLogger(__name__)

##────────────────────────────────────────────────────────────────────────────}}}

## {{{                      --     instruct utils     --


class Instruction:
    @staticmethod
    def match(value: Optional[str]) -> Optional['Instruction']:
        raise NotImplementedError

    def process(self, comp_res: CompositionResult, path: KeyPath, loader) -> CompositionResult:
        raise NotImplementedError


@ftrace()
def process_instructions(comp_res: CompositionResult, loader) -> CompositionResult:
    # then all other instructions
    instruction_nodes = []
    seen_paths = set()

    def find_instruction_nodes(node: Node, path: KeyPath):
        nonlocal instruction_nodes
        nonlocal seen_paths
        if hasattr(node, 'tag') and node.tag:
            if (path not in seen_paths) and (inst := match_instruct(node.tag)):
                instruction_nodes.append((inst, path))

    def refresh_instruction_nodes():
        nonlocal instruction_nodes
        instruction_nodes = []
        comp_res.make_map()
        comp_res.walk(find_instruction_nodes)
        instruction_nodes = sorted(instruction_nodes, key=lambda x: len(x[1]))

    refresh_instruction_nodes()

    while instruction_nodes:
        inst, path = instruction_nodes.pop(0)
        assert path not in seen_paths, f"Instruction {inst} at {path} already processed"
        seen_paths.add(path)
        comp_res = inst.process(comp_res, path.copy(), loader)
        refresh_instruction_nodes()

    return comp_res


##────────────────────────────────────────────────────────────────────────────}}}
## {{{                          --     define     --
class Define(Instruction):
    """
    `!define var_name : value`

    Define a variable var_name with the value of the node
    and add it to the parent node's context
    The node is then removed from the parent node
    (if you want to define and keep the node, use !define_keep)

    If value is an interpolation, this node triggers composition-time evaluation
    """

    @staticmethod
    def match(value: Optional[str]) -> Optional['Define']:
        if not value:
            return None
        if value == '!define':
            return Define()
        return None

    def get_name_and_value(self, comp_res, path, loader):
        if not path.is_mapping_key():
            raise ValueError(
                f"instruction {self.__class__.__name__} must be a mapping key, but got {path}"
            )
        key_node = path.get_obj(comp_res.root)
        value_node = path.removed_mapping_key().get_obj(comp_res.root)
        parent_node = path.parent.get_obj(comp_res.root)
        assert isinstance(parent_node, DraconMappingNode)

        if isinstance(value_node, InterpolableNode):
            value = evaluate_expression(
                value_node.value,
                current_path=path,
                root_obj=comp_res.root,
                context=value_node.context,
            )
        else:
            value = loader.load_composition_result(CompositionResult(root=value_node))

        var_name = key_node.value
        assert (
            var_name.isidentifier()
        ), f"Invalid variable name in {self.__class__.__name__} instruction: {var_name}"

        del parent_node[var_name]

        return var_name, value, parent_node

    @ftrace(watch=[])
    def process(self, comp_res: CompositionResult, path: KeyPath, loader):
        var_name, value, parent_node = self.get_name_and_value(comp_res, path, loader)

        walk_node(
            node=parent_node,
            callback=partial(add_to_context, {var_name: value}),
        )

        return comp_res


class SetDefault(Define):
    """
    `!set_default var_name : default_value`

    Similar to !define, but only sets the variable if it doesn't already exist in the context

    If value is an interpolation, this node triggers composition-time evaluation
    """

    @staticmethod
    def match(value: Optional[str]) -> Optional['SetDefault']:
        if not value:
            return None
        if value == '!set_default':
            return SetDefault()
        return None

    @ftrace(watch=[])
    def process(self, comp_res: CompositionResult, path: KeyPath, loader):
        var_name, value, parent_node = self.get_name_and_value(comp_res, path, loader)

        walk_node(
            node=parent_node,
            callback=partial(
                add_to_context, {var_name: value}, merge_key=MergeKey(raw='<<{>~}[>~]')
            ),
        )

        return comp_res


##────────────────────────────────────────────────────────────────────────────}}}
## {{{                           --     each     --


class Each(Instruction):
    PATTERN = r"!each\(([a-zA-Z_]\w*)\)"

    """
    `!each(var_name) list-like-expr : value`

    Duplicate the value node for each item in the list-like node and assign the item 
    to the variable var_name (which is added to the context).
    
    If list-like-expr is an interpolation, this node triggers its composition-time evaluation.

    For sequence values:
        !each(i) ${range(3)}:
            - value_${i}
    
    For mapping values with dynamic keys:
        !each(i) ${range(3)}:
            key_${i}: value_${i}

    Removed from final composition.
    """

    def __init__(self, var_name: str):
        self.var_name = var_name

    @staticmethod
    def match(value: Optional[str]) -> Optional['Each']:
        if not value:
            return None
        match = re.match(Each.PATTERN, value)
        if match:
            var_name = match.group(1)
            return Each(var_name)
        return None

    @ftrace(inputs=False, watch=[])
    def process(self, comp_res: CompositionResult, path: KeyPath, loader):
        if not path.is_mapping_key():
            raise ValueError(f"instruction 'each' must be a mapping key, but got {path}")

        base_key_node = path.get_obj(comp_res.root)
        base_value_node = path.removed_mapping_key().get_obj(comp_res.root)

        key_node = base_key_node
        value_node = base_value_node

        parent_node = path.parent.get_obj(comp_res.root)

        assert isinstance(parent_node, DraconMappingNode)
        assert isinstance(
            key_node, InterpolableNode
        ), f"Expected an interpolable node for 'each' instruction, but got {key_node}, a {type(key_node)}"

        list_like = evaluate_expression(
            key_node.value,
            current_path=path,
            root_obj=comp_res.root,
            context=key_node.context,
        )

        logger.debug(
            f"Processing each instruction, key_node.context.{self.var_name}={key_node.context.get(self.var_name)}"
        )

        # remove the original each instruction node
        new_parent = parent_node.copy()
        del new_parent[key_node.value]

        mkey = MergeKey(raw='{<~}[~<]')
        # Handle sequence values
        if isinstance(value_node, DraconSequenceNode):
            assert len(parent_node) == 1, "Cannot use !each with a sequence node in a mapping"
            new_parent = DraconSequenceNode.from_mapping(parent_node, empty=True)
            logger.debug(f"Processing an each instruction with a sequence node. {list_like=}")

            for item in list_like:
                logger.debug(f"  each: {item=}")
                item_ctx = ShallowDict({self.var_name: item})
                logger.debug(
                    f"  after merge into key_node.ctx, item_ctx.{self.var_name}={item_ctx.get(self.var_name)}"
                )
                for node in value_node.value:
                    if isinstance(node, DeferredNode):
                        new_value_node = node.copy(deepcopy_composition=False)
                    else:
                        new_value_node = deepcopy(node)

                    walk_node(
                        node=new_value_node,
                        callback=partial(add_to_context, item_ctx, merge_key=mkey),
                    )

                    new_parent.append(new_value_node)

        # Handle mapping values
        elif isinstance(value_node, DraconMappingNode):
            logger.debug(f"Processing an each instruction with a dict node. {list_like=}")
            for item in list_like:
                item_ctx = merged(key_node.context, {self.var_name: item}, MergeKey(raw='{<~}'))
                for knode, vnode in value_node.items():
                    new_vnode = deepcopy(vnode)
                    # can't add the knode directly to the new_parent, as that would result in duplicate keys
                    # we need to evaluate the key node first
                    assert isinstance(
                        knode, InterpolableNode
                    ), f"Keys inside an !each instruction must be interpolable (so that they're unique), but got {knode}"
                    new_knode = deepcopy(knode)
                    add_to_context(item_ctx, new_knode, mkey)
                    scalar_knode = DraconScalarNode(tag=new_knode.tag, value=new_knode.evaluate())
                    new_parent.append((scalar_knode, new_vnode))
                    walk_node(
                        node=new_vnode,
                        callback=partial(add_to_context, item_ctx, merge_key=mkey),
                    )

        else:
            raise ValueError(
                f"Invalid value node for 'each' instruction: {value_node} of type {type(value_node)}"
            )

        # del parent_node[key_node.value]
        comp_res.set_at(path.parent, new_parent)

        return comp_res


##────────────────────────────────────────────────────────────────────────────}}}
## {{{                            --     if     --


def as_bool(value: str | int | bool) -> bool:
    if isinstance(value, bool):
        return value
    if isinstance(value, int):
        return bool(value)
    if isinstance(value, str):
        try:
            return bool(int(value))
        except ValueError:
            pass
        if value.lower() in ['true']:
            return True
        if value.lower() in ['false', 'null', 'none', '']:
            return False
    raise ValueError(f"Could not convert {value} to bool")


class If(Instruction):
    """
    `!if expr : value`

    Evaluate the truthiness of expr (if it's an interpolation, it evaluates it).
    If truthy, then value replaces this entire node.
    If falsy, then the entire node is removed.
    """

    @staticmethod
    def match(value: Optional[str]) -> Optional['If']:
        if not value:
            return None
        if value == '!if':
            return If()
        return None

    @ftrace(watch=[])
    def process(self, comp_res: CompositionResult, path: KeyPath, loader):
        if not path.is_mapping_key():
            raise ValueError(f"instruction 'if' must be a key, but got {path}")

        value_path = path.removed_mapping_key()
        parent_path = path.parent

        key_node = path.get_obj(comp_res.root)
        value_node = value_path.get_obj(comp_res.root)
        parent_node = parent_path.get_obj(comp_res.root)

        assert key_node.tag == '!if', f"Expected tag '!if', but got {key_node.tag}"

        expr = key_node.value

        if isinstance(key_node, InterpolableNode):
            evaluated_expr = bool(
                evaluate_expression(
                    expr,
                    current_path=path,
                    root_obj=comp_res.root,
                    context=key_node.context,
                )
            )
        else:
            evaluated_expr = as_bool(expr)

        if evaluated_expr:
            if isinstance(value_node, DraconMappingNode):
                assert isinstance(
                    parent_node, DraconMappingNode
                ), 'if statement with mapping must appear in a mapping'
                for key, node in value_node.items():
                    parent_node.append((key, node))
            elif isinstance(value_node, DraconSequenceNode):
                raise NotImplementedError(
                    "if statement containing a sequence is not yet implemented"
                )
            else:
                assert isinstance(
                    parent_node, DraconMappingNode
                ), 'if statement with scalar-like must appear in a mapping'
                comp_res.set_at(parent_path, value_node)

        del parent_node[key_node.value]
        return comp_res


##────────────────────────────────────────────────────────────────────────────}}}

AVAILABLE_INSTRUCTIONS = [SetDefault, Define, Each, If]


def match_instruct(value: str) -> Optional[Instruction]:
    matches = [inst.match(value) for inst in AVAILABLE_INSTRUCTIONS]
    # need to refresh
    for match in matches:
        if match:
            return match
    return None

================
File: dracon/interpolation_utils.py
================
from typing import (
    Any,
    Dict,
    Literal,
)
from pydantic.dataclasses import dataclass
from functools import lru_cache

import pyparsing as pp
import re
from dracon.utils import ftrace, DictLike


class InterpolationError(Exception):
    pass


BASE_DRACON_SYMBOLS: Dict[str, Any] = {}

## {{{                    --     interpolation exprs     --


@dataclass
class InterpolationMatch:
    start: int
    end: int
    expr: str

    def contains(self, pos: int) -> bool:
        return self.start <= pos < self.end


def fast_prescreen_interpolation_exprs_check(  # 5000x faster prescreen but very simple and limited
    text: str, interpolation_start_char='$', interpolation_boundary_chars=('{}', '()')
) -> bool:
    start_patterns = [interpolation_start_char + bound[0] for bound in interpolation_boundary_chars]
    for start_pattern in start_patterns:
        if start_pattern in text:
            return True
    return False


@lru_cache(maxsize=1024)
def outermost_interpolation_exprs(
    text: str, interpolation_start_char='$', interpolation_boundary_chars=('{}', '()')
) -> list[InterpolationMatch]:
    matches = []
    if not fast_prescreen_interpolation_exprs_check(
        text, interpolation_start_char, interpolation_boundary_chars
    ):
        return matches

    scanner = pp.MatchFirst(
        [
            pp.originalTextFor(pp.nestedExpr(bounds[0], bounds[1]))
            for bounds in interpolation_boundary_chars
        ]
    )
    scanner = pp.Combine(interpolation_start_char + scanner)
    for match, start, end in scanner.scanString(text):
        matches.append(InterpolationMatch(start, end, match[0][2:-1]))
    return sorted(matches, key=lambda m: m.start)


def outermost_comptime_interpolations(text: str) -> list[InterpolationMatch]:
    return outermost_interpolation_exprs(
        text, interpolation_start_char='$', interpolation_boundary_chars=('()',)
    )


def outermost_lazy_interpolations(text: str) -> list[InterpolationMatch]:
    return outermost_interpolation_exprs(
        text, interpolation_start_char='$', interpolation_boundary_chars=('{}',)
    )


##────────────────────────────────────────────────────────────────────────────}}}
## {{{             --     find references [@,&](keypaths, anchors)     --

# Find all field references in an expression string and replace them with a function call


@dataclass
class ReferenceMatch:
    start: int
    end: int
    expr: str
    symbol: Literal['@', '&']


NOT_ESCAPED_REGEX = r"(?<!\\)(?:\\\\)*"
# INVALID_KEYPATH_CHARS = r'[]() ,:=+-*%<>!&|^~@#$?;{}"\'`'
INVALID_KEYPATH_CHARS = r'[]() ,+-*%<>!&|^~@#$?;{}"\'`'
SPECIAL_KEYPATH_CHARS = './\\'  # Added backslash to handle escaping of itself


def find_field_references(expr: str) -> list[ReferenceMatch]:
    # Regex pattern to match keypaths
    pattern = f"{NOT_ESCAPED_REGEX}[&@]([^{re.escape(INVALID_KEYPATH_CHARS)}]|(?:\\\\.))*"

    matches = []
    for match in re.finditer(pattern, expr):
        start, end = match.span()
        full_match = match.group()
        keypath = full_match[1:]
        symbol = full_match[0]
        assert symbol in ('@', '&')

        # Clean up escaping, but keep backslashes for special keypath characters
        cleaned_keypath = ''
        i = 0
        while i < len(keypath):
            if keypath[i] == '\\' and i + 1 < len(keypath):
                if keypath[i + 1] in SPECIAL_KEYPATH_CHARS:
                    cleaned_keypath += keypath[i : i + 2]
                    i += 2
                else:
                    cleaned_keypath += keypath[i + 1]
                    i += 2
            else:
                cleaned_keypath += keypath[i]
                i += 1

        # Check if the keypath ends with an odd number of backslashes
        if len(keypath) - len(keypath.rstrip('\\')) % 2 == 1:
            end -= 1
            cleaned_keypath = cleaned_keypath[:-1]

        matches.append(ReferenceMatch(start, end, cleaned_keypath, symbol))

    return matches


##────────────────────────────────────────────────────────────────────────────}}}
## {{{                --     find interpolable variables     --

# an interpolable variable is a special $VARIABLE defined by dracon (or the user)
# they are immmediately replaced by their value when found in the expression string
# pattern is $ + CAPITAL_LETTER + [a-zA-Z0-9_]


@dataclass
class VarMatch:
    start: int
    end: int
    varname: str


def find_interpolable_variables(expr: str) -> list[VarMatch]:
    matches = []
    for match in re.finditer(rf"{NOT_ESCAPED_REGEX}\$[A-Z][a-zA-Z0-9_]*", expr):
        start, end = match.span()
        matches.append(VarMatch(start, end, match.group()))
    return matches


def resolve_interpolable_variables(expr: str, symbols: DictLike[str, Any]) -> str:
    var_matches = find_interpolable_variables(expr)
    if not var_matches:
        return expr
    offset = 0
    for match in var_matches:
        if match.varname not in symbols:
            raise InterpolationError(f"Variable {match.varname} not found in {symbols=}")
        newexpr = str(symbols[match.varname])
        expr = expr[: match.start + offset] + newexpr + expr[match.end + offset :]
        original_len = match.end - match.start
        offset += len(newexpr) - original_len
    return expr


##────────────────────────────────────────────────────────────────────────────}}}

================
File: dracon/interpolation.py
================
## {{{                          --     imports     --
from asteval import Interpreter
from typing import (
    Any,
    Dict,
    Optional,
    List,
)
from dracon.keypath import KeyPath
from copy import copy
from typing import (
    Protocol,
    runtime_checkable,
)
from dracon.utils import DictLike, ftrace, deepcopy, ser_debug
import dracon.utils as utils
from dracon.nodes import DraconMappingNode, ContextNode

from dracon.interpolation_utils import (
    outermost_interpolation_exprs,
    InterpolationMatch,
    find_field_references,
    resolve_interpolable_variables,
)


##────────────────────────────────────────────────────────────────────────────}}}


class DraconError(Exception):
    pass


class InterpolationError(DraconError):
    pass


BASE_DRACON_SYMBOLS: Dict[str, Any] = {}


def debug_string_state(label: str, s: str):
    print(f"\n=== {label} ===")
    print(f"Raw string: {repr(s)}")
    print("Backslash count: ", {s.count('\\')})
    print("=" * 40)


## {{{                        --     NodeLookup     --


class NodeLookup:
    """a DictLike that allows for keypaths to be used as keys"""

    def __init__(self, root_node=None):
        self.root_node = root_node
        self.available_paths: set[str] = set()

    def __getitem__(self, keypathstr: str):
        if keypathstr not in self.available_paths:
            raise KeyError(
                f"KeyPath {keypathstr} not found in NodeLookup. Available paths: {self.available_paths}"
            )
        keypath = KeyPath(keypathstr)
        obj = keypath.get_obj(self.root_node)
        return obj

    def items(self):
        for keypathstr in self.available_paths:
            yield keypathstr, self[keypathstr]

    def __repr__(self):
        return f"NodeLookup(root_obj={self.root_node}, available_paths={self.available_paths})"

    def merged_with(
        self,
        other,
        *_,
        **__,
    ):
        assert self.root_node == other.root_node, 'Root object mismatch'
        new = NodeLookup(self.root_node)
        new.available_paths = self.available_paths.union(other.available_paths)
        return new

    def __deepcopy__(self, memo):
        new = NodeLookup(self.root_node)
        new.available_paths = self.available_paths.copy()
        return new


##────────────────────────────────────────────────────────────────────────────}}}


## {{{                           --     eval utils    --


@runtime_checkable
class LazyProtocol(Protocol):
    def resolve(self) -> Any: ...

    name: str
    current_path: KeyPath
    root_obj: Any
    context: DictLike


def resolve_field_references(expr: str):
    keypath_matches = find_field_references(expr)
    if not keypath_matches:
        return expr
    offset = 0
    for match in keypath_matches:
        if match.symbol == '@':
            newexpr = (
                f"(__DRACON__PARENT_PATH + __dracon_KeyPath('{match.expr}'))"
                f".get_obj(__DRACON__CURRENT_ROOT_OBJ)"
            )
        elif match.symbol == '&':
            raise ValueError(f"Ampersand references in {expr} should have been handled earlier")
        else:
            raise ValueError(f"Invalid symbol {match.symbol} in {expr}")

        expr = expr[: match.start + offset] + newexpr + expr[match.end + offset :]
        original_len = match.end - match.start
        offset += len(newexpr) - original_len
    return expr


def preprocess_expr(expr: str, symbols: Optional[dict] = None):
    expr = resolve_field_references(expr)
    expr = resolve_interpolable_variables(expr, symbols or {})
    return expr


USE_SAFE_EVAL = False


def do_safe_eval(expr: str, symbols: Optional[dict] = None) -> Any:
    expr = preprocess_expr(expr, symbols)

    if USE_SAFE_EVAL:
        # Original safe implementation using asteval
        safe_eval = Interpreter(user_symbols=symbols or {}, max_string_length=1000)
        res = safe_eval.eval(expr, raise_errors=False)
        errors = safe_eval.error
        if errors:
            errors = [': '.join(e.get_error()) for e in errors]
            errormsg = '\n'.join(errors)
            raise InterpolationError(f"Error evaluating expression {expr}:\n{errormsg}")
        return res
    else:
        import traceback

        try:
            eval_globals = {}
            eval_globals.update(__builtins__)  # type: ignore
            eval_globals.update(symbols or {})
            return eval(expr, eval_globals)
        except Exception as e:
            import traceback

            # Get the actual error traceback, skipping the wrapping
            # error_tb = '\n'.join(traceback.format_exception(type(e), e, e.__traceback__))
            raise InterpolationError(f"Error evaluating expression {expr}") from e


@ftrace(watch=[])
def dracon_resolve(obj, **ctx):
    from dracon.resolvable import Resolvable
    from dracon.merge import add_to_context
    from dracon.composer import walk_node
    from functools import partial

    err = ser_debug(obj, operation='deepcopy')
    if err:
        print(f"Error in deepcopy when resolving {obj}")

    if isinstance(obj, Resolvable):
        newobj = deepcopy(obj).resolve(ctx)
        return newobj

    node = deepcopy(obj)
    walk_node(
        node=node,
        callback=partial(add_to_context, ctx),
    )

    return node


def prepare_symbols(current_path, root_obj, context):
    symbols = copy(BASE_DRACON_SYMBOLS)
    symbols.update(
        {
            "__DRACON__CURRENT_PATH": current_path,
            "__DRACON__PARENT_PATH": current_path.parent,
            "__DRACON__CURRENT_ROOT_OBJ": root_obj,
            "__DRACON_RESOLVE": dracon_resolve,
            "__dracon_KeyPath": KeyPath,
        }
    )
    symbols.update(context or {})
    return symbols


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                     --     evaluate expression   --


@ftrace(watch=[])
def evaluate_expression(
    expr: str,
    current_path: str | KeyPath = '/',
    root_obj: Any = None,
    allow_recurse: int = 5,
    init_outermost_interpolations: Optional[List[InterpolationMatch]] = None,
    context: Optional[Dict[str, Any]] = None,
) -> Any:
    from dracon.merge import merged, MergeKey

    # Initialize interpolations
    if init_outermost_interpolations is None:
        interpolations = outermost_interpolation_exprs(expr)
    else:
        interpolations = init_outermost_interpolations

    # Return the expression if there are no interpolations
    if not interpolations:
        return expr

    # Ensure current_path is a KeyPath instance
    if isinstance(current_path, str):
        current_path = KeyPath(current_path)

    symbols = prepare_symbols(current_path, root_obj, context)

    # Helper function to resolve Lazy instances
    def recurse_lazy_resolve(expr):
        if isinstance(expr, LazyProtocol):
            expr.current_path = current_path
            expr.root_obj = root_obj
            expr.context = merged(expr.context, context, MergeKey(raw='{<+}'))
            expr = expr.resolve()
        return expr

    # Check if the entire expression is a single interpolation
    if (
        len(interpolations) == 1
        and interpolations[0].start == 0
        and interpolations[0].end == len(expr)
    ):
        # Resolve and evaluate the single interpolation
        interpolation_expr = interpolations[0].expr
        resolved_expr = evaluate_expression(
            interpolation_expr,
            current_path,
            root_obj,
            allow_recurse=allow_recurse,
            context=context,
        )
        evaluated_expr = do_safe_eval(str(resolved_expr), symbols)
        endexpr = recurse_lazy_resolve(evaluated_expr)
    else:
        # Process and replace each interpolation within the expression
        offset = 0
        for match in interpolations:
            resolved_expr = evaluate_expression(
                match.expr,
                current_path,
                root_obj,
                allow_recurse=allow_recurse,
                context=context,
            )
            evaluated_expr = do_safe_eval(str(resolved_expr), symbols)
            newexpr = str(recurse_lazy_resolve(evaluated_expr))
            expr = expr[: match.start + offset] + newexpr + expr[match.end + offset :]
            offset += len(newexpr) - (match.end - match.start)
        endexpr = expr

    # Recurse if allowed and necessary
    if allow_recurse != 0 and isinstance(endexpr, str):
        return evaluate_expression(endexpr, current_path, root_obj, allow_recurse=allow_recurse - 1)
    return endexpr


##────────────────────────────────────────────────────────────────────────────}}}


## {{{                     --     InterpolableNode     --
class InterpolableNode(ContextNode):
    def __init__(
        self,
        value,
        start_mark=None,
        end_mark=None,
        tag=None,
        anchor=None,
        comment=None,
        init_outermost_interpolations=None,
        context=None,
    ):
        self.init_outermost_interpolations = init_outermost_interpolations
        ContextNode.__init__(
            self,
            value,
            start_mark=start_mark,
            end_mark=end_mark,
            tag=tag,
            comment=comment,
            anchor=anchor,
            context=context,
        )
        self.referenced_nodes = NodeLookup()

    def __getstate__(self):
        state = super().__getstate__()
        state['init_outermost_interpolations'] = self.init_outermost_interpolations
        state['referenced_nodes'] = self.referenced_nodes
        return state

    def __setstate__(self, state):
        super().__setstate__(state)
        self.init_outermost_interpolations = state['init_outermost_interpolations']
        self.referenced_nodes = state['referenced_nodes']

    def evaluate(self, path='/', root_obj=None, context=None):
        context = context or {}
        context = {**self.context, **context}
        newval = evaluate_expression(
            self.value,
            current_path=path,
            root_obj=root_obj,
            context=context,  # type: ignore
        )
        return newval

    def preprocess_ampersand_references(self, match, comp_res, current_path):
        available_anchors = comp_res.anchor_paths
        context_str = ''

        # references can also have a list of variable definitions attached to them
        # syntax is ${&unique_id:var1=expr1,var2=expr2}
        # these come from the surrounding expression or context and should be passed
        # to the resolve method. It's sort of a asteval-specific limitation becasue there's no
        # locals() or globals() accessible from "inside" the expression...

        if ':' in match.expr:
            match.expr, vardefs = match.expr.split(':')
            if vardefs:
                context_str = ',' + vardefs

        match_parts = match.expr.split('.', 1)
        if match_parts[0] in available_anchors:  # we're matching an anchor
            keypath = available_anchors[match_parts[0]].copy()
            keypath = keypath.down(match_parts[1]) if len(match_parts) > 1 else keypath
        else:  # we're trying to match a keypath
            keypath = current_path.parent.down(KeyPath(match.expr))

        if self.referenced_nodes.root_node is not None:
            assert self.referenced_nodes.root_node == comp_res.root, 'Root object mismatch'
        else:
            self.referenced_nodes.root_node = comp_res.root

        keypathstr = str(keypath.simplified())
        self.referenced_nodes.available_paths.add(keypathstr)
        newexpr = f'__DRACON_RESOLVE(__DRACON_NODES["{keypathstr}"] {context_str})'

        if '__DRACON_NODES' not in self.context:
            self.context['__DRACON_NODES'] = self.referenced_nodes

        return newexpr

    def preprocess_references(self, comp_res, current_path):
        """
        Preprocess field references in the node's value by handling ampersand ('&')
        symbols within interpolation expressions. At ('@') references are handled at a later stage.

        Scans the node's value for field references and, for each ampersand reference that is located
        within an interpolation, replaces it with a "_DRACON_RESOLVE_(...)" call that resolves the referenced node.

        If the current node is used as a mapping key, the parent's mapping
        is recomputed to reflect any changes.

        """

        if self.init_outermost_interpolations is None:
            self.init_outermost_interpolations = outermost_interpolation_exprs(self.value)

        assert self.init_outermost_interpolations is not None
        interps = self.init_outermost_interpolations
        references = find_field_references(self.value)

        offset = 0
        for match in references:
            newexpr = match.expr
            if match.symbol == '&' and any([i.contains(match.start) for i in interps]):
                newexpr = self.preprocess_ampersand_references(match, comp_res, current_path)

                self.value = (
                    self.value[: match.start + offset] + newexpr + self.value[match.end + offset :]
                )
                offset += len(newexpr) - match.end + match.start
            elif match.symbol == '@' and any([i.contains(match.start) for i in interps]):
                ...  # handled in postproc
            else:
                raise ValueError(f'Unknown interpolation symbol: {match.symbol}')

        if references:
            self.init_outermost_interpolations = outermost_interpolation_exprs(self.value)

        if current_path.is_mapping_key():
            parent_node = current_path.parent.get_obj(comp_res.root)
            assert isinstance(parent_node, DraconMappingNode)
            parent_node._recompute_map()

    def flush_references(self):
        if '__DRACON_NODES' in self.context:
            del self.context['__DRACON_NODES']

    # def __deepcopy__(self, memo):
    #     # use ContextNode's deepcopy method
    #     new_node = super().__deepcopy__(memo)
    #     new_node.init_outermost_interpolations = self.init_outermost_interpolations
    #     new_node.referenced_nodes = self.referenced_nodes
    #     return new_node

    def copy(self):
        """Create a copy of the interpolable node with shallow copied context and referenced nodes."""
        new_node = self.__class__(
            value=self.value,
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            tag=self.tag,
            anchor=self.anchor,
            comment=self.comment,
            context=self.context.copy(),
            init_outermost_interpolations=self.init_outermost_interpolations,
        )
        if hasattr(self, 'referenced_nodes') and self.referenced_nodes is not None:
            new_node.referenced_nodes = self.referenced_nodes
        return new_node


##───────────────────────────────────────────────────────────────────────────}}}


@ftrace()
def preprocess_references(comp_res):
    comp_res.find_special_nodes('interpolable', lambda n: isinstance(n, InterpolableNode))
    comp_res.sort_special_nodes('interpolable')

    for path in comp_res.pop_all_special('interpolable'):
        node = path.get_obj(comp_res.root)
        assert isinstance(node, InterpolableNode), f"Invalid node type: {type(node)}  => {node}"
        node.preprocess_references(comp_res, path)

    return comp_res

================
File: dracon/keypath.py
================
from enum import Enum
from functools import lru_cache
from typing import List, Union, Hashable, Any, Optional, TypeVar, Type, Protocol, Tuple, Deque
from typing_extensions import runtime_checkable
from collections import deque
from ruamel.yaml.nodes import Node
from dracon.utils import node_repr, list_like, dict_like
import re


class KeyPathToken(Enum):
    ROOT = 0
    UP = 1
    MAPPING_KEY = 2  # indicates that the path points to the key of a mapping, not the value
    SINGLE_WILDCARD = 3  # represents '*' in glob patterns
    MULTI_WILDCARD = 4  # represents '**' in glob patterns


MAPPING_KEY = KeyPathToken.MAPPING_KEY


@lru_cache(maxsize=512)
def escape_keypath_part(part: str) -> str:
    return part.replace('.', '\\.').replace('/', '\\/')


@lru_cache(maxsize=512)
def unescape_keypath_part(part: str) -> str:
    return part.replace('\\.', '.').replace('\\/', '/')


def parse_part(part: str) -> Union[Hashable, KeyPathToken]:
    if part == '*':
        return KeyPathToken.SINGLE_WILDCARD
    elif part == '**':
        return KeyPathToken.MULTI_WILDCARD
    return part


@lru_cache(maxsize=1000)
def simplify_parts_recursive(
    parts: Tuple[Union[Hashable, KeyPathToken], ...],
) -> Tuple[Union[Hashable, KeyPathToken], ...]:
    if not parts:
        return tuple()

    if len(parts) == 1:
        return parts

    if parts[-1] == KeyPathToken.ROOT:
        return (KeyPathToken.ROOT,)

    if parts[-1] == KeyPathToken.UP:
        # Simplify everything before the UP token
        prefix = simplify_parts_recursive(parts[:-1])

        if not prefix:
            # If nothing before UP, keep the UP
            return (KeyPathToken.UP,)

        if prefix[-1] == KeyPathToken.ROOT:
            # Can't go up from root
            return prefix

        if prefix[-1] == KeyPathToken.UP:
            # Multiple UPs stack
            return prefix + (KeyPathToken.UP,)

        # Remove the last element unless it's ROOT
        if len(prefix) > 1 and prefix[-2] == KeyPathToken.MAPPING_KEY:
            # If we're removing a mapping key, remove both tokens
            return prefix[:-2]
        return prefix[:-1]

    # Simplify everything before current token and append current
    return simplify_parts_recursive(parts[:-1]) + (parts[-1],)


# def simplify_parts(parts: Union[List, Tuple]) -> Tuple[Union[Hashable, KeyPathToken], ...]:
# return simplify_parts_recursive(tuple(parts))


@lru_cache(maxsize=10000)
def simplify_parts_cached(parts: Tuple) -> Tuple[Union[Hashable, KeyPathToken], ...]:
    """
    Non-recursive version of simplify_parts that processes parts from left to right using a stack.
    This version maintains the same logic but eliminates recursion for better performance with deep paths.
    """
    if not parts:
        return tuple()

    # Convert input to tuple if it's a list
    parts = tuple(parts)

    # Special cases for simple inputs
    if len(parts) == 1:
        return parts

    # Initialize stack for processing
    stack: Deque[Union[Hashable, KeyPathToken]] = deque()

    for part in parts:
        if part == KeyPathToken.ROOT:
            # ROOT token clears the stack and becomes the only element
            stack.clear()
            stack.append(KeyPathToken.ROOT)

        elif part == KeyPathToken.UP:
            if not stack:
                # If stack is empty, just add UP token
                stack.append(KeyPathToken.UP)
            elif stack[-1] == KeyPathToken.ROOT:
                # Can't go up from root, keep the root
                continue
            elif stack[-1] == KeyPathToken.UP:
                # Multiple UPs stack
                stack.append(KeyPathToken.UP)
            else:
                # Remove the last element unless it's ROOT
                if len(stack) >= 2 and stack[-2] == KeyPathToken.MAPPING_KEY:
                    # If we're removing a mapping key, remove both tokens
                    stack.pop()
                    stack.pop()
                else:
                    stack.pop()

        else:
            # For any other token, just append it to the stack
            stack.append(part)

    return tuple(stack)


def simplify_parts(parts):
    if not parts:
        return tuple()
    return simplify_parts_cached(tuple(parts))


@lru_cache(maxsize=512)
def parse_string(path: str) -> List[Union[Hashable, KeyPathToken]]:
    if not path:
        return []

    parts = []
    dot_count = 0
    current_part = ""

    escaped = False

    for char in path:
        if char == '\\' and not escaped:
            escaped = True
            continue
        elif char == '/' and not escaped:
            if current_part:
                parts.append(parse_part(current_part))
                current_part = ""
            parts.append(KeyPathToken.ROOT)
            dot_count = 0
        elif char == '.' and not escaped:
            if current_part:
                parts.append(parse_part(current_part))
                current_part = ""
            dot_count += 1
            if dot_count > 1:
                parts.append(KeyPathToken.UP)
        else:
            current_part += char
            dot_count = 0
        escaped = False
    if current_part:
        parts.append(parse_part(current_part))
    return parts


class KeyPath:
    def __init__(
        self, path: Union[str, List[Union[Hashable, KeyPathToken]]], simplify: bool = True
    ):
        self.is_simple = False
        if isinstance(path, (list, tuple)) and not isinstance(path, str):
            self.parts = list(path)  # Create a copy to avoid modifying the input
        else:
            self.parts = self._parse_string(str(path))
        if simplify:
            self.simplify()

    def _parse_string(self, path: str) -> List[Union[Hashable, KeyPathToken]]:
        return parse_string(path)

    def clear(self) -> 'KeyPath':
        self.parts = []
        return self

    def rootless(self) -> 'KeyPath':
        simple = self.simplified()
        if simple.parts[0] == KeyPathToken.ROOT:
            simple.parts = simple.parts[1:]
        return simple

    def up(self, simplify=True) -> 'KeyPath':
        self.is_simple = False
        self.parts.append(KeyPathToken.UP)
        if simplify:
            return self.simplify()
        return self

    # unicode emoji for key:
    @property
    def parent(self) -> 'KeyPath':
        return self.copy().up()

    def pop(self) -> Union[Hashable, KeyPathToken]:
        return self.parts.pop()

    def front_pop(self) -> Union[Hashable, KeyPathToken]:
        return self.parts.pop(0)

    def with_added_parts(self, *parts) -> 'KeyPath':
        kcopy = self.copy()
        kcopy.parts.extend(parts)
        return kcopy

    def down(self, path: "str | KeyPath | KeyPathToken") -> 'KeyPath':
        self.is_simple = False
        if isinstance(path, int):
            path = str(path)
        if isinstance(path, KeyPathToken):
            self.parts.append(path)
        elif isinstance(path, KeyPath):
            self.parts.extend(path.parts)
        elif isinstance(path, list):
            return self.down(KeyPath(path))
        else:
            # escape if it's a string
            return self.down(KeyPath(escape_keypath_part(path)))
        return self

    def match(self, target: 'KeyPath') -> bool:
        """
        Match this KeyPath (as a pattern) against a target KeyPath.
        Supports '*' for single-level wildcard, '**' for multi-level wildcard,
        and partial matching within individual path segments.
        """

        def match_parts(pattern_parts, target_parts):
            pi = ti = 0
            while pi < len(pattern_parts) and ti < len(target_parts):
                if pattern_parts[pi] == KeyPathToken.MULTI_WILDCARD:
                    # Try to match the rest of the pattern against the rest of the target
                    return any(
                        match_parts(pattern_parts[pi + 1 :], target_parts[i:])
                        for i in range(ti, len(target_parts) + 1)
                    )
                elif pattern_parts[pi] == KeyPathToken.SINGLE_WILDCARD:
                    # Match any single part
                    pi += 1
                    ti += 1
                elif isinstance(pattern_parts[pi], str) and isinstance(target_parts[ti], str):
                    # Convert glob pattern to regex pattern
                    regex_pattern = '^' + re.escape(pattern_parts[pi]).replace('\\*', '.*') + '$'
                    if re.match(regex_pattern, target_parts[ti]):
                        pi += 1
                        ti += 1
                    else:
                        return False
                elif pattern_parts[pi] == target_parts[ti]:
                    # Exact match for non-string parts (e.g., KeyPathToken.ROOT)
                    pi += 1
                    ti += 1
                else:
                    return False
            # Check if we've matched all parts
            return pi == len(pattern_parts) and ti == len(target_parts)

        return match_parts(self.simplified().parts, target.simplified().parts)

    # same as down
    def append(self, part: Union[Hashable, KeyPathToken]) -> 'KeyPath':
        return self.down(part)

    @property
    def stem(self):
        if self.is_mapping_key():
            return self.parts[:-2]
        if len(self.parts) <= 1:
            return '/'
        return self.parts[-1]

    # same as down but not in place
    def __add__(self, other) -> 'KeyPath':
        return self.copy().down(other)

    def copy(self) -> 'KeyPath':
        kc = KeyPath([], simplify=False)
        kc.parts = self.parts.copy()
        kc.is_simple = self.is_simple
        return kc

    def __deepcopy__(self, memo) -> 'KeyPath':
        return self.copy()

    def simplify(self) -> 'KeyPath':
        if self.is_simple:
            return self
        self.parts = list(simplify_parts(self.parts))
        self.is_simple = True
        return self

    def simplified(self) -> 'KeyPath':
        if self.is_simple:
            return self.copy()
        new = KeyPath(self.parts, simplify=True)
        return new

    def __str__(self) -> str:
        result = ''
        prev = None
        for part in self.parts:
            if part == KeyPathToken.ROOT:
                result += '/'
            elif part == KeyPathToken.UP:
                result += '.' if prev == KeyPathToken.UP else '..'
            elif part == MAPPING_KEY:
                result += '🔑:' if prev in {KeyPathToken.ROOT, KeyPathToken.UP, None} else '.🔑:'
            elif part == KeyPathToken.SINGLE_WILDCARD:
                result += '*' if prev in {KeyPathToken.ROOT, KeyPathToken.UP, None} else '.*'
            elif part == KeyPathToken.MULTI_WILDCARD:
                result += '**' if prev in {KeyPathToken.ROOT, KeyPathToken.UP, None} else '.**'
            else:
                if prev not in {KeyPathToken.ROOT, KeyPathToken.UP, None, MAPPING_KEY}:
                    result += '.'
                result += escape_keypath_part(str(part))
            prev = part
        return result

    def __repr__(self) -> str:
        return f"KeyPath('{self}')"

    def __len__(self) -> int:
        if self.is_mapping_key():
            return len(self.parts) - 1
        return len(self.parts)

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, KeyPath):
            return NotImplemented
        return self.parts == other.parts

    def __hash__(self) -> int:
        return hash(tuple(self.parts))

    def __getitem__(self, index) -> Union[Hashable, KeyPathToken]:
        return self.parts[index]

    def __iter__(self):
        return iter(self.parts)

    def startswith(self, other: 'KeyPath') -> bool:
        if len(other) > len(self):
            return False
        return self.parts[: len(other)] == other.parts

    def check_correctness(self) -> None:
        if self.parts and self.parts[-1] == KeyPathToken.MAPPING_KEY:
            raise ValueError(f'KeyPath cannot end with a mapping key: {self}')

    # protocol that tests if an object has a keypath_passthrough prperty:
    @runtime_checkable
    class Passthrough(Protocol):
        @property
        def keypath_passthrough(self):
            raise NotImplementedError

    def get_obj(
        self, obj: Any, create_path_if_not_exists=False, default_mapping_constructor=None
    ) -> Any:
        if not self.is_simple:
            simplified = self.simplified()
            return simplified.get_obj(obj, create_path_if_not_exists, default_mapping_constructor)

        self.check_correctness()

        # make sure there's no wildcards in the path
        if any(
            part in {KeyPathToken.SINGLE_WILDCARD, KeyPathToken.MULTI_WILDCARD}
            for part in self.parts
        ):
            raise ValueError(f'Cannot get object from path with wildcards: {self}')

        res = obj
        try:
            for i, part in enumerate(self.parts):
                if part == KeyPathToken.UP:
                    raise ValueError(f'Cannot get object from unsimplifiable path: {self}')
                if part == KeyPathToken.ROOT:
                    continue
                if part == KeyPathToken.MAPPING_KEY:
                    if i != len(self.parts) - 2:
                        raise ValueError(f'Invalid mapping key in path: {self}')
                    assert hasattr(res, 'get_key')
                    res = res.get_key(self.parts[-1])
                    return res
                res = _get_obj_impl(
                    res, part, create_path_if_not_exists, default_mapping_constructor
                )
        except AttributeError as e:
            raise AttributeError(f'Could not get object from path: {self}') from e
        return res

    def is_mapping_key(self, simplify=False) -> bool:
        if simplify and not self.is_simple:
            simplified = self.simplified()
            return simplified.is_mapping_key(simplify=False)
        if len(self.parts) < 2:
            return False
        return self.parts[-2] == KeyPathToken.MAPPING_KEY

    def removed_mapping_key(self) -> 'KeyPath':
        if not self.is_mapping_key():
            return self
        kcopy = self.copy()
        kcopy.parts.pop(-2)
        return kcopy


def _get_obj_impl(
    obj: Any, attr: Any, create_path_if_not_exists=False, default_mapping_constructor=None
) -> Any:
    """
    Get an attribute from an object, handling various types of objects.
    """
    from dracon.deferred import DeferredNode

    if isinstance(obj, DeferredNode):
        return _get_obj_impl(
            obj.keypath_passthrough,
            attr,
            create_path_if_not_exists,
            default_mapping_constructor,
        )
    if list_like(obj):
        return obj[int(attr)]
    try:
        return obj[attr]
    except (TypeError, KeyError):
        if hasattr(obj, attr):
            return getattr(obj, attr)
        else:
            try:  # check if we can access it with __getitem__
                return obj[attr]
            except (TypeError, KeyError) as e:
                if create_path_if_not_exists:
                    assert default_mapping_constructor is not None
                    obj[attr] = default_mapping_constructor()
                    return obj[attr]
                if isinstance(obj, Node):
                    import traceback

                    tback = traceback.format_exc(limit=10)

                    raise AttributeError(
                        f'Could not find attribute {attr} in node \n{node_repr(obj)} of type {type(obj)}. \nTraceback:\n{tback}'
                    ) from None
                else:
                    raise AttributeError(f'Could not find attribute {attr} in {obj}') from None


ROOTPATH = KeyPath('/')

================
File: dracon/lazy.py
================
from typing import (
    Any,
    Dict,
    Callable,
    Optional,
    List,
    TypeVar,
    Generic,
    Annotated,
    Protocol,
    runtime_checkable,
)
from dracon.keypath import KeyPath, ROOTPATH, MAPPING_KEY
from pydantic import BaseModel, field_validator, ConfigDict, WrapValidator, Field
from dracon.interpolation_utils import (
    InterpolationMatch,
)
from dracon.interpolation import evaluate_expression, InterpolationError, DraconError
from dracon.utils import list_like, dict_like

import inspect


## {{{                     --     LazyInterpolable     --

T = TypeVar('T')


class Lazy(Generic[T]):
    def __init__(
        self, value: Any = None, validator: Optional[Callable[[Any], Any]] = None, name=None
    ):
        self.value = value
        self.validator = validator
        self.name = name

    def validate(self, value):
        if self.validator is not None:
            try:
                return self.validator(value)
            except Exception as e:
                quoted_name = f' "{self.name}"' if self.name else ''
                raise InterpolationError(
                    f"Failed to lazyly validate attribute {quoted_name}: {e}"
                ) from None
        return value

    def resolve(self) -> T:
        return self.validate(self.value)

    def get(self, owner_instance, setval=False):
        newval = self.resolve()
        if setval:
            setattr(owner_instance, self.name, newval)
        return newval

    def __set_name__(self, owner, name):
        self.name = name


T = TypeVar('T')


class LazyInterpolable(Lazy[T]):
    """A lazy object that can be resolved (i.e. interpolated) to a value when needed."""

    def __init__(
        self,
        value: Any,
        validator: Optional[Callable[[Any], Any]] = None,
        name=None,
        current_path: KeyPath = ROOTPATH,
        root_obj: Any = None,
        init_outermost_interpolations: Optional[List[InterpolationMatch]] = None,
        permissive: bool = False,
        context: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(value, validator, name)

        self.context = context
        self.current_path = current_path
        self.root_obj = root_obj
        self.init_outermost_interpolations = init_outermost_interpolations
        self.permissive = permissive
        if not self.permissive:
            assert isinstance(
                value, (str, tuple)
            ), f"LazyInterpolable expected string, got {type(value)}. Did you mean to contruct with permissive=True?"

    def __getstate__(self):
        """Get the object's state for pickling."""
        state = {
            'value': self.value,
            'name': self.name,
            'current_path': self.current_path,
            'permissive': self.permissive,
            'context': self.context,
            # Store init_outermost_interpolations if it's picklable
            'init_outermost_interpolations': self.init_outermost_interpolations
            if self.init_outermost_interpolations
            else None,
        }

        # Handle root_obj specially if needed
        if hasattr(self.root_obj, '__getstate__'):
            state['root_obj'] = self.root_obj
        else:
            state['root_obj'] = None  # Will be reattached after unpickling

        # Don't pickle the validator function - it will be reattached by the owner
        return state

    def __setstate__(self, state):
        """Restore the object's state after unpickling."""
        # Initialize with default values
        self.__init__(
            value=state['value'],
            name=state['name'],
            current_path=state['current_path'],
            root_obj=state['root_obj'],
            init_outermost_interpolations=state['init_outermost_interpolations'],
            permissive=state['permissive'],
            context=state['context'],
            validator=None,  # Validator will be reattached by the owner if needed
        )

    def __repr__(self):
        return f"LazyInterpolable({self.value})"

    def resolve(self, context_override=None) -> T:
        if isinstance(self.value, str):
            try:
                ctx = self.context if self.context is not None else {}
                if context_override is not None:
                    ctx.update(context_override)
                self.value = evaluate_expression(
                    self.value,
                    self.current_path,
                    self.root_obj,
                    init_outermost_interpolations=self.init_outermost_interpolations,
                    context=ctx,
                )
            except Exception as e:
                raise type(e)(f"Error resolving lazy value \"{self.value}\": {str(e)}") from None

        return self.validate(self.value)

    def get(self, owner_instance, setval=False):
        """Get the value of the lazy object, and optionally set it as an attribute of the owner instance."""
        if hasattr(owner_instance, '_dracon_root_obj'):
            self.root_obj = owner_instance._dracon_root_obj
            assert hasattr(
                owner_instance, '_dracon_current_path'
            ), f"Instance {owner_instance} has no current path"
            self.current_path = owner_instance._dracon_current_path + self.name

        newval = self.resolve()

        if setval:
            setattr(owner_instance, self.name, newval)

        return newval

    def reattach_validator(self, validator: Optional[Callable[[Any], Any]]):
        """Reattach a validator after unpickling."""
        self._validator = validator


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                     --     resolve all lazy     --


def num_array_like(obj):
    return hasattr(obj, 'dtype') and hasattr(obj, 'shape') and hasattr(obj, 'ndim')


def resolve_single_key(path, key, root, context_override=None):
    """resolve a single lazy key and return info about the transformation"""
    parent_path = path.parent

    try:
        parent = parent_path.get_obj(root)
        key.root_obj = root
        key.current_path = path.removed_mapping_key()

        # find the actual key in parent
        lazy_key = None
        for k in parent.keys():
            if isinstance(k, LazyInterpolable) and (id(k) == id(key) or str(k) == str(key)):
                lazy_key = k
                break

        if lazy_key is None:
            raise DraconError(f"couldn't find LazyInterpolable key in parent at {path}")

        # resolve the key
        resolved_key = key.resolve(context_override=context_override)
        if not isinstance(resolved_key, (str, int, float, bool)):
            resolved_key = str(resolved_key)

        # update mapping
        value = parent[lazy_key]
        del parent[lazy_key]
        parent[resolved_key] = value

        if hasattr(parent, '_recompute_map'):
            parent._recompute_map()

        # return the transformation info
        old_path = parent_path.down(str(lazy_key))
        new_path = parent_path.down(str(resolved_key))
        return True, (old_path, new_path)

    except Exception as e:
        return False, None


def resolve_single_value(path, value, root, context_override=None):
    """resolve a single lazy value"""
    try:
        parent = path.parent.get_obj(root)
        value.root_obj = root
        value.current_path = path

        resolved_value = value.resolve(context_override=context_override)

        stem = path.stem
        if stem == '/' or stem == ROOTPATH:
            raise ValueError("cannot resolve root path")

        set_val(parent, stem, resolved_value)
        return True

    except Exception as e:
        return False


def set_val(parent: Any, key, value: Any) -> None:
    if list_like(parent):
        parent[int(key)] = value
    elif isinstance(parent, BaseModel):
        # handle pydantic models specifically
        setattr(parent, key, value)
    elif hasattr(parent, key):
        setattr(parent, key, value)
    else:
        try:
            parent[key] = value
        except TypeError:
            try:
                # last fallback: try setting attribute anyway
                setattr(parent, key, value)
            except AttributeError:
                raise AttributeError(f'Could not set attribute {key} in {parent}') from None


def collect_lazy_by_depth(obj, path=ROOTPATH, seen=None):
    """collect all lazy objects grouped by depth"""
    if seen is None:
        seen = set()

    lazy_keys_by_depth = {}  # depth -> list of (path, lazy_key)
    lazy_values = []  # list of (path, lazy_value)

    def _collect(o, p, seen_set):
        # skip None values and class types
        if o is None or isinstance(o, type):
            return

        obj_id = id(o)
        if obj_id in seen_set:
            return
        seen_set.add(obj_id)

        depth = len(p)

        # handle LazyInterpolable directly
        if isinstance(o, LazyInterpolable):
            if p.is_mapping_key():
                if depth not in lazy_keys_by_depth:
                    lazy_keys_by_depth[depth] = []
                lazy_keys_by_depth[depth].append((p, o))
            else:
                lazy_values.append((p, o))
            return

        # handle dict-like objects
        if dict_like(o):
            for k, v in list(o.items()):
                if isinstance(k, LazyInterpolable):
                    key_path = p.copy().down(MAPPING_KEY).down(str(k))
                    if depth not in lazy_keys_by_depth:
                        lazy_keys_by_depth[depth] = []
                    lazy_keys_by_depth[depth].append((key_path, k))

                value_path = p.copy().down(str(k))
                _collect(v, value_path, seen_set)

        # handle list-like objects (excluding strings, bytes, and numeric arrays)
        elif list_like(o) and not isinstance(o, (str, bytes, type)) and not num_array_like(o):
            try:
                # Double-check that we can actually enumerate this object
                for i, item in enumerate(o):
                    item_path = p.copy().down(str(i))
                    _collect(item, item_path, seen_set)
            except TypeError:
                # If enumeration fails, just skip this object
                pass

        # handle Pydantic models
        elif isinstance(o, BaseModel):
            # access model's fields directly through __dict__ to get raw values
            for field_name, field_value in o.__dict__.items():
                # skip private attributes and special pydantic fields
                if field_name.startswith('_'):
                    continue

                item_path = p.copy().down(field_name)
                _collect(field_value, item_path, seen_set)

        # handle other objects with attributes
        elif hasattr(o, '__dict__') and not isinstance(o, (str, int, float, bool, bytes, type)):
            # skip traversing built-in types and callables
            if (
                o.__class__.__module__ in ('builtins', '__builtin__')
                or callable(o)
                or inspect.isclass(o)
                or inspect.ismodule(o)
            ):
                return

            try:
                # get object attributes
                for attr_name, attr_value in vars(o).items():
                    # skip private attributes, methods, and special attributes
                    if (
                        attr_name.startswith('_')
                        or callable(attr_value)
                        or attr_name in {'__dict__', '__weakref__'}
                    ):
                        continue

                    attr_path = p.copy().down(attr_name)
                    _collect(attr_value, attr_path, seen_set)
            except (TypeError, ValueError, AttributeError):
                # If we can't get attributes for some reason, skip this object
                pass

    _collect(obj, path, seen)
    return lazy_keys_by_depth, lazy_values


def resolve_all_lazy(
    obj,
    root_obj=None,
    current_path=None,
    visited=None,
    context_override=None,
    max_passes=20,
    min_passes=2,
):
    """resolves all lazy objects in a multi-pass approach by depth level"""
    if visited is None:
        visited = set()

    if root_obj is None:
        root_obj = obj if not hasattr(obj, '_dracon_root_obj') else obj._dracon_root_obj

    if current_path is None:
        current_path = (
            ROOTPATH if not hasattr(obj, '_dracon_current_path') else obj._dracon_current_path
        )

    # process key resolution in multiple passes by depth
    unresolved_count = 0
    pass_num = 0

    while pass_num < max_passes:
        lazy_keys_by_depth, lazy_values = collect_lazy_by_depth(obj, current_path)

        if not lazy_keys_by_depth and not lazy_values:
            break

        # resolve keys by depth (shallowest first)
        depths = sorted(lazy_keys_by_depth.keys())
        if not depths:  # no more keys to resolve, move on to values
            keys_resolved = 0
        else:  # resolve keys at the shallowest depth only
            current_depth = depths[0]
            keys_resolved = 0
            for path, key in lazy_keys_by_depth[current_depth]:
                success, _ = resolve_single_key(path, key, root_obj, context_override)
                if success:
                    keys_resolved += 1

        # if no keys were resolved, move on to values
        if not keys_resolved:
            values_resolved = 0
            for path, value in lazy_values:
                success = resolve_single_value(path, value, root_obj, context_override)
                if success:
                    values_resolved += 1

            unresolved_count += values_resolved
            if values_resolved == 0:  # nothing more to resolve
                break
        else:
            unresolved_count += keys_resolved

        pass_num += 1

    if unresolved_count > 0 or min_passes > 0:
        return resolve_all_lazy(
            obj, root_obj, current_path, visited, context_override, max_passes, min_passes - 1
        )

    return obj


##────────────────────────────────────────────────────────────────────────────}}}

## {{{              --     recursive lazy container update     --


def recursive_update_lazy_container(obj, root_obj, current_path, seen=None):
    """
    Recursively update the root object and current path of all nested lazy objects.
    """

    if seen is None:
        seen = set()

    obj_id = id(obj)
    if obj_id in seen:
        return  # skip already processed objects to break cycles
    seen.add(obj_id)

    if is_lazy_compatible(obj):
        obj._dracon_root_obj = root_obj
        obj._dracon_current_path = current_path

    if dict_like(obj):
        for key, value in obj.items():
            new_path = current_path + str(key)
            recursive_update_lazy_container(value, root_obj, new_path, seen)

    elif list_like(obj) and not isinstance(obj, (str, bytes)) and not num_array_like(obj):
        for i, item in enumerate(obj):
            new_path = current_path + str(i)
            recursive_update_lazy_container(item, root_obj, new_path, seen)


##────────────────────────────────────────────────────────────────────────────}}}


@runtime_checkable
class LazyCapable(Protocol):
    """
    A protocol for objects that can hold lazy values and resolve them
    even if they have relative and absolute keypath references.

    For example, a field like "${.name}" should be resolved to the value of the
    "name" field of the current object, while "${/sub.name}" should be resolved
    to the value of root_obj["sub"]["name"].

    For that to work, the object must have the following attributes:

    """

    _dracon_root_obj: Any  # The root object from which to resolve absolute keypaths
    _dracon_current_path: str  # The current path of the object in the root object


def is_lazy_compatible(v: Any) -> bool:
    return isinstance(v, LazyCapable)


def wrap_lazy_validator(v: Any, handler, info) -> Any:
    return Lazy(v, validator=handler, name=info.field_name)


LazyVal = Annotated[
    T | Lazy[T],
    WrapValidator(wrap_lazy_validator),
    Field(validate_default=True),
]


class LazyDraconModel(BaseModel):
    _dracon_root_obj: Optional[Any] = None
    _dracon_current_path: KeyPath = ROOTPATH

    def _update_lazy_container_attributes(self, root_obj, current_path, recurse=True):
        """
        Update the lazy attributes of the model with the root object and current path.
        """
        self._dracon_root_obj = root_obj
        self._dracon_current_path = current_path
        if recurse:
            for key, value in self.__dict__.items():
                if is_lazy_compatible(value):
                    new_path = current_path + KeyPath(str(key))
                    value._update_lazy_container_attributes(root_obj, new_path, recurse=True)

    model_config = ConfigDict(arbitrary_types_allowed=True, validate_default=True)

    @field_validator("*", mode="wrap")
    @classmethod
    def ignore_lazy(cls, v, handler, info):
        if isinstance(v, Lazy):
            if v.validator is None:
                v.validator = handler
            return v
        return handler(v, info)

    def __getattribute__(self, name):
        attr = super().__getattribute__(name)
        if isinstance(attr, Lazy):
            attr.__set_name__(self, name)
            return attr.__get__(self)
        # if it's a list or tuple of Lazy, resolve them
        if isinstance(attr, (list, tuple)):
            for i, item in enumerate(attr):
                if isinstance(item, Lazy):
                    item.name = f'{name}.{i}'
                    attr[i] = item.resolve()
            setattr(self, name, attr)
        return attr

================
File: dracon/loader.py
================
## {{{                          --     imports     --
from ruamel.yaml import Node
import os
from typing import Any, Callable, Dict, Optional, Type, Annotated, TypeVar
from functools import partial

from cachetools import cached, LRUCache
from cachetools.keys import hashkey
from pathlib import Path
from pydantic import BeforeValidator, Field, PlainSerializer

from dracon.include import DEFAULT_LOADERS, compose_from_include_str

from dracon.composer import (
    IncludeNode,
    CompositionResult,
    DraconComposer,
    delete_unset_nodes,
)

from dracon.draconstructor import Draconstructor
from dracon.keypath import KeyPath, ROOTPATH
from dracon.yaml import PicklableYAML

from dracon.utils import (
    DictLike,
    MetadataDictLike,
    ListLike,
    ShallowDict,
    ftrace,
    deepcopy,
    make_hashable,
    ser_debug,
    node_repr,
)

from dracon.interpolation import InterpolableNode, preprocess_references
from dracon.merge import process_merges, add_to_context, merged, MergeKey
from dracon.instructions import process_instructions
from dracon.deferred import DeferredNode, process_deferred
from dracon.representer import DraconRepresenter

from dracon.lazy import DraconError

from dracon import dracontainer


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                       --     DraconLoader     --


DEFAULT_CONTEXT = {
    # some SAFE os functions (not all of them are safe)
    # need no side effects, and no access to the filesystem
    'getenv': os.getenv,
    'getcwd': os.getcwd,
}


def dillcopy(obj):
    import dill

    return dill.loads(dill.dumps(obj))


@ftrace()
def construct(node_or_val, **kwargs):
    if isinstance(node_or_val, Node):
        loader = DraconLoader(**kwargs)
        compres = CompositionResult(root=node_or_val)
        return loader.load_composition_result(compres, post_process=True)

    return node_or_val


class DraconLoader:
    def __init__(
        self,
        custom_loaders: Optional[Dict[str, Callable]] = None,
        capture_globals: bool = True,
        base_dict_type: Type[DictLike] = dracontainer.Mapping,
        base_list_type: Type[ListLike] = dracontainer.Sequence,
        enable_interpolation: bool = False,
        context: Optional[Dict[str, Any]] = None,
        deferred_paths: Optional[list[KeyPath | str]] = None,
    ):
        self.custom_loaders = DEFAULT_LOADERS.copy()
        self.custom_loaders.update(custom_loaders or {})
        self._capture_globals = capture_globals
        self._context_arg = context
        self._enable_interpolation = enable_interpolation
        self.referenced_nodes = {}
        self.deferred_paths = [KeyPath(p) for p in (deferred_paths or [])]
        self.base_dict_type = base_dict_type
        self.base_list_type = base_list_type

        self._init_yaml()

        self.context = (
            ShallowDict[str, Any](self._context_arg)
            if self._context_arg
            else ShallowDict[str, Any]()
        )
        self.yaml.constructor.context = self.context.copy()
        self.reset_context()

    def _init_yaml(self):
        self.yaml = PicklableYAML()
        self.yaml.Composer = DraconComposer
        self.yaml.Constructor = Draconstructor
        self.yaml.Representer = DraconRepresenter

        self.yaml.composer.interpolation_enabled = self._enable_interpolation
        self.yaml.constructor.yaml_base_dict_type = self.base_dict_type

    def reset_context(self):
        self.update_context(DEFAULT_CONTEXT)
        self.update_context(
            {
                'construct': partial(
                    construct,
                    custom_loaders=self.custom_loaders,
                    capture_globals=self._capture_globals,
                    enable_interpolation=self._enable_interpolation,
                    context=self.context,
                )
            }
        )

    def __hash__(self):
        return hash(
            (
                make_hashable(self.context),
                tuple(self.deferred_paths),
                self._enable_interpolation,
            )
        )

    @ftrace()
    def update_context(self, kwargs):
        add_to_context(kwargs, self)

    def copy(self):
        new_loader = DraconLoader(
            custom_loaders=self.custom_loaders.copy(),
            capture_globals=self._capture_globals,
            base_dict_type=self.base_dict_type,
            base_list_type=self.base_list_type,
            enable_interpolation=self._enable_interpolation,
            context=self.context.copy() if self.context else None,
        )
        new_loader.referenced_nodes = self.referenced_nodes.copy()
        new_loader.yaml.constructor.yaml_constructors = (
            self.yaml.constructor.yaml_constructors.copy()
        )
        return new_loader

    def __deepcopy__(self, memo):
        return self.copy()

    def __getstate__(self):
        state = self.__dict__.copy()
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)

    @ftrace()
    def compose_config_from_str(self, content: str) -> CompositionResult:
        composed_content = cached_compose_config_from_str(self.yaml, content)
        return self.post_process_composed(composed_content)

    @ftrace()
    def load_node(self, node):
        try:
            self.yaml.constructor.referenced_nodes = self.referenced_nodes
            if self.yaml.constructor.context is None:
                self.yaml.constructor.context = self.context.copy() or {}
            return self.yaml.constructor.construct_document(node)
        except Exception as e:
            raise DraconError(f"Error loading node {node}") from e

    @ftrace()
    def load_composition_result(self, compres: CompositionResult, post_process=True):
        if post_process:
            compres = self.post_process_composed(compres)
        return self.load_node(compres.root)

    @ftrace()
    def load(self, config_path: str | Path):
        self.reset_context()
        if isinstance(config_path, Path):
            config_path = config_path.resolve().as_posix()
        if ":" not in config_path:
            config_path = f"file:{config_path}"
        comp = compose_from_include_str(self, config_path, custom_loaders=self.custom_loaders)
        return self.load_composition_result(comp)

    @ftrace()
    def loads(self, content: str):
        comp = self.compose_config_from_str(content)
        return self.load_composition_result(comp)

    @ftrace()
    def post_process_composed(self, comp: CompositionResult):
        ser_debug(self, operation='deepcopy')
        ser_debug(comp, operation='deepcopy')
        comp = preprocess_references(comp)
        comp = process_deferred(comp, force_deferred_at=self.deferred_paths)  # type: ignore
        comp.walk_no_path(
            callback=partial(add_to_context, self.context, merge_key=MergeKey(raw='{>~}[>~]'))
        )
        comp = self.update_deferred_nodes(comp)
        comp = process_instructions(comp, self)
        comp = self.process_includes(comp)
        comp, merge_changed = process_merges(comp)
        comp, delete_changed = delete_unset_nodes(comp)
        if merge_changed or delete_changed:
            comp.make_map()
        comp = self.save_references(comp)
        comp.update_paths()

        # one more round of processing deferred nodes to catch them at new paths
        comp = process_deferred(comp, force_deferred_at=self.deferred_paths)  # type: ignore
        comp = self.update_deferred_nodes(comp)
        comp.update_paths()

        return comp

    @ftrace()
    def update_deferred_nodes(self, comp_res: CompositionResult):
        # copies the loader into deferred nodes so they can resume their composition by themselves

        deferred_nodes = []

        def find_deferred_nodes(node: Node, path: KeyPath):
            if isinstance(node, DeferredNode):
                deferred_nodes.append((node, path))

        comp_res.walk(find_deferred_nodes)
        deferred_nodes = sorted(deferred_nodes, key=lambda x: len(x[1]), reverse=True)

        for node, _ in deferred_nodes:
            if node._loader is None:
                node._loader = self.copy()
            node._full_composition = comp_res
            if node._clear_ctx:
                for k in node._clear_ctx:
                    node.context.pop(k, None)
                    node._loader.context.pop(k, None)

        return comp_res

    @ftrace(watch=[])
    def save_references(self, comp_res: CompositionResult):
        # the preprocessed refernces are stored as paths that point to refered nodes
        # however, after all the merging and including is done, we need to save
        # the nodes themselves so that they can't be affected by further changes (e.g. construction)

        # TODO: should belong to CompositionResult, not the loader

        comp_res.find_special_nodes('interpolable', lambda n: isinstance(n, InterpolableNode))

        referenced_nodes = {}

        for path in comp_res.pop_all_special('interpolable'):
            node = path.get_obj(comp_res.root)
            assert isinstance(node, InterpolableNode), f"Invalid node type: {type(node)}"
            node.flush_references()
            for i, n in node.referenced_nodes.items():
                if i not in referenced_nodes:
                    referenced_nodes[i] = deepcopy(n)

        self.referenced_nodes = ShallowDict(
            merged(self.referenced_nodes, referenced_nodes, MergeKey(raw='{<~}[<~]'))
        )
        return comp_res

    @ftrace(watch=[])
    def process_includes(self, comp_res: CompositionResult) -> CompositionResult:
        comp_res.find_special_nodes('include', lambda n: isinstance(n, IncludeNode))

        if not comp_res.special_nodes['include']:
            return comp_res

        # Process the current batch of includes
        comp_res.sort_special_nodes('include')
        for inode_path in comp_res.pop_all_special('include'):
            inode = inode_path.get_obj(comp_res.root)
            assert isinstance(inode, IncludeNode), f"Invalid node type: {type(inode)}"

            new_loader = self.copy()
            include_composed = compose_from_include_str(
                new_loader,
                include_str=inode.value,
                include_node_path=inode_path,
                composition_result=comp_res,
                custom_loaders=self.custom_loaders,
                node=inode,
            )
            comp_res.merge_composition_at(inode_path, include_composed)

        # Recursive call to process any new includes that were brought in
        return self.process_includes(comp_res)

    def dump(self, data, stream=None):
        if stream is None:
            from io import StringIO

            string_stream = StringIO()
            self.yaml.dump(data, string_stream)
            return string_stream.getvalue()
        else:
            return self.yaml.dump(data, stream)

    def dump_to_node(self, data):
        return dump_to_node(data)


##────────────────────────────────────────────────────────────────────────────}}}


def dump_to_node(data):
    if isinstance(data, Node):
        return data
    representer = DraconRepresenter()
    return representer.represent_data(data)


def load(config_path: str | Path, raw_dict=False, **kwargs):
    loader = DraconLoader(**kwargs)
    if raw_dict:
        loader.yaml.constructor.yaml_base_dict_type = dict
        loader.yaml.constructor.yaml_base_list_type = list
    return loader.load(config_path)


def load_node(node: Node, **kwargs):
    loader = DraconLoader(**kwargs)
    return loader.load_node(node)


def load_file(config_path: str | Path, raw_dict=True, **kwargs):
    return load(f'file:{config_path}', raw_dict, **kwargs)


@ftrace()
def loads(config_str: str, raw_dict=False, **kwargs):
    loader = DraconLoader(**kwargs)
    if raw_dict:
        loader.yaml.constructor.yaml_base_dict_type = dict
    return loader.loads(config_str)


def dump(data, stream=None, **kwargs):
    loader = DraconLoader(**kwargs)
    return loader.dump(data, stream)


def load_config_to_dict(maybe_config: str | DictLike) -> DictLike:
    if isinstance(maybe_config, str):
        loader = DraconLoader()
        conf = loader.load(maybe_config)
        conf.set_metadata({'dracon_origin': maybe_config})
        return conf
    return maybe_config


def compose_config_from_str(yaml, content):
    yaml.compose(content)
    assert isinstance(yaml.composer, DraconComposer)
    res = yaml.composer.get_result()
    return res


def cached_compose_config_from_str(yaml, content):
    cop = deepcopy(_cached_compose_config_from_str(yaml, content))
    return cop


@cached(LRUCache(maxsize=128), key=lambda yaml, content: hashkey(content))
def _cached_compose_config_from_str(yaml, content):
    return compose_config_from_str(yaml, content)


T = TypeVar('T')

LoadedConfig = Annotated[
    T | str,
    BeforeValidator(load_config_to_dict),
    PlainSerializer(lambda x: serialize_loaded_config(x)),
    Field(validate_default=True),
]


def serialize_loaded_config(c: DictLike) -> str | DictLike:
    if isinstance(c, MetadataDictLike):
        origin = c.get_metadata().get('dracon_origin')
        if origin is not None:
            return origin
    return c

================
File: dracon/merge.py
================
from typing import Optional, Any
import re
from pydantic import BaseModel
from enum import Enum
from dracon.utils import dict_like, DictLike, ListLike, ftrace, deepcopy, list_like
from dracon.nodes import (
    MergeNode,
    DraconMappingNode,
    DraconSequenceNode,
    IncludeNode,
)
from ruamel.yaml.nodes import Node
from dracon.keypath import KeyPath
from functools import lru_cache


def make_default_empty_mapping_node():
    return DraconMappingNode(
        tag='',
        value=[],
    )


@ftrace(watch=[])
def process_merges(comp_res):
    """
    Process all merge nodes in the composition result recursively until there are no more merges to process.
    Returns the modified composition result and whether any merges were performed.
    """
    any_merges = False

    while True:
        # Find all merge nodes
        comp_res.find_special_nodes('merge', lambda n: isinstance(n, MergeNode))
        comp_res.sort_special_nodes('merge')

        # Check if we found any merge nodes
        if not comp_res.special_nodes['merge']:
            break

        any_merges = True

        for merge_path in comp_res.pop_all_special('merge'):
            # Get value path (remove mapping key)
            merge_path = merge_path.removed_mapping_key()
            merge_node = merge_path.get_obj(comp_res.root)
            parent_path = merge_path.copy().up()
            node_key = merge_path[-1]
            parent_node = parent_path.get_obj(comp_res.root)

            # Validate parent node is a dictionary
            if not dict_like(parent_node):
                raise ValueError(
                    'While processing merge node',
                    merge_node.start_mark,
                    'Parent of merge node must be a dictionary',
                    f'but got {type(parent_node)} at {parent_node.start_mark}',
                )

            # Get the merge key node and validate
            assert node_key in parent_node, f'Key {node_key} not found in parent node'
            key_node = parent_node.get_key(node_key)
            assert isinstance(
                key_node, MergeNode
            ), f'Invalid merge node type: {type(key_node)} at {node_key}. {merge_path=}'

            try:
                merge_key = MergeKey(raw=key_node.merge_key_raw)
            except Exception as e:
                raise ValueError(
                    'While processing merge node',
                    merge_node.start_mark,
                    f'Error: {str(e)}',
                ) from None

            del parent_node[node_key]

            if merge_key.keypath:
                parent_path = parent_path + KeyPath(merge_key.keypath)

            new_parent = parent_path.get_obj(comp_res.root)
            new_parent = merged(new_parent, merge_node, merge_key)
            assert isinstance(new_parent, Node)

            comp_res.set_at(parent_path, new_parent)

        comp_res.make_map()

    return comp_res, any_merges


class MergeMode(Enum):
    # -> in the case of two dictionaries, append new keys
    # and will recursively merge subdict keys
    # when same keys are leaves and not dictionaries, see priority
    # when keys are lists, see list_mode
    # -> in the case of two lists, append new items
    APPEND = 'append'  # symbol: +

    # -> in the case of two dictionaries,
    # fully replace conflicting keys and append new keys
    # -> in the case of two lists, replace the whole list
    REPLACE = 'replace'  # symbol: ~


class MergePriority(Enum):
    NEW = 'new'  # symbol: <
    EXISTING = 'existing'  # symbol: >


class MergeKey(BaseModel):
    raw: str
    dict_mode: MergeMode = MergeMode.APPEND
    dict_priority: MergePriority = MergePriority.EXISTING
    dict_depth: Optional[int] = None
    list_mode: MergeMode = MergeMode.REPLACE
    list_priority: MergePriority = MergePriority.EXISTING
    list_depth: Optional[int] = None
    keypath: Optional[str] = None

    @staticmethod
    def is_merge_key(key: str) -> bool:
        return key.startswith('<<')

    def get_mode_priority(
        self,
        mode_str: str,
        default_mode=MergeMode.APPEND,
        default_priority=MergePriority.EXISTING,
    ):
        # + means RECURSE or APPEND
        # ~ means REPLACE
        # > means EXISTING
        # < means NEW
        mode, priority = default_mode, default_priority
        assert (
            '+' not in mode_str or '~' not in mode_str
        ), 'Only one of + or ~ is allowed in dict_mode'
        if '+' in mode_str:
            mode = MergeMode.APPEND
        if '~' in mode_str:
            mode = MergeMode.REPLACE
        assert (
            '>' not in mode_str or '<' not in mode_str
        ), 'Only one of > or < is allowed in dict_priority'

        if '>' in mode_str:
            priority = MergePriority.EXISTING
        if '<' in mode_str:
            priority = MergePriority.NEW

        depth = None
        depth_str = re.search(r'(\d+)', mode_str)
        if depth_str:
            depth = int(depth_str.group(1))

        return mode, priority, depth

    def model_post_init(self, *args, **kwargs):
        # to find the dict_mode and list_mode, we need to parse the raw key
        # things inside {} concern the dict_mode and priority
        # things inside [] concern the list_mode and priority

        super().model_post_init(*args, **kwargs)

        # check that only zero or one [] and {} are present
        assert self.raw.count('{') <= 1, 'Only one {} is allowed in merge key'
        assert self.raw.count('[') <= 1, 'Only one [] is allowed in merge key'
        # check that they close properly
        assert self.raw.count('{') == self.raw.count('}'), 'Mismatched {} in merge key'
        assert self.raw.count('[') == self.raw.count(']'), 'Mismatched [] in merge key'

        # check if it has a keypath part (anything after @)
        default_dict_priority = MergePriority.EXISTING
        default_dict_mode = MergeMode.APPEND
        default_list_priority = MergePriority.EXISTING
        default_list_mode = MergeMode.REPLACE

        keypath_str = re.search(r'@(.+)', self.raw)
        if keypath_str:  # it's an @ keypath, aka an override
            self.keypath = keypath_str.group(1)
            # by default, we override with the new value
            default_dict_priority = MergePriority.NEW
            default_list_priority = MergePriority.NEW

        dict_str = re.search(r'{(.+)}', self.raw)
        if dict_str:
            dict_str = dict_str.group(1)
        else:
            dict_str = ''

        self.dict_mode, self.dict_priority, self.dict_depth = self.get_mode_priority(
            dict_str, default_mode=default_dict_mode, default_priority=default_dict_priority
        )

        list_str = re.search(r'\[(.+)\]', self.raw)
        if list_str:
            list_str = list_str.group(1)
        else:
            list_str = ''
        self.list_mode, self.list_priority, self.list_depth = self.get_mode_priority(
            list_str, default_mode=default_list_mode, default_priority=default_list_priority
        )


DEFAULT_ADD_TO_CONTEXT_MERGE_KEY = MergeKey(raw='<<{~<}[~<]')


def merged(existing: Any, new: Any, k: MergeKey = DEFAULT_ADD_TO_CONTEXT_MERGE_KEY) -> DictLike:
    def merge_value(v1: Any, v2: Any, depth: int = 0) -> Any:
        if type(v1) is type(v2) and hasattr(v1, 'merged_with') and hasattr(v2, 'merged_with'):
            return v1.merged_with(v2, depth + 1)
        elif dict_like(v1) and dict_like(v2):
            return merge_dicts(v1, v2, depth + 1)
        elif list_like(v1) and list_like(v2):
            return merge_lists(v1, v2, depth + 1)
        else:
            return v1 if k.dict_priority == MergePriority.EXISTING else v2

    def merge_dicts(dict1: DictLike, dict2: DictLike, depth: int = 0) -> DictLike:
        pdict, other = (
            (dict1, dict2) if k.dict_priority == MergePriority.EXISTING else (dict2, dict1)
        )

        if k.dict_depth is not None and depth > k.dict_depth:
            return pdict

        result = pdict.copy()

        if hasattr(pdict, 'tag') and hasattr(other, 'tag'):
            # we're dealing with nodes
            if pdict.tag.startswith('!'):
                result.tag = pdict.tag
            elif other.tag.startswith('!'):
                result.tag = other.tag

        for key, value in other.items():
            if key not in result:
                result[key] = value
            elif k.dict_mode == MergeMode.APPEND:
                result[key] = (
                    merge_value(result[key], value, depth + 1)
                    if k.dict_priority == MergePriority.EXISTING
                    else merge_value(value, result[key], depth + 1)
                )
        return result

    def merge_lists(list1: ListLike, list2: ListLike, depth: int = 0) -> ListLike:
        if (k.list_depth is not None and depth > k.list_depth) or k.list_mode == MergeMode.REPLACE:
            return list1 if k.list_priority == MergePriority.EXISTING else list2
        return list1 + list2 if k.list_priority == MergePriority.EXISTING else list2 + list1

    return merge_value(existing, new)


def add_to_context(context, item, merge_key=DEFAULT_ADD_TO_CONTEXT_MERGE_KEY):
    if hasattr(item, 'context'):
        item.context = context_add(item.context, context, merge_key)
    if hasattr(item, '_clear_ctx') and item._clear_ctx:
        for k in item._clear_ctx:
            if k in item.context:
                del item.context[k]


@ftrace(inputs=False, output=False, watch=[])
def reset_context(item, ignore_dracon_namespace=True):
    newctx = {}
    if hasattr(item, 'context'):
        for k, v in item.context.items():
            if ignore_dracon_namespace and k.startswith('__DRACON_'):
                newctx[k] = v
        item.context = newctx


@ftrace(inputs=False, output=False, watch=[])
def context_add(base, newcontext, merge_key=DEFAULT_ADD_TO_CONTEXT_MERGE_KEY):
    m = merged(base, newcontext, merge_key)
    return m


def dict_diff(dict1, dict2):
    """
    Returns a dictionary with the differences between dict1 and dict2
    """
    diff = {}
    for key, value in dict1.items():
        if key not in dict2:
            diff[key] = value
        elif value != dict2[key]:
            if dict_like(value) and dict_like(dict2[key]):
                diff[key] = dict_diff(value, dict2[key])
            else:
                diff[key] = dict2[key]
    for key, value in dict2.items():
        if key not in dict1:
            diff[key] = value
    return diff


# ideal syntax:
# <<{>~}(attr1,attr2{+<}[+](subattr{~})): "value"

================
File: dracon/nodes.py
================
## {{{                          --     imports     --
from ruamel.yaml.nodes import Node, MappingNode, SequenceNode, ScalarNode
from ruamel.yaml.tag import Tag
from dracon.utils import dict_like, list_like, node_repr, deepcopy, make_hashable, ShallowDict
from typing import Any, Hashable, Optional
##────────────────────────────────────────────────────────────────────────────}}}

## {{{                           --     utils     --

MERGE_TAG = Tag(suffix='tag:yaml.org,2002:merge')
STR_TAG = Tag(suffix='tag:yaml.org,2002:str')

DRACON_UNSET_VALUE = '__!DRACON_UNSET_VALUE!__'
DEFAULT_MAP_TAG = 'tag:yaml.org,2002:map'
DEFAULT_SEQ_TAG = 'tag:yaml.org,2002:seq'
DEFAULT_SCALAR_TAG = 'tag:yaml.org,2002:str'


def reset_tag(node):
    if isinstance(node, SequenceNode):
        node.tag = DEFAULT_SEQ_TAG
    elif isinstance(node, MappingNode):
        node.tag = DEFAULT_MAP_TAG
    else:
        node.tag = DEFAULT_SCALAR_TAG


def make_node(value: Any, tag=None, **kwargs):
    if isinstance(value, Node):
        if tag is not None:
            value.tag = tag
        return value

    if dict_like(value):
        return DraconMappingNode(
            tag or DEFAULT_MAP_TAG,
            value=[(make_node(k), make_node(v)) for k, v in value.items()],
            **kwargs,
        )
    elif list_like(value):
        return DraconSequenceNode(
            tag or DEFAULT_SEQ_TAG, value=[make_node(v) for v in value], **kwargs
        )
    else:
        return ScalarNode(tag or DEFAULT_SCALAR_TAG, value, **kwargs)


##────────────────────────────────────────────────────────────────────────────}}}


def base_node_hash(node):
    return hash(
        (node.tag, node.value, node.start_mark.line, node.start_mark.column, node.start_mark.name)
    )


class DraconScalarNode(ScalarNode):
    def __init__(
        self,
        tag,
        value,
        start_mark=None,
        end_mark=None,
        style=None,
        comment=None,
        anchor=None,
    ):
        ScalarNode.__init__(self, tag, value, start_mark, end_mark, comment=comment, anchor=anchor)

    def __str__(self):
        return node_repr(self)

    def __repr__(self):
        return node_repr(self)

    def __getstate__(self):
        state = {
            'tag': self.tag,
            'value': self.value,
            'start_mark': self.start_mark,
            'end_mark': self.end_mark,
            'style': self.style,
            'comment': self.comment,
            'anchor': self.anchor,
        }
        return state

    def __setstate__(self, state):
        self.tag = state['tag']
        self.value = state['value']
        self.start_mark = state['start_mark']
        self.end_mark = state['end_mark']
        self.style = state['style']
        self.comment = state['comment']
        self.anchor = state['anchor']


class ContextNode(DraconScalarNode):
    def __init__(
        self,
        value,
        start_mark=None,
        end_mark=None,
        tag=None,
        anchor=None,
        comment=None,
        context=None,
    ):
        DraconScalarNode.__init__(
            self,
            value=value,
            start_mark=start_mark,
            end_mark=end_mark,
            tag=tag,
            comment=comment,
            anchor=anchor,
        )
        self.context = (
            ShallowDict(context or {}) if not isinstance(context, ShallowDict) else context
        )

    def __getstate__(self):
        state = DraconScalarNode.__getstate__(self)
        state['context'] = self.context.copy()
        return state

    def __setstate__(self, state):
        DraconScalarNode.__setstate__(self, state)
        self.context = state['context']


    def copy(self):
        """Create a shallow copy with the context also shallow copied."""
        return self.__class__(
            value=self.value,
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            tag=self.tag,
            anchor=self.anchor,
            comment=self.comment,
            context=self.context.copy(),  # Shallow copy the context
        )


class IncludeNode(ContextNode):
    def __init__(
        self,
        value,
        start_mark=None,
        end_mark=None,
        tag=None,
        anchor=None,
        comment=None,
        context=None,
    ):
        ContextNode.__init__(
            self,
            value=value,
            start_mark=start_mark,
            end_mark=end_mark,
            tag=tag,
            comment=comment,
            anchor=anchor,
            context=context,
        )


class MergeNode(DraconScalarNode):
    def __init__(self, value, start_mark=None, end_mark=None, tag=None, anchor=None, comment=None):
        self.merge_key_raw = value
        DraconScalarNode.__init__(
            self, STR_TAG, value, start_mark, end_mark, comment=comment, anchor=anchor
        )

    def __getstate__(self):
        state = DraconScalarNode.__getstate__(self)
        state['merge_key_raw'] = self.merge_key_raw
        return state

    def __setstate__(self, state):
        DraconScalarNode.__setstate__(self, state)
        self.merge_key_raw = state['merge_key_raw']


class UnsetNode(DraconScalarNode):
    def __init__(self, start_mark=None, end_mark=None, tag=None, anchor=None, comment=None):
        DraconScalarNode.__init__(
            self,
            tag=STR_TAG,
            value='',
            start_mark=start_mark,
            end_mark=end_mark,
            comment=comment,
            anchor=anchor,
        )

    def __deepcopy__(self, memo):
        return UnsetNode(
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            tag=self.tag,
            anchor=self.anchor,
            comment=self.comment,
        )


## {{{                        --     MappingNode     --


class DraconMappingNode(MappingNode):
    def __init__(
        self,
        tag: Any,
        value: Any,
        start_mark: Any = None,
        end_mark: Any = None,
        flow_style: Any = None,
        comment: Any = None,
        anchor: Any = None,
    ) -> None:
        MappingNode.__init__(self, tag, value, start_mark, end_mark, flow_style, comment, anchor)
        self._recompute_map()

    def _recompute_map(self):
        self.map = {}  # key value -> index
        for idx, (key, _) in enumerate(self.value):
            if not hasattr(key, 'value'):
                raise ValueError(f'Key {key!r} has no value attribute')
            key_val = str(key.value)
            if key_val in self.map:
                raise ValueError(f'Duplicate key: {key_val!r}')
            self.map[key_val] = idx

    # and implement a get[] (and set) method
    def __getitem__(self, key: Hashable) -> Node:
        if isinstance(key, Node):
            key = key.value
        key_str = str(key)
        return self.value[self.map[key_str]][1]

    def __setitem__(self, key: Hashable, value: Node):
        if isinstance(key, Node):
            keyv = key.value
        else:
            keyv = key
        key_str = str(keyv)
        if key_str in self.map:
            idx = self.map[key_str]
            realkey, _ = self.value[idx]
            self.value[idx] = (realkey, value)
        else:
            # assert isinstance(key, Node)
            self.value.append((key, value))
            self._recompute_map()

    def __delitem__(self, key: Hashable):
        if isinstance(key, Node):
            key = key.value
        # Convert key to string for lookup
        key_str = str(key)
        idx = self.map[key_str]
        del self.value[idx]
        self._recompute_map()

    def __contains__(self, key: Hashable) -> bool:
        if isinstance(key, Node):
            key = key.value
        key_str = str(key)
        return key_str in self.map

    def keys(self):
        return self.map.keys()

    def values(self):
        return (value for _, value in self.value)

    def items(self):
        return self.value

    def get(self, key: Hashable, default=None):
        return self[key] if key in self else default

    def get_key(self, key: Hashable):
        key_str = str(key)
        idx = self.map[key_str]
        return self.value[idx][0]

    def __len__(self):
        return len(self.map)

    def copy(self):
        return self.__class__(
            tag=self.tag,
            value=self.value.copy(),
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            flow_style=self.flow_style,
            comment=self.comment,
            anchor=self.anchor,
        )

    def __deepcopy__(self, memo):
        copied_value = deepcopy(self.value, memo)
        n = self.__class__(
            tag=self.tag,
            value=copied_value,
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            flow_style=self.flow_style,
            comment=self.comment,
            anchor=self.anchor,
        )
        n.ctag = self.ctag
        n.id = self.id
        return n

    def append(self, newvalue: tuple[Node, Node]):
        key, _ = newvalue
        self.value.append(newvalue)
        key_str = str(key.value)
        if key_str in self.map:
            raise ValueError(f'Duplicate key: {key_str}')
        self.map[key_str] = len(self.value) - 1

    def clear(self):
        self.value = []
        self.map = {}

    def __str__(self):
        return node_repr(self)

    def __repr__(self):
        return node_repr(self)

    def __getstate__(self):
        state = {
            'tag': self.tag,
            'value': self.value,
            'start_mark': self.start_mark,
            'end_mark': self.end_mark,
            'flow_style': self.flow_style,
            'comment': self.comment,
            'anchor': self.anchor,
            'map': self.map,
        }
        return state

    def __setstate__(self, state):
        self.tag = state['tag']
        self.value = state['value']
        self.start_mark = state['start_mark']
        self.end_mark = state['end_mark']
        self.flow_style = state['flow_style']
        self.comment = state['comment']
        self.anchor = state['anchor']
        self.map = state['map']


##────────────────────────────────────────────────────────────────────────────}}}


## {{{                       --     SequenceNode     --
class DraconSequenceNode(SequenceNode):
    def __getitem__(self, index: int) -> Node:
        return self.value[index]

    def __setitem__(self, index: int, value: Node):
        self.value[index] = value

    def __delitem__(self, index: int):
        del self.value[index]

    def __add__(self, other: 'DraconSequenceNode') -> 'DraconSequenceNode':
        return self.__class__(
            tag=self.tag,
            value=self.value + other.value,
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            flow_style=self.flow_style,
            comment=self.comment,
            anchor=self.anchor,
        )

    def __len__(self) -> int:
        return len(self.value)

    def __contains__(self, value: Node) -> bool:
        return value in self.value

    def __iter__(self):
        return iter(self.value)

    def __append__(self, value: Node):
        self.value.append(value)

    def __extend__(self, values: list[Node]):
        self.value.extend(values)

    def extend(self, values: list[Node]):
        self.value.extend(values)

    def append(self, value: Node):
        self.value.append(value)

    def __getstate__(self):
        state = {
            'tag': self.tag,
            'value': self.value,
            'start_mark': self.start_mark,
            'end_mark': self.end_mark,
            'flow_style': self.flow_style,
            'comment': self.comment,
            'anchor': self.anchor,
        }
        return state

    def __setstate__(self, state):
        self.tag = state['tag']
        self.value = state['value']
        self.start_mark = state['start_mark']
        self.end_mark = state['end_mark']
        self.flow_style = state['flow_style']
        self.comment = state['comment']
        self.anchor = state['anchor']

    @classmethod
    def from_mapping(cls, mapping: DraconMappingNode, empty=False, elt_tag=None):
        tag = mapping.tag
        if tag == DEFAULT_MAP_TAG:
            tag = DEFAULT_SEQ_TAG
        newseq = cls(
            tag=tag,
            value=[],
            start_mark=mapping.start_mark,
            end_mark=mapping.end_mark,
            flow_style=mapping.flow_style,
            comment=mapping.comment,
            anchor=mapping.anchor,
        )

        if not empty:
            elt_tag = elt_tag or DEFAULT_MAP_TAG
            for key, value in mapping.items():
                mapval = DraconMappingNode(
                    tag=elt_tag,
                    value=[(key, value)],
                )
                newseq.append(mapval)

        return newseq

    def __str__(self):
        return node_repr(self)

    def __repr__(self):
        return node_repr(self)

    def __deepcopy__(self, memo):
        n = self.__class__(
            tag=self.tag,
            value=deepcopy(self.value, memo),
            start_mark=self.start_mark,
            end_mark=self.end_mark,
            flow_style=self.flow_style,
            comment=self.comment,
            anchor=self.anchor,
        )
        n.ctag = self.ctag
        n.id = self.id
        return n

    # def __hash__(self):
    # return hash((tuple(self.value), base_node_hash(self)))


##────────────────────────────────────────────────────────────────────────────}}}


## {{{                          --     hashes     --
def dracon_scalar_node_hash(self):
    startmark_hash = (
        hash((self.start_mark.line, self.start_mark.column, self.start_mark.name))
        if self.start_mark
        else 0
    )

    return hash(
        (self.__class__.__name__, self.tag, self.value, self.anchor, startmark_hash, self.ctag)
    )


def context_node_hash(self):
    base_hash = dracon_scalar_node_hash(self)
    context_items = make_hashable(self.context)
    return hash((base_hash, context_items))


def include_node_hash(self):
    """Hash function for IncludeNode."""
    return context_node_hash(self)


def merge_node_hash(self):
    base_hash = dracon_scalar_node_hash(self)
    return hash((base_hash, self.merge_key_raw))


def unset_node_hash(self):
    # UnsetNode hash is simpler since it has fixed value
    return hash((self.__class__.__name__, self.tag, self.anchor))


def dracon_mapping_node_hash(self):
    items_hash = hash(
        tuple((k.value, hash(v)) for k, v in sorted(self.value, key=lambda x: x[0].value))
    )
    return hash((self.__class__.__name__, self.tag, items_hash, self.anchor))


def dracon_sequence_node_hash(self):
    elements_hash = hash(tuple(hash(v) for v in self.value))
    return hash((self.__class__.__name__, self.tag, elements_hash, self.anchor))


# DraconScalarNode.__hash__ = dracon_scalar_node_hash
# ContextNode.__hash__ = context_node_hash
# IncludeNode.__hash__ = include_node_hash
# MergeNode.__hash__ = merge_node_hash
# UnsetNode.__hash__ = unset_node_hash
# DraconMappingNode.__hash__ = dracon_mapping_node_hash
# DraconSequenceNode.__hash__ = dracon_sequence_node_hash

##────────────────────────────────────────────────────────────────────────────}}}

================
File: dracon/representer.py
================
from ruamel.yaml.representer import RoundTripRepresenter
from ruamel.yaml.nodes import MappingNode, ScalarNode
from typing import Protocol
from ruamel.yaml.scalarstring import PlainScalarString
from pydantic import BaseModel
from dracon.utils import list_like, dict_like
from dracon.resolvable import Resolvable
from dracon.deferred import DeferredNode
from dracon.interpolation import InterpolableNode
from typing import Any, Hashable, Mapping, Sequence, Union
from typing_extensions import runtime_checkable

import numpy as np


# protocol to identify classes that have a dracon_dump method
@runtime_checkable
class DraconDumpable(Protocol):
    def dracon_dump_to_node(self, representer): ...


class DraconRepresenter(RoundTripRepresenter):
    def __init__(self, *args, full_module_path=True, exclude_defaults=True, **kwargs):
        super().__init__(*args, **kwargs)
        self.full_module_path = (
            full_module_path  # if True, the full module path will be used as the tag
        )
        self.exclude_defaults = exclude_defaults

    def represent_data(self, data: Any) -> Any:
        if isinstance(data, DraconDumpable):
            return data.dracon_dump_to_node(self)
        if isinstance(data, BaseModel):
            return self.represent_pydantic_model(data)
        if isinstance(data, Sequence) and not isinstance(data, str):
            return self.represent_list(data)

        return super().represent_data(data)

    def represent_pydantic_model(self, data):
        assert isinstance(data, BaseModel)

        tag = f"!{data.__class__.__name__}"
        if self.full_module_path:
            tag = f"!{data.__class__.__module__}.{data.__class__.__name__}"

        model_dump = data.model_dump(exclude_defaults=self.exclude_defaults)

        # we dump the object using the model_dump method
        # (which uses the preffered aliases and serializations)
        # EXCEPT for the fields that are BaseModel instances
        # where we recursively call this method instead

        for name, attr in dict(data).items():
            if isinstance(attr, BaseModel):
                model_dump[name] = attr
            elif list_like(attr):
                for i, x in enumerate(attr):
                    if isinstance(x, BaseModel):
                        model_dump[name][i] = x
            elif dict_like(attr):
                for k, v in attr.items():
                    if isinstance(v, BaseModel):
                        model_dump[name][k] = v

        node = self.represent_mapping(tag, model_dump)
        return node


# TODO:
# - [ ] make keypaths regex to specify which keys are deferred
# -

================
File: dracon/resolvable.py
================
from typing import (
    Any,
    TypeVar,
    Generic,
    Type,
    Optional,
    List,
)


from dracon.nodes import (
    make_node,
    IncludeNode,
    MergeNode,
    DraconMappingNode,
)

from pydantic_core import core_schema
from dracon.utils import get_inner_type, deepcopy


T = TypeVar("T")

"""
> # Resolvable objects
> A Resolvable stores the actual yaml node + the constructor that can be used to resume construction.

It's essentially a snapshot of the construction process.
It's useful when you want to 
 -> manually orchestrate the construction of some objects in a specific order (e.g. you need to parse some args first)
 -> add some context to the construction (for example, $SOME_VAR) that is not available at the time of parsing
 -> merge 2 or more yaml nodes manually (the dracon merge operator works on nodes or dicts, not general objects)

>[!WARNING] Resolvable != Interpolable
> `Resolvable` could understandably be confused with a `(Lazy)Interpolable`. They are different concepts:
>  -> The `(Lazy)Interpolable` class is used to store and defer the __interpolation__ of an interpolable __value__ e.g. `${2 + 2}`
>  -> The `Resolvable` class is used to pause and defer the __construction__ of a __whole branch__ until asked to resume (resolve)
> A resolvable can contain Interpolable leaves, and can even contain other Resolvables. 

"""


class Resolvable(Generic[T]):
    def __init__(
        self,
        node: Optional[Any] = None,
        ctor: Optional[Any] = None,
        inner_type: Optional[Type[T]] = None,
    ):
        self.node = node
        self.ctor = ctor
        if inner_type is not None:
            self.inner_type = inner_type
        else:
            self.inner_type = get_inner_type(self.__class__)

    @classmethod
    def __get_pydantic_core_schema__(cls, source_type: Any, handler: Any) -> core_schema.CoreSchema:
        cls_type = get_inner_type(cls)
        t_schema = handler(cls_type)
        return core_schema.union_schema(
            [
                t_schema,
                core_schema.is_instance_schema(cls),
            ]
        )

    def resolve(self, context=None):
        """
        Resolve the object from the stored node and constructor, adding context if needed
        Note: it doesn't necessarily returns an object of inner_type. In theory yes,
        but in practice, the node tag could have been changed at any point. And a resolvable doesn't
        enforce any constraints on the type of the object it will return. It just pauses the construction
        and allows you to resume it later.
        """
        assert self.ctor is not None
        assert self.node is not None
        ctor = deepcopy(self.ctor)
        ctor.context.update(context or {})
        return ctor.construct_object(self.node)

    def copy(self):
        return deepcopy(self)

    def empty(self):
        return self.node is None or not self.node.value

    def __bool__(self):
        return not self.empty()

    def __repr__(self):
        return f"Resolvable(node={self.node}, inner_type={self.inner_type})"

================
File: dracon/trace.py
================
import os
import sys
import inspect
from contextlib import ContextDecorator, nullcontext
from typing import Any, Dict, Optional, Set, List, Callable
from dataclasses import dataclass

# color configuration
COLORS = {
    'RED': '\033[38;2;249;65;68m',
    'ORANGE': '\033[38;2;243;114;44m',
    'YELLOW_ORANGE': '\033[38;2;248;150;30m',
    'LIGHT_ORANGE': '\033[38;2;249;132;74m',
    'YELLOW': '\033[38;2;249;199;79m',
    'GREEN': '\033[38;2;144;190;109m',
    'TEAL': '\033[38;2;67;170;139m',
    'DARK_TEAL': '\033[38;2;77;144;142m',
    'BLUE_GREY': '\033[38;2;87;117;144m',
    'BLUE': '\033[38;2;39;125;161m',
    'GREY': '\033[90m',
    'RESET': '\033[0m',
}

# default configuration settings
DEFAULT_COLORS = {
    'line_number': COLORS['GREY'],
    'input': '',
    'input_name': COLORS['GREEN'],
    'output': '',
    'reset': COLORS['RESET'],
}

DEFAULT_GLYPHS = {
    'assign': '←',
    'create': '=',
    'return': '>',
}


# stream handling classes for output padding
class PaddedStdout:
    """Handles padded output stream with proper indentation."""

    def __init__(self, stream: Any, padding: str):
        if isinstance(stream, PaddedStdout):
            self.base_stream = stream.base_stream
            self.padding = stream.padding + padding
        else:
            self.base_stream = stream
            self.padding = padding
        self.buffer = ''

    def write(self, data: str) -> None:
        self.buffer += data
        while '\n' in self.buffer:
            line, self.buffer = self.buffer.split('\n', 1)
            self.base_stream.write(self.padding + line + '\n')

    def flush(self) -> None:
        if self.buffer:
            self.base_stream.write(self.padding + self.buffer)
            self.buffer = ''
        self.base_stream.flush()


class pad_output(ContextDecorator):
    """Context manager for handling padded output."""

    def __init__(self, padding: str):
        self.padding = padding

    def __enter__(self):
        self._old_stdout = sys.stdout
        sys.stdout = PaddedStdout(sys.stdout, self.padding)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout = self._old_stdout


# alias to with_indent for backward compatibility
with_indent = pad_output


@dataclass
class TraceConfig:
    """Configuration container for trace settings."""

    colors: Dict[str, str]
    glyphs: Dict[str, str]
    truncate_length: int
    preface_filename: bool
    inputs: bool
    output: bool
    watch: Optional[List[str]]
    name: Optional[str]

    @classmethod
    def create_default(cls, **kwargs):
        """Create trace configuration with default values."""
        config = cls(
            colors=DEFAULT_COLORS.copy(),
            glyphs=DEFAULT_GLYPHS.copy(),
            truncate_length=200,
            preface_filename=True,
            inputs=True,
            output=True,
            watch=None,
            name=None,
        )
        for key, value in kwargs.items():
            if hasattr(config, key):
                if key == 'colors' and value:
                    config.colors.update(value)
                elif key == 'glyphs' and value:
                    config.glyphs.update(value)
                else:
                    setattr(config, key, value)
        return config


class TracePrinter:
    """Handles all trace-related output formatting and printing."""

    def __init__(self, config: TraceConfig, func_color: str):
        self.config = config
        self.func_color = func_color
        # Padding with a vertical bar; note the trailing space.
        self.padding = f'{func_color}│ {COLORS["RESET"]}'

    def truncate(self, s: Any) -> str:
        """Truncate long strings with ellipsis in the middle."""
        s = str(s)
        lines = []
        for line in s.split('\n'):
            if len(line) > self.config.truncate_length:
                half = (self.config.truncate_length - 3) // 2
                out = line[:half] + '...' + line[-half:]
                lines.append(out)
            else:
                lines.append(line)
        return '\n'.join(lines)

    def print_function_entry(self, func_name: str, line_info: str) -> None:
        """Print function entry with proper formatting."""
        preface_str = (
            f"{self.config.colors['line_number']}@{line_info}{self.config.colors['reset']}"
            if self.config.preface_filename
            else ""
        )
        func_call_str = f"{self.func_color}┌ {func_name}{preface_str}{self.config.colors['reset']}("
        # Remove extra padding: if outermost, remove only one character (the extra space)
        if isinstance(sys.stdout, PaddedStdout):
            sys.stdout.write('\b\b')
        print(func_call_str)

    def print_arguments(self, bound_args: inspect.BoundArguments) -> Set[str]:
        """Print function arguments and return set of argument names."""
        arg_strings = []
        input_arg_names = set(bound_args.arguments.keys())

        for name, value in bound_args.arguments.items():
            arg_str = (
                f"{self.config.colors['input']}{self.truncate(value)}{self.config.colors['reset']}"
            )
            if '\n' in arg_str:
                print(f"{self.func_color}  {name} = {self.config.colors['reset']}")
                arg_str = arg_str.lstrip('\n')
                with pad_output("    "):
                    print(arg_str)
            else:
                arg_strings.append(
                    f"{self.func_color}  {name} = {self.config.colors['reset']}{arg_str}"
                )

        for arg_str in arg_strings:
            print(arg_str)
        print("):")
        return input_arg_names

    def print_variable_update(self, line_info: str, var: str, value: Any, is_new: bool) -> None:
        """Print variable updates with proper formatting."""
        glyph = self.config.glyphs['create'] if is_new else self.config.glyphs['assign']
        print(
            f"{self.config.colors['line_number']}{line_info}:{self.config.colors['reset']}"
            f"{var} {glyph} {self.truncate(value)}"
        )

    def print_function_exit(self, func_name: str, result: Any) -> None:
        """Print function exit with result."""
        if isinstance(sys.stdout, PaddedStdout):
            sys.stdout.write('\b\b')
        print(f"{self.func_color}└ {func_name}{self.config.colors['reset']} ")

        @pad_output(
            f"\b\b  {self.func_color}{self.config.glyphs['return']} {self.config.colors['reset']}"
        )
        def print_res():
            print(
                f"{self.config.colors['output']}{self.truncate(result)}{self.config.colors['reset']}"
            )

        print_res()


class TraceHandler:
    """Manages function tracing and variable watching."""

    def __init__(self, config: TraceConfig, printer: TracePrinter, input_arg_names: Set[str]):
        self.config = config
        self.printer = printer
        self.input_arg_names = input_arg_names
        self.watched_vars_prev = {}
        self.watching_all = config.watch is None
        if not self.watching_all and config.watch:
            for var in config.watch:
                self.watched_vars_prev[var] = None

    def handle_variable_updates(self, frame: Any, line_info: str) -> None:
        """Handle updates to watched variables."""
        local_vars = frame.f_locals
        vars_to_watch = local_vars.keys() if self.watching_all else self.config.watch

        for var in vars_to_watch:
            if var in local_vars:
                value = local_vars[var]
                prev_value = self.watched_vars_prev.get(var, '__UNINITIALIZED__')
                if prev_value == '__UNINITIALIZED__':
                    if var not in self.input_arg_names:
                        self.printer.print_variable_update(line_info, var, value, is_new=True)
                    self.watched_vars_prev[var] = value
                elif value != prev_value:
                    self.printer.print_variable_update(line_info, var, value, is_new=False)
                    self.watched_vars_prev[var] = value


def get_color_for_func(func_name: str) -> str:
    """Assign a consistent color to a function based on its name."""
    colors_list = [
        COLORS['RED'],
        COLORS['ORANGE'],
        COLORS['YELLOW_ORANGE'],
        COLORS['LIGHT_ORANGE'],
        COLORS['YELLOW'],
        COLORS['GREEN'],
        COLORS['TEAL'],
        COLORS['DARK_TEAL'],
        COLORS['BLUE_GREY'],
        COLORS['BLUE'],
    ]
    hash_value = sum(ord(c) for c in func_name) % len(colors_list)
    return colors_list[hash_value]


def get_source_info(func: Callable, frame: Optional[Any] = None) -> tuple:
    """Extract source file and line information."""
    filename = inspect.getsourcefile(func)
    filename = filename.split("/")[-1] if filename else "<unknown>"
    if frame:
        lineno = frame.f_lineno
        if frame.f_code.co_filename:
            filename = frame.f_code.co_filename.split("/")[-1]
    else:
        try:
            _, lineno = inspect.getsourcelines(func)
        except Exception:
            lineno = 0
    return filename, lineno


# Global counter for nested traced calls.
TRACE_DEPTH = 0


def ftrace(**kwargs):
    """
    A function decorator to trace execution, displaying inputs, outputs, and watched variables.

    parameters:
        inputs (bool): whether to show function inputs
        output (bool): whether to show function output
        watch (list): list of variable names to watch, or None for all
        preface_filename (bool): whether to show filename in trace
        colors (dict): custom colors for trace output
        glyphs (dict): custom glyphs for trace output
        truncate_length (int): maximum length for displayed values
        name (str): custom name for the traced function
    """
    if not os.getenv('ENABLE_FTRACE'):
        return lambda func: func

    config = TraceConfig.create_default(**kwargs)

    def decorator(func):
        func_color = get_color_for_func(func.__name__)
        printer = TracePrinter(config, func_color)

        def wrapper(*args, **kwargs):
            global TRACE_DEPTH
            # Always use padded output so that every traced call shows its vertical bar.
            with pad_output(printer.padding):
                TRACE_DEPTH += 1
                try:
                    func_class = (
                        f'{func.__qualname__.split(".")[0]}.'
                        if hasattr(func, '__qualname__') and '.' in func.__qualname__
                        else ''
                    )
                    func_name = f"{func_class}{func.__name__}"
                    call_frame = inspect.currentframe().f_back
                    filename, lineno = get_source_info(func, call_frame)
                    line_info = (
                        f"{filename}:l.{lineno}" if config.preface_filename else f"l.{lineno}"
                    )

                    # Print function entry and arguments.
                    printer.print_function_entry(func_name, line_info)
                    input_arg_names = set()
                    if config.inputs:
                        sig = inspect.signature(func)
                        bound_args = sig.bind(*args, **kwargs)
                        bound_args.apply_defaults()
                        input_arg_names = printer.print_arguments(bound_args)
                    else:
                        print(f"{func_name})")

                    # Set up tracing.
                    trace_handler = TraceHandler(config, printer, input_arg_names)

                    def local_trace(frame, event, arg):
                        if event == 'line':
                            _, lineno = get_source_info(func, frame)
                            li = (
                                f"{filename}:l.{lineno}"
                                if config.preface_filename
                                else f"l.{lineno}"
                            )
                            trace_handler.handle_variable_updates(frame, li)
                        return local_trace

                    def global_trace(frame, event, arg):
                        if event == 'call' and frame.f_code == func.__code__:
                            return local_trace
                        return None

                    sys.settrace(global_trace)
                    try:
                        result = func(*args, **kwargs)
                    finally:
                        sys.settrace(None)
                        if config.output:
                            printer.print_function_exit(func_name, result)
                    return result
                finally:
                    TRACE_DEPTH -= 1

        return wrapper

    return decorator

================
File: dracon/utils.py
================
## {{{                          --     imports     --
from collections.abc import Mapping, Sequence, Set
from ruamel.yaml.nodes import MappingNode, SequenceNode
from types import ModuleType, FunctionType
from typing import (
    Iterable,
    Hashable,
    Optional,
    TypeVar,
    Type,
    Tuple,
    Generic,
    Dict,
    Any,
    Protocol,
    Iterator,
    runtime_checkable,
    get_args,
)

import pickle
import copy
import sys
from dracon.trace import ftrace as ftrace
import os
from dracon.trace import with_indent

import logging
from collections.abc import MutableMapping

logger = logging.getLogger(__name__)
##────────────────────────────────────────────────────────────────────────────}}}

## {{{                      --     dict/list like     --{{{
K = TypeVar('K')
V = TypeVar('V')
E = TypeVar('E')
T = TypeVar('T')


# a dict that doesnt't allow deep copying (it always returns a shallow copy)
class ShallowDict(MutableMapping, Generic[K, V]):
    def __init__(self, *args, **kwargs):
        self._dict = dict(*args, **kwargs)

    def __getitem__(self, key: K) -> V:
        return self._dict[key]

    def __setitem__(self, key: K, value: V) -> None:
        self._dict[key] = value

    def __delitem__(self, key: K) -> None:
        del self._dict[key]

    def __iter__(self) -> Iterator[K]:
        return iter(self._dict)

    def __len__(self) -> int:
        return len(self._dict)

    def __copy__(self):
        # always return a shallow copy
        return ShallowDict(self._dict)

    def __deepcopy__(self, memo):
        # force deep copy to behave as a shallow copy
        return self.__copy__()

    def copy(self):
        return self.__copy__()

    def __repr__(self):
        return f'ShallowDict({self._dict})'


@runtime_checkable
class DictLike(Protocol[K, V]):
    def keys(self) -> Iterable[K]: ...
    def values(self) -> Iterable[V]: ...
    def items(self) -> Iterable[Tuple[K, V]]: ...
    def __getitem__(self, key: K) -> V: ...
    def __contains__(self, key: K) -> bool: ...
    def __setitem__(self, key: K, value: V) -> None: ...


@runtime_checkable
class MetadataDictLike(Protocol[K, V]):
    def keys(self) -> Iterable[K]: ...
    def values(self) -> Iterable[V]: ...
    def items(self) -> Iterable[Tuple[K, V]]: ...
    def __getitem__(self, key: K) -> V: ...
    def __contains__(self, key: K) -> bool: ...
    def __setitem__(self, key: K, value: V) -> None: ...
    def get_metadata(self) -> Dict: ...
    def set_metadata(self, metadata: Dict): ...


def metadata_dict_like(obj) -> bool:
    return isinstance(obj, MetadataDictLike)


def dict_like(obj) -> bool:
    return all(
        callable(getattr(obj, method, None))
        for method in ('keys', 'values', 'items', '__getitem__', '__contains__', '__setitem__')
    )


@runtime_checkable
class ListLike_Permissive(Protocol[E]):
    def __getitem__(self, index: int) -> Any: ...

    # can be concatenated with another list-like object:
    def __add__(self, other: 'ListLike_Permissive[E]') -> 'ListLike_Permissive[E]': ...

    def __iter__(self) -> Iterator[E]: ...

    def __len__(self) -> int: ...


def permissive_list_like(obj) -> bool:
    return isinstance(obj, ListLike_Permissive)


class ListLikeMeta(type):
    def __instancecheck__(cls, instance):
        return (
            permissive_list_like(instance)
            and not dict_like(instance)
            and not isinstance(instance, str)
        )


class ListLike(Generic[E], metaclass=ListLikeMeta):
    def __getitem__(self, index: int) -> Any: ...

    def __add__(self, other: 'ListLike[E]') -> 'ListLike[E]': ...

    def __append__(self, item: E) -> None: ...

    def __len__(self) -> int: ...

    def __iter__(self) -> Iterator[E]: ...


def list_like(obj) -> bool:
    return (
        all(
            callable(getattr(obj, method, None))
            for method in ('__getitem__', '__add__', '__iter__', '__len__')
        )
        and not dict_like(obj)
        and not isinstance(obj, str)
    )


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                         --     deepcopy     --


def debug_serialization(
    obj, operation='pickle', path='', max_depth=20, max_size_mb=None, seen=None
):
    if not os.getenv('ENABLE_SER_DEBUG', False):
        return

    if seen is None:
        seen = set()
    output = {}
    if max_depth <= 0:
        return output

    if id(obj) in seen:
        return output
    seen.add(id(obj))

    try:
        if operation == 'pickle':
            pickle.dumps(obj)
        elif operation == 'dill':
            import dill

            dill.dumps(obj)
        elif operation == 'sizeof':
            from .asizeof import asizeof

            s = asizeof(obj)
            import numpy as np
            import pandas as pd

            if (max_size_mb is not None) and (s > max_size_mb * 1024 * 1024):
                pretty_size = f"{s / 1024 / 1024:.2f} MB"
                if isinstance(obj, (np.ndarray, pd.DataFrame, pd.Series)):
                    return {
                        "path": path,
                        "operation": operation,
                        "err": f"Size exceeds {max_size_mb} MB: {pretty_size}",
                        "failing_children": [],
                    }
                raise ValueError(f"Size exceeds {max_size_mb} MB: {pretty_size}")

        elif operation == 'deepcopy':
            import copy

            copy.deepcopy(obj)
        else:
            raise ValueError(f"Unsupported operation: {operation}")
    except Exception as e:
        failing_children = []

        if hasattr(obj, "context"):
            result = debug_serialization(
                obj.context,
                operation,
                path=f"{path}.context",
                max_depth=max_depth - 1,
                max_size_mb=max_size_mb,
                seen=seen,
            )
            if result:
                failing_children.append(result)

        if dict_like(obj):
            for k, v in obj.items():
                result = debug_serialization(
                    v,
                    operation,
                    path=f"{path}[{k}]",
                    max_depth=max_depth - 1,
                    max_size_mb=max_size_mb,
                    seen=seen,
                )
                if result:
                    failing_children.append(result)

        elif list_like(obj):
            for i, v in enumerate(obj):
                result = debug_serialization(
                    v,
                    operation,
                    path=f"{path}[{i}]",
                    max_depth=max_depth - 1,
                    max_size_mb=max_size_mb,
                    seen=seen,
                )
                if result:
                    failing_children.append(result)

        elif hasattr(obj, "__dict__"):
            for k, v in obj.__dict__.items():
                result = debug_serialization(
                    v,
                    operation,
                    path=f"{path}.{k}",
                    max_depth=max_depth - 1,
                    max_size_mb=max_size_mb,
                    seen=seen,
                )
                if result:
                    failing_children.append(result)

        elif hasattr(obj, "items") and not isinstance(obj, (str, bytes)):
            try:
                for k, v in obj.items():
                    result = debug_serialization(
                        v,
                        operation,
                        path=f"{path}[{k}]",
                        max_depth=max_depth - 1,
                        max_size_mb=max_size_mb,
                        seen=seen,
                    )
                    if result:
                        failing_children.append(result)
            # somethings might change size while iterating:
            except RuntimeError:
                pass

        elif hasattr(obj, "__iter__") and not isinstance(obj, (str, bytes)):
            for i, v in enumerate(obj):
                result = debug_serialization(
                    v,
                    operation,
                    path=f"{path}[{i}]",
                    max_depth=max_depth - 1,
                    max_size_mb=max_size_mb,
                    seen=seen,
                )
                if result:
                    failing_children.append(result)

        elif hasattr(obj, "__getitem__") and not isinstance(obj, (str, bytes)):
            try:
                for i in range(len(obj)):
                    result = debug_serialization(
                        obj[i],
                        operation,
                        path=f"{path}[{i}]",
                        max_depth=max_depth - 1,
                        max_size_mb=max_size_mb,
                        seen=seen,
                    )
                    if result:
                        failing_children.append(result)
            except (TypeError, IndexError, KeyError):
                pass  # some objects with __getitem__ don't support integer indexing

        try:
            errmsg = f"{type(e).__name__}: {str(e)}"
        except Exception as e:
            errmsg = f"Error in exception handling: {str(e)}"

        output = {
            "path": path,
            "operation": operation,
            "err": errmsg,
            "failing_children": failing_children,
        }

    return output


def ser_debug(obj, operation='deepcopy', **kwargs):
    out = debug_serialization(obj, operation=operation, **kwargs)
    if out:
        errors = {}

        def collect_errors(err):
            nonlocal errors
            if not err["failing_children"]:
                errors[err["path"]] = err["err"]
            else:
                for child in err["failing_children"]:
                    collect_errors(child)

        collect_errors(out)
        if errors:
            # find original call to this ser_debug function to get the caller's line number and file name
            import inspect

            stack = inspect.stack()
            lineno = stack[1].lineno
            filename = stack[1].filename
            logger.error(f"Serialization error in {filename}:{lineno}")

            for k, v in errors.items():
                logger.error(f"{k}: {v}")
    return out


def make_hashable(obj: Any) -> Hashable:
    """
    Recursively converts unhashable objects into hashable ones.

    Handles:
    - Basic hashable types (int, str, float, bool, etc.)
    - Dictionaries -> frozenset of tuples
    - Lists/Tuples -> tuple of hashable items
    - Sets -> frozenset
    - Custom objects -> string representation
    - None -> None

    Args:
        obj: Any Python object

    Returns:
        A hashable version of the input object
    """
    # Handle None
    if obj is None:
        return None

    # Try direct hashing first
    try:
        hash(obj)
        return obj
    except TypeError:
        pass

    # Handle mappings (dict-like objects)
    if isinstance(obj, Mapping):
        items = sorted((make_hashable(k), make_hashable(v)) for k, v in obj.items())
        return frozenset(items)

    # Handle sequences (list-like objects)
    if isinstance(obj, Sequence) and not isinstance(obj, (str, bytes)):
        return tuple(make_hashable(item) for item in obj)

    # Handle sets
    if isinstance(obj, Set):
        return frozenset(make_hashable(item) for item in obj)

    # Handle numpy arrays if present
    try:
        import numpy as np

        if isinstance(obj, np.ndarray):
            return hash(obj.tobytes())
    except ImportError:
        pass

    # Handle pandas objects if present
    try:
        import pandas as pd

        if isinstance(obj, (pd.DataFrame, pd.Series)):
            return hash(obj.to_string())
    except ImportError:
        pass

    # Fallback for other objects
    try:
        return str(obj)
    except Exception:
        return f"<unhashable-{type(obj).__name__}>"


def _try_marshal(obj: T) -> Optional[T]:
    """Attempt to marshal and unmarshal an object. Return None if not possible."""
    try:
        import marshal

        return marshal.loads(marshal.dumps(obj))
    except Exception:
        return None


def _deepcopy(obj: T, memo=None) -> T:
    # if memo is None:
    #     memo = {}

    # obj_id = id(obj)
    # if obj_id in memo:
    #     return memo[obj_id]

    # if hasattr(obj, '__deepcopy__'):
    #     result = obj.__deepcopy__(memo)
    #     memo[obj_id] = result
    #     return result

    try:
        return copy.deepcopy(obj, memo)

    except Exception as e:
        if isinstance(obj, (ModuleType, FunctionType, type)):
            return obj  # Return the object itself for modules, functions and types
        else:
            raise e


T = TypeVar('T')


deepcopy = _deepcopy
##────────────────────────────────────────────────────────────────────────────}}}

## {{{                         --     printing     --


def node_repr(
    node,
    prefix='',
    is_last=True,
    is_root=True,
    enable_colors=False,
    context_paths=None,
    context_filter=None,
    show_biggest_context=0,  # show n biggest variables in context
    _seen=None,
):
    if _seen is None:
        _seen = set()

    node_id = id(node)
    if node_id in _seen:
        return f"<circular reference to {node.__class__.__name__}>"

    _seen.add(node_id)

    try:
        if enable_colors:
            BLUE = '\033[94m'
            GREEN = '\033[92m'
            YELLOW = '\033[93m'
            MAGENTA = '\033[95m'
            GREY = '\033[90m'
            DARK_BLUE = '\033[34m'
            DARK_GREEN = '\033[32m'
            WHITE = '\033[97m'
            RESET = '\033[0m'
        else:
            BLUE = ''
            GREEN = ''
            YELLOW = ''
            MAGENTA = ''
            GREY = ''
            DARK_BLUE = ''
            DARK_GREEN = ''
            WHITE = ''
            RESET = ''

        TAG_COLOR: str = DARK_BLUE
        YAML_TAG_COLOR: str = GREY
        VAL_COLOR: str = WHITE
        TYPE_COLOR: str = YELLOW
        KEY_COLOR: str = MAGENTA
        TREE_COLOR: str = GREY
        CONTEXT_COLOR: str = GREEN
        DEFERRED_COLOR: str = BLUE

        VERTICAL: str = TREE_COLOR + '│ ' + RESET
        ELBOW: str = TREE_COLOR + '├─' + RESET
        ELBOW_END: str = TREE_COLOR + '└─' + RESET
        EMPTY: str = TREE_COLOR + '  ' + RESET

        SHORT_TAGS = {
            'tag:yaml.org,2002:int': 'int',
            'tag:yaml.org,2002:str': 'str',
            'tag:yaml.org,2002:float': 'float',
            'tag:yaml.org,2002:bool': 'bool',
            'tag:yaml.org,2002:null': 'null',
            'tag:yaml.org,2002:map': 'map',
            'tag:yaml.org,2002:seq': 'seq',
        }
        for k, v in SHORT_TAGS.items():
            SHORT_TAGS[k] = YAML_TAG_COLOR + v + RESET

        NODE_TYPES = {
            'ScalarNode': '',
            'MappingNode': '',
            'SequenceNode': '',
            'InterpolableNode': '[INTRP]',
            'MergeNode': '[MERGE]',
            'IncludeNode': '[INCL]',
            'DeferredNode': f'{DEFERRED_COLOR}[DEFER]{RESET}',
        }

        def format_context(node):
            if (
                not hasattr(node, 'context')
                or not node.context
                or (not context_paths and not context_filter and not show_biggest_context)
            ):
                return ''

            # Convert string paths to KeyPath objects
            from dracon.keypath import KeyPath

            paths = (
                [KeyPath(p) if isinstance(p, str) else p for p in context_paths]
                if context_paths
                else []
            )

            # Filter context based on paths or custom filter
            matching_items = []
            sizes = {}
            if show_biggest_context > 0:
                from .asizeof import asizeof

                na = set()

                for key, value in node.context.items():
                    try:
                        sizes[key] = asizeof(value)
                    except Exception:
                        na.add(key)
                sizes = list(sorted(sizes.items(), key=lambda item: item[1], reverse=True))
                # add N/A on top of the list
                sizes = [(k, 'N/A') for k in na] + sizes[:show_biggest_context]
                sizes = dict(sizes)

            for key, value in node.context.items():
                key_path = f"/{key}"  # Convert context key to path format
                if any(path.match(KeyPath(key_path)) for path in paths):
                    matching_items.append(f"{key}={value}")
                elif context_filter and context_filter(key, value):
                    matching_items.append(f"{key}={value}")

            if matching_items or show_biggest_context:
                items_str = ', '.join(matching_items)
                if show_biggest_context:

                    def pretty_size(s):
                        if isinstance(s, str):
                            return s
                        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
                            if s < 1000.0:
                                return f"{s:.2f} {unit}"
                            s /= 1000.0
                        return f"{s:.2f} PB"

                    items_str += ', '
                    items_str += ', '.join(
                        f"{key}={pretty_size(value)}" for key, value in sizes.items()
                    )
                return f'{CONTEXT_COLOR}[ctx: {items_str}]{RESET}'
            return ''

        def get_node_repr(node):
            is_deferred = hasattr(node, '__class__') and node.__class__.__name__ == 'DeferredNode'
            if is_deferred:
                defer_tag = NODE_TYPES.get('DeferredNode', '')
            else:
                defer_tag = ''

            ntag = ''
            if hasattr(node, 'tag'):
                ntag = node.tag
            tag = SHORT_TAGS.get(ntag, ntag)
            node_type = type(node).__name__ if not is_deferred else type(node.value).__name__
            tstring = f'{TYPE_COLOR}{NODE_TYPES.get(node_type,"")}{RESET}'
            nctx = format_context(node)

            if isinstance(node, (MappingNode, SequenceNode)) or (
                is_deferred and isinstance(node.value, (MappingNode, SequenceNode))
            ):
                return f'{TAG_COLOR}{tag}{RESET} {nctx} {tstring} {defer_tag}'

            nvalue = node.value if is_deferred else node
            if hasattr(nvalue, 'value'):
                nvalue = nvalue.value

            return (
                f'{TAG_COLOR}{tag}{RESET} {nctx} {VAL_COLOR}{nvalue}{RESET} {tstring} {defer_tag}'
            )

        output = ''

        if is_root:
            output += TREE_COLOR + '●─' + get_node_repr(node) + ' '
        else:
            connector: str = ELBOW_END if is_last else ELBOW
            line_prefix = prefix + connector
            output += line_prefix + get_node_repr(node) + '\n'

        # For DeferredNode, we want to traverse its value
        traverse_node = (
            node.value
            if hasattr(node, '__class__') and node.__class__.__name__ == 'DeferredNode'
            else node
        )

        if isinstance(traverse_node, MappingNode):
            if is_root:
                output = '\n' + output + '\n'
            child_prefix = prefix + (EMPTY if is_last else VERTICAL)
            items = traverse_node.value
            n = len(items)

            for i, (key, value) in enumerate(items):
                is_last_item = i == n - 1

                # Print the key
                key_connector: str = ELBOW_END if is_last_item else ELBOW
                key_line_prefix = child_prefix + key_connector

                if hasattr(key, 'value'):
                    key_repr = f'{TAG_COLOR}{SHORT_TAGS.get(key.tag, key.tag)}{RESET} {TREE_COLOR}󰌆{KEY_COLOR} {key.value} {RESET}'
                    keytypestr = f'{TYPE_COLOR}{NODE_TYPES.get(type(key).__name__,"")}{RESET}'
                    key_repr += f'{keytypestr}'
                else:
                    key_repr = f'noval(<{type(key)}>{key}) [KEY]'
                output += key_line_prefix + key_repr + '\n'

                # Recursively print the value
                child_output = node_repr(
                    value,
                    prefix=child_prefix + (EMPTY if is_last_item else VERTICAL),
                    is_last=True,
                    is_root=False,
                    enable_colors=enable_colors,
                    context_paths=context_paths,
                    context_filter=context_filter,
                    show_biggest_context=show_biggest_context,
                    _seen=_seen,
                )
                output += child_output

        elif isinstance(traverse_node, SequenceNode):
            child_prefix = prefix + (EMPTY if is_last else VERTICAL)
            items = traverse_node.value
            n = len(items)

            for i, value in enumerate(items):
                is_last_item = i == n - 1
                child_output = node_repr(
                    value,
                    prefix=child_prefix,
                    is_last=is_last_item,
                    is_root=False,
                    enable_colors=enable_colors,
                    context_paths=context_paths,
                    context_filter=context_filter,
                    show_biggest_context=show_biggest_context,
                    _seen=_seen,
                )
                output += child_output

        return output
    finally:
        _seen.remove(node_id)


##────────────────────────────────────────────────────────────────────────────}}}

## {{{                    --     resolvable helpers     --


def get_inner_type(resolvable_type: Type):
    args = get_args(resolvable_type)
    if args:
        return args[0]
    return Any


##────────────────────────────────────────────────────────────────────────────}}}

================
File: dracon/yaml.py
================
import re
from ruamel.yaml import YAML
from typing import Any, Optional, Union, Dict
import copyreg


class PicklableYAML(YAML):
    """A picklable version of ruamel.yaml.YAML"""

    def __init__(self, *args, typ='rt', **kwargs):
        super().__init__(*args, typ=typ, **kwargs)
        self._registered_types = {}  # Store registered types
        self.allow_unicode = True
        self.escape_char = None

    def register_class(self, cls):
        """Override register_class to keep track of registered types"""
        self._registered_types[cls.yaml_tag] = cls
        return super().register_class(cls)

    def __getstate__(self) -> Dict[str, Any]:
        """Get the object's state for pickling."""
        state = self.__dict__.copy()

        # Remove unpicklable attributes that are recreated on demand
        unpicklable_attrs = {
            '_reader',
            '_scanner',
            '_parser',
            '_composer',
            '_constructor',
            '_resolver',
            '_emitter',
            '_serializer',
            '_representer',
            '_stream',
            '_context_manager',
        }

        for attr in unpicklable_attrs:
            state.pop(attr, None)

        # Convert compiled regexes to patterns
        for key, value in state.items():
            if isinstance(value, re.Pattern):
                state[key] = value.pattern

        # Store essential configuration
        state['_config'] = {
            'typ': self.typ,
            'pure': getattr(self, 'pure', False),
            'plug_ins': getattr(self, 'plug_ins', []),
            'version': self.version,
            'old_indent': self.old_indent,
            'width': self.width,
            'preserve_quotes': self.preserve_quotes,
            'default_flow_style': self.default_flow_style,
            'encoding': self.encoding,
            'allow_unicode': self.allow_unicode,
            'line_break': self.line_break,
            'allow_duplicate_keys': self.allow_duplicate_keys,
        }

        return state

    def __setstate__(self, state: Dict[str, Any]) -> None:
        """Restore the object's state after unpickling."""
        # Extract configuration
        config = state.pop('_config', {})
        registered_types = state.get('_registered_types', {})

        # Reinitialize the base YAML object with saved configuration
        super().__init__(
            typ=config.get('typ', ['rt']),
            pure=config.get('pure', False),
            plug_ins=config.get('plug_ins', []),
        )

        # Restore compiled regexes
        for key, value in state.items():
            if isinstance(value, str) and key.endswith('_pattern'):
                state[key] = re.compile(value)

        # Update instance with saved state
        self.__dict__.update(state)

        # Restore configuration attributes
        for key, value in config.items():
            if hasattr(self, key):
                setattr(self, key, value)

        # Re-register types
        for cls in registered_types.values():
            self.register_class(cls)

    def __deepcopy__(self, memo):
        """Implement deep copy support."""
        state = self.__getstate__()
        new_instance = self.__class__()
        new_instance.__setstate__(state)
        return new_instance


# Register with copyreg to handle pickling
def _pickle_yaml(yaml):
    state = yaml.__getstate__()
    return PicklableYAML, (), state


copyreg.pickle(YAML, _pickle_yaml)

================
File: .gitignore
================
dracon/tests/configs/__pycache__
dracon/tests/__pycache__
dracon/__pycache__
dracon.egg-info
__pycache__/
*.pyc
*.pyo
*.pyd
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
__archive
.DS_Store
prof
dracon/deepcopy_log.csv
site

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Jean Disset

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

Note: This project contains the file "asizeof.py" from the Pympler library, which 
is licensed under a BSD 3-clause license. See the copyright notice in that file 
for details.

================
File: mkdocs.yml
================
site_name: Dracon
site_description: A modular configuration system and CLI generator for Python
repo_url: https://github.com/jdisset/dracon
repo_name: jdisset/dracon

theme:
  name: material
  palette:
    - media: "(prefers-color-scheme: light)"
      scheme: default
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-7
        name: Switch to dark mode
    - media: "(prefers-color-scheme: dark)"
      scheme: slate
      primary: indigo
      accent: indigo
      toggle:
        icon: material/brightness-4
        name: Switch to light mode
  features:
    - navigation.sections
    - navigation.top
    - search.highlight
    - search.share
    - search.suggest
    - content.tabs.link
    - content.code.annotate
    - content.code.copy

markdown_extensions:
  - admonition
  - pymdownx.details
  - pymdownx.superfences
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.inlinehilite
  - pymdownx.snippets
  - pymdownx.tabbed:
      alternate_style: true
  - attr_list
  - md_in_html

nav:
  - Home: index.md
  - Guides:
      - File Inclusion: includes.md
      - Expression Interpolation: interpolation.md
      - Merging: merging.md
      - Command Line Programs: cli.md
      - Node Instructions: instructions.md
      - Advanced Usage: advanced.md

================
File: pyproject.toml
================
[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "dracon"
authors = [{name = "Jean Disset", email = "jdisset@mit.edu"}]
version = "0.1.0"
description = "A configuration library that extends yaml's alias, anchor and merge system, combined with Pydantic's type hints"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "pydantic>=2.0",
    "ruamel.yaml",
	"xxhash",
	"asteval",
	"cachetools",
	"regex"
]

[project.optional-dependencies]
test = ["pytest"]

[tool.hatch.build.targets.wheel]
packages = ["dracon"]

[tool.hatch.build.targets.sdist]
include = [
    "/dracon",
    "/tests",
]

[tool.hatch.version]
path = "dracon/__init__.py"

================
File: README.md
================
# Dracon

A modular configuration system for Python that extends YAML with powerful features like file inclusion, interpolation, dynamic object construction and allow to generate extensible, documented command line interfaces.

## Key Features

- **File Inclusion**: Include and merge other configuration files
- **Advanced Merging**: Flexible merge strategies for complex configurations
- **Expression Interpolation**: Use Python expressions within your YAML files
- **Deferred Construction**: Control when and how objects are constructed
- **Pydantic Integration**: Build type-safe configuration models
- **CLI Support**: Generate full-fledged, extensible command-line programs from Pydantic models

## Quick Start

```python
from dracon import DraconLoader

# Load a configuration file
loader = DraconLoader(context={"instance_id": 1}) # Define context variables
config = loader.load("config.yaml")

# Access configuration values
print(config.database.host)
print(config.service.port)
```

### Basic Configuration Example

```yaml
# config.yaml
database:
  host: ${env:DB_HOST or 'localhost'}
  port: 5432
  credentials: !include "file:./secrets.yaml"

service:
  name: "MyService"
  port: ${8080 + instance_id} # assuming instance_id is defined in context
  settings: !include "pkg:mypackage:settings/${env:ENV}.yaml"
```

## Core Features

### File Inclusion

Include other configuration files using `!include` or `*loader:` syntax:

```yaml
# Multiple include syntaxes
settings: !include "config/settings.yaml" # default is file loader
database: !include file:config/database.yaml
database_2: *file:config/database.yaml # can use the * syntax also
api_key: *env:API_KEY # environment variable loader
defaults: !include pkg:my_package:configs/defaults.yaml
```

### Expression Interpolation

Use Python expressions with `${...}` syntax:

```yaml
!define env: prod # Define a variable

service:
  port: ${base_port + instance_id}
  url: ${"http://" + host + ":" + str(@/service/port)}
  mode: ${'production' if env == 'prod' else 'development'}
```

### Flexible Merging

Control how configurations are merged using merge operators:

```yaml
# Merge with different strategies
<<{+>}: *file:base.yaml          # Append recursively, existing values take priority
<<{~<}: *file:overrides.yaml     # Replace values, new values take priority
<<{+>}@settings: *file:settings  # Merge at specific path
```

### Type-Safe Configurations

Use Pydantic models for type validation:

```python
from pydantic import BaseModel
from typing import List

class DatabaseConfig(BaseModel):
    host: str
    port: int
    replicas: List[str]

class ServiceConfig(BaseModel):
    name: str
    db: DatabaseConfig
    port: int

# Load and validate configuration
config = loader.load("config.yaml")
service_config = ServiceConfig(**config)
```

## Why Dracon?

Dracon solves common configuration management challenges:

- **Environment-Specific Configs**: Easily handle different environments using file inclusion and interpolation
- **Complex Deployments**: Use advanced merging to handle layered configurations
- **Type Safety**: Validate configurations at load time with Pydantic integration
- **Command-Line Programs**: Generate CLI programs from Pydantic models

## Installation

```bash
pip install dracon
```

## Documentation

Visit [full documentation](https://dracon.readthedocs.io/) for detailed guides and examples.

================
File: repomix.config.json
================
{
  "output": {
    "filePath": "repomix-output.txt",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": ["**/site/*", "dracon/asizeof.py"]
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}



================================================================
End of Codebase
================================================================
